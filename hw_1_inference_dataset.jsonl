{"prefix": "def get_vid_from_url(url):\n        \"\"\"Extracts video ID from URL.", "suffix": "        return match1(url, r'youtu\\.be/([^?/]+)') or \\\n          match1(url, r'youtube\\.com/embed/([^/?]+)') or \\\n          match1(url, r'youtube\\.com/v/([^/?]+)') or \\\n          match1(url, r'youtube\\.com/watch/([^/?]+)') or \\\n          parse_query_param(url, 'v') or \\\n          parse_query_param(parse_query_param(url, 'u'), 'v')", "gt": "        \"\"\""}
{"prefix": "def sina_xml_to_url_list(xml_data):\n    \"\"\"str->list\n    Convert XML to URL List.\n    From Biligrab.\n    \"\"\"\n    rawurl = []\n    dom = parseString(xml_data)\n    for node in dom.getElementsByTagName('durl'):\n        url = node.getElementsByTagName('url')[0]\n        rawurl.append(url.childNodes[0].data)", "suffix": "", "gt": "    return rawurl"}
{"prefix": "", "suffix": "    \"\"\"From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js\n    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal\n    L110\"\"\"\n    strSeed = \"gGddgPfeaf_gzyr\"\n    prehash = upid + \"_\" + strSeed\n    return md5(prehash.encode('utf-8')).hexdigest()", "gt": "def makeMimi(upid):"}
{"prefix": "", "suffix": "    \"\"\"wrapper\"\"\"\n    #'http://video.fc2.com/en/content/20151021bTVKnbEw'\n    #'http://xiaojiadianvideo.asia/content/20151021bTVKnbEw'\n    #'http://video.fc2.com/ja/content/20151021bTVKnbEw'\n    #'http://video.fc2.com/tw/content/20151021bTVKnbEw'\n    hostname = urlparse(url).hostname\n    if not ('fc2.com' in hostname or 'xiaojiadianvideo.asia' in hostname):\n        return False\n    upid = match1(url, r'.+/content/(\\w+)')\n\n    fc2video_download_by_upid(upid, output_dir, merge, info_only)", "gt": "def fc2video_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):"}
{"prefix": "def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Dailymotion videos by URL.\n    \"\"\"\n\n    html = get_content(rebuilt_url(url))\n    info = json.loads(match1(html, r'qualities\":({.+?}),\"'))\n    title = match1(html, r'\"video_title\"\\s*:\\s*\"([^\"]+)\"') or \\\n            match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n    title = unicodize(title)\n\n    for quality in ['1080','720','480','380','240','144','auto']:\n        try:\n            real_url = info[quality][1][\"url\"]\n            if real_url:\n                break\n        except KeyError:\n            pass\n\n    mime, ext, size = url_info(real_url)", "suffix": "    print_info(site_info, title, mime, size)\n    if not info_only:\n        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)", "gt": ""}
{"prefix": "def dictify(r,root=True):", "suffix": "    if root:\n        return {r.tag : dictify(r, False)}\n    d=copy(r.attrib)\n    if r.text:\n        d[\"_text\"]=r.text\n    for x in r.findall(\"./*\"):\n        if x.tag not in d:\n            d[x.tag]=[]\n        d[x.tag].append(dictify(x,False))\n    return d", "gt": "    \"\"\"http://stackoverflow.com/a/30923963/2946714\"\"\""}
{"prefix": "def ucas_download_single(url, output_dir = '.', merge = False, info_only = False, **kwargs):\n    '''video page'''\n    html = get_content(url)\n    # resourceID is UUID\n    resourceID = re.findall( r'resourceID\":\"([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})', html)[0]\n    assert resourceID != '', 'Cannot find resourceID!'\n\n    title = match1(html, r'<div class=\"bc-h\">(.+)</div>')\n    url_lists = _ucas_get_url_lists_by_resourceID(resourceID)\n    assert url_lists, 'Cannot find any URL of such class!'\n    \n    for k, part in enumerate(url_lists):\n        part_title = title + '_' + str(k)\n        print_info(site_info, part_title, 'flv', 0)", "suffix": "            download_urls(part, part_title, 'flv', total_size=None, output_dir=output_dir, merge=merge)", "gt": "        if not info_only:"}
{"prefix": "def ucas_download_playlist(url, output_dir = '.', merge = False, info_only = False, **kwargs):\n    '''course page'''\n    html = get_content(url)\n\n    parts = re.findall( r'(getplaytitle.do\\?.+)\"', html)", "suffix": "\n    for part_path in parts:\n        ucas_download('http://v.ucas.ac.cn/course/' + part_path, output_dir=output_dir, merge=merge, info_only=info_only)", "gt": "    assert parts, 'No part found!'"}
{"prefix": "def sina_download_by_vid(vid, title=None, output_dir='.', merge=True, info_only=False):\n    \"\"\"Downloads a Sina video by its unique vid.\n    http://video.sina.com.cn/\n    \"\"\"\n    xml = api_req(vid)\n    urls, name, size = video_info(xml)\n    if urls is None:\n        log.wtf(name)", "suffix": "    print_info(site_info, title, 'flv', size)\n    if not info_only:\n        download_urls(urls, title, 'flv', size, output_dir = output_dir, merge = merge)", "gt": "    title = name"}
{"prefix": "def sina_download_by_vkey(vkey, title=None, output_dir='.', merge=True, info_only=False):\n    \"\"\"Downloads a Sina video by its unique vkey.\n    http://video.sina.com/\n    \"\"\"\n\n    url = 'http://video.sina.com/v/flvideo/%s_0.flv' % vkey\n    type, ext, size = url_info(url)\n\n    print_info(site_info, title, 'flv', size)\n    if not info_only:", "suffix": "", "gt": "        download_urls([url], title, 'flv', size, output_dir = output_dir, merge = merge)"}
{"prefix": "def sina_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Sina videos by URL.\n    \"\"\"\n    if 'news.sina.com.cn/zxt' in url:\n        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n        return", "suffix": "    vid = match1(url, r'vid=(\\d+)')\n    if vid is None:\n        video_page = get_content(url)\n        vid = hd_vid = match1(video_page, r'hd_vid\\s*:\\s*\\'([^\\']+)\\'')\n        if hd_vid == '0':\n            vids = match1(video_page, r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'').split('|')\n            vid = vids[-1]\n\n    if vid is None:\n        vid = match1(video_page, r'vid:\"?(\\d+)\"?')\n    if vid:\n        #title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')\n        sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n    else:\n        vkey = match1(video_page, r'vkey\\s*:\\s*\"([^\"]+)\"')\n        if vkey is None:\n            vid = match1(url, r'#(\\d+)')\n            sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n            return\n        title = match1(video_page, r'title\\s*:\\s*\"([^\"]+)\"')\n        sina_download_by_vkey(vkey, title=title, output_dir=output_dir, merge=merge, info_only=info_only)", "gt": ""}
{"prefix": "def yixia_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):\n    \"\"\"wrapper\"\"\"\n    hostname = urlparse(url).hostname\n    if 'n.miaopai.com' == hostname: \n        smid = match1(url, r'n\\.miaopai\\.com/media/([^.]+)') \n        miaopai_download_by_smid(smid, output_dir, merge, info_only)\n        return\n    elif 'miaopai.com' in hostname:  #Miaopai\n        yixia_download_by_scid = yixia_miaopai_download_by_scid\n        site_info = \"Yixia Miaopai\"\n\n        scid = match1(url, r'miaopai\\.com/show/channel/([^.]+)\\.htm') or \\\n               match1(url, r'miaopai\\.com/show/([^.]+)\\.htm') or \\\n               match1(url, r'm\\.miaopai\\.com/show/channel/([^.]+)\\.htm') or \\\n               match1(url, r'm\\.miaopai\\.com/show/channel/([^.]+)')\n\n    elif 'xiaokaxiu.com' in hostname:  #Xiaokaxiu\n        yixia_download_by_scid = yixia_xiaokaxiu_download_by_scid\n        site_info = \"Yixia Xiaokaxiu\"\n\n        if re.match(r'http://v.xiaokaxiu.com/v/.+\\.html', url):  #PC\n            scid = match1(url, r'http://v.xiaokaxiu.com/v/(.+)\\.html')\n        elif re.match(r'http://m.xiaokaxiu.com/m/.+\\.html', url):  #Mobile\n            scid = match1(url, r'http://m.xiaokaxiu.com/m/(.+)\\.html')\n", "suffix": "        pass\n\n    yixia_download_by_scid(scid, output_dir, merge, info_only)", "gt": "    else:"}
{"prefix": "def veoh_download(url, output_dir = '.', merge = False, info_only = False, **kwargs):\n    '''Get item_id'''\n    if re.match(r'http://www.veoh.com/watch/\\w+', url):\n        item_id = match1(url, r'http://www.veoh.com/watch/(\\w+)')\n    elif re.match(r'http://www.veoh.com/m/watch.php\\?v=\\.*', url):\n        item_id = match1(url, r'http://www.veoh.com/m/watch.php\\?v=(\\w+)')\n    else:", "suffix": "    veoh_download_by_id(item_id, output_dir = '.', merge = False, info_only = info_only, **kwargs)", "gt": "        raise NotImplementedError('Cannot find item ID')"}
{"prefix": "def veoh_download_by_id(item_id, output_dir = '.', merge = False, info_only = False, **kwargs):\n    \"\"\"Source: Android mobile\"\"\"\n    webpage_url = 'http://www.veoh.com/m/watch.php?v={item_id}&quality=1'.format(item_id = item_id)\n\n    #grab download URL\n    a = get_content(webpage_url, decoded=True)\n    url = match1(a, r'<source src=\"(.*?)\\\"\\W')\n", "suffix": "    title = match1(a, r'<meta property=\"og:title\" content=\"([^\"]*)\"')\n\n    type_, ext, size = url_info(url)\n    print_info(site_info, title, type_, size)\n    if not info_only:\n        download_urls([url], title, ext, total_size=None, output_dir=output_dir, merge=merge)", "gt": "    #grab title"}
{"prefix": "def download_by_id(self, vid = '', title = None, output_dir='.', merge=True, info_only=False,**kwargs):", "suffix": "        \n        Keyword arguments:\n        self: self\n        vid: The video ID for BokeCC cloud, something like\n        FE3BB999594978049C33DC5901307461\n        \n        Calls the prepare() to download the video.\n        \n        If no title is provided, this method shall try to find a proper title\n        with the information providin within the\n        returned content of the API.\"\"\"\n\n        assert vid\n\n        self.prepare(vid = vid, title = title, **kwargs)\n\n        self.extract(**kwargs)\n\n        self.download(output_dir = output_dir, \n                    merge = merge, \n                    info_only = info_only, **kwargs)", "gt": "        \"\"\"self, str->None"}
{"prefix": "def get_vid_from_url(self, url):\n        \"\"\"Extracts video ID from live.qq.com.\n        \"\"\"\n        hit = re.search(r'live.qq.com/(\\d+)', url)\n        if hit is not None:\n            return hit.group(1)", "suffix": "        if hit is not None:\n            return self.get_room_id_from_url(hit.group(1))\n        html = get_content(url)\n        room_id = match1(html, r'room_id\\\":(\\d+)')\n        if room_id is None:\n            log.wtf('Unknown page {}'.format(url))\n        return room_id", "gt": "        hit = re.search(r'live.qq.com/directory/match/(\\d+)', url)"}
{"prefix": "def sprint(text, *colors):", "suffix": "    return \"\\33[{}m{content}\\33[{}m\".format(\";\".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text", "gt": "    \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\""}
{"prefix": "def print_log(text, *colors):", "suffix": "    sys.stderr.write(sprint(\"{}: {}\".format(script_name, text), *colors) + \"\\n\")", "gt": "    \"\"\"Print a log message to standard error.\"\"\""}
{"prefix": "def e(message, exit_code=None):\n    \"\"\"Print an error log message.\"\"\"\n    print_log(message, YELLOW, BOLD)", "suffix": "        sys.exit(exit_code)", "gt": "    if exit_code is not None:"}
{"prefix": "def wtf(message, exit_code=1):\n    \"\"\"What a Terrible Failure!\"\"\"\n    print_log(message, RED, BOLD)\n    if exit_code is not None:", "suffix": "", "gt": "        sys.exit(exit_code)"}
{"prefix": "def detect_os():\n    \"\"\"Detect operating system.\n    \"\"\"\n\n    # Inspired by:\n    # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py\n\n    syst = system().lower()\n    os = 'unknown'\n\n    if 'cygwin' in syst:\n        os = 'cygwin'\n    elif 'darwin' in syst:\n        os = 'mac'\n    elif 'linux' in syst:\n        os = 'linux'\n        # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423\n        try:\n            with open('/proc/version', 'r') as f:\n                if 'microsoft' in f.read().lower():\n                    os = 'wsl'\n        except: pass", "suffix": "        os = 'windows'\n    elif 'bsd' in syst:\n        os = 'bsd'\n\n    return os", "gt": "    elif 'windows' in syst:"}
{"prefix": "def miaopai_download_by_fid(fid, output_dir = '.', merge = False, info_only = False, **kwargs):\n    '''Source: Android mobile'''", "suffix": "\n    mobile_page = get_content(page_url, headers=fake_headers_mobile)\n    url = match1(mobile_page, r'<video id=.*?src=[\\'\"](.*?)[\\'\"]\\W')\n    if url is None:\n        wb_mp = re.search(r'<script src=([\\'\"])(.+?wb_mp\\.js)\\1>', mobile_page).group(2)\n        return miaopai_download_by_wbmp(wb_mp, fid, output_dir=output_dir, merge=merge,\n                                        info_only=info_only, total_size=None, **kwargs)\n    title = match1(mobile_page, r'<title>((.|\\n)+?)</title>')\n    if not title:\n        title = fid\n    title = title.replace('\\n', '_')\n    ext, size = 'mp4', url_info(url)[2]\n    print_info(site_info, title, ext, size)\n    if not info_only:\n        download_urls([url], title, ext, total_size=None, output_dir=output_dir, merge=merge)", "gt": "    page_url = 'http://video.weibo.com/show?fid=' + fid + '&type=mp4'"}
{"prefix": "", "suffix": "    \"\"\"str->None\"\"\"\n    # https://vimeo.com/channels/464686\n    channel_id = match1(url, r'http://vimeo.com/channels/(\\w+)')\n    vimeo_download_by_channel_id(channel_id, output_dir, merge, info_only, **kwargs)", "gt": "def vimeo_download_by_channel(url, output_dir='.', merge=False, info_only=False, **kwargs):"}
{"prefix": "def vimeo_download_by_channel_id(channel_id, output_dir='.', merge=False, info_only=False, **kwargs):\n    \"\"\"str/int->None\"\"\"\n    html = get_content('https://api.vimeo.com/channels/{channel_id}/videos?access_token={access_token}'.format(channel_id=channel_id, access_token=access_token))", "suffix": "    id_list = []\n\n    #print(data)\n    for i in data['data']:\n        id_list.append(match1(i['uri'], r'/videos/(\\w+)'))\n\n    for id in id_list:\n        try:\n            vimeo_download_by_id(id, None, output_dir, merge, info_only, **kwargs)\n        except urllib.error.URLError as e:\n            log.w('{} failed with {}'.format(id, e))", "gt": "    data = loads(html)"}
{"prefix": "def vimeo_download_by_id(id, title=None, output_dir='.', merge=True, info_only=False, **kwargs):\n    '''\n    try:\n        # normal Vimeo video\n        html = get_content('https://vimeo.com/' + id)\n        cfg_patt = r'clip_page_config\\s*=\\s*(\\{.+?\\});'\n        cfg = json.loads(match1(html, cfg_patt))\n        video_page = get_content(cfg['player']['config_url'], headers=fake_headers)\n        title = cfg['clip']['title']", "suffix": "    except:\n        # embedded player - referer may be required\n        if 'referer' in kwargs:\n            fake_headers['Referer'] = kwargs['referer']\n\n        video_page = get_content('http://player.vimeo.com/video/%s' % id, headers=fake_headers)\n        title = r1(r'<title>([^<]+)</title>', video_page)\n        info = loads(match1(video_page, r'var t=(\\{.+?\\});'))\n\n    streams = info['request']['files']['progressive']\n    streams = sorted(streams, key=lambda i: i['height'])\n    url = streams[-1]['url']\n\n    type, ext, size = url_info(url, faker=True)\n\n    print_info(site_info, title, type, size)\n    if not info_only:\n        download_urls([url], title, ext, size, output_dir, merge=merge, faker=True)\n    '''\n    site = VimeoExtractor()\n    site.download_by_vid(id, info_only=info_only, output_dir=output_dir, merge=merge, **kwargs)", "gt": "        info = loads(video_page)"}
{"prefix": "def ckplayer_get_info_by_xml(ckinfo):", "suffix": "    Information for CKPlayer API content.\"\"\"\n    e = ET.XML(ckinfo)\n    video_dict = {'title': '',\n                  #'duration': 0,\n                  'links': [],\n                  'size': 0,\n                  'flashvars': '',}\n    dictified = dictify(e)['ckplayer']\n    if 'info' in dictified:\n        if '_text' in dictified['info'][0]['title'][0]:  #title\n            video_dict['title'] = dictified['info'][0]['title'][0]['_text'].strip()\n\n    #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration\n        #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()\n\n    if '_text' in dictified['video'][0]['size'][0]:  #size exists for 1 piece\n        video_dict['size'] = sum([int(i['size'][0]['_text']) for i in dictified['video']])\n\n    if '_text' in dictified['video'][0]['file'][0]:  #link exist\n        video_dict['links'] = [i['file'][0]['_text'].strip() for i in dictified['video']]\n\n    if '_text' in dictified['flashvars'][0]:\n        video_dict['flashvars'] = dictified['flashvars'][0]['_text'].strip()\n\n    return video_dict", "gt": "    \"\"\"str->dict"}
{"prefix": "def get_video_url_from_video_id(video_id):\n    \"\"\"Splicing URLs according to video ID to get video details\"\"\"\n    # from js\n    data = [\"\"] * 256\n    for index, _ in enumerate(data):\n        t = index\n        for i in range(8):\n            t = -306674912 ^ unsigned_right_shitf(t, 1) if 1 & t else unsigned_right_shitf(t, 1)\n        data[index] = t\n\n    def tmp():\n        rand_num = random.random()\n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\".format(video_id=video_id,\n                                                                              random_num=str(rand_num)[2:])\n        e = o = r = -1\n        i, a = 0, len(path)\n        while i < a:\n            e = ord(path[i])\n            i += 1\n            if e < 128:\n                r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ e)]\n            else:\n                if e < 2048:\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (192 | e >> 6 & 31))]\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n                else:\n                    if 55296 <= e < 57344:\n                        e = (1023 & e) + 64\n                        i += 1\n                        o = 1023 & t.url(i)\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (240 | e >> 8 & 7))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 2 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | o >> 6 & 15 | (3 & e) << 4))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & o))]\n                    else:\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (224 | e >> 12 & 15))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 6 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n", "suffix": "\n    while 1:\n        url = tmp()\n        if url.split(\"=\")[-1][0] != \"-\":  # \u53c2\u6570s\u4e0d\u80fd\u4e3a\u8d1f\u6570\n            return url", "gt": "        return \"https://ib.365yg.com{path}&s={param}\".format(path=path, param=unsigned_right_shitf(r ^ -1, 0))"}
{"prefix": "def get_vid_from_url(url):\n        \"\"\"Extracts video ID from URL.\n        \"\"\"", "suffix": "        if not vid:\n            vid = match1(url, 'https?://www.mgtv.com/hz/bdpz/\\d+/(\\d+).html')\n        return vid", "gt": "        vid = match1(url, 'https?://www.mgtv.com/(?:b|l)/\\d+/(\\d+).html')"}
{"prefix": "def get_mgtv_real_url(url):\n        \"\"\"str->list of str", "suffix": "        content = loads(get_content(url))\n        m3u_url = content['info']\n        split = urlsplit(m3u_url)\n        \n        base_url = \"{scheme}://{netloc}{path}/\".format(scheme = split[0],\n                                                      netloc = split[1],\n                                                      path = dirname(split[2]))\n\n        content = get_content(content['info'])  #get the REAL M3U url, maybe to be changed later?\n        segment_list = []\n        segments_size = 0\n        for i in content.split():\n            if not i.startswith('#'):  #not the best way, better we use the m3u8 package\n                segment_list.append(base_url + i)\n            # use ext-info for fast size calculate\n            elif i.startswith('#EXT-MGTV-File-SIZE:'):\n                segments_size += int(i[i.rfind(':')+1:])\n\n        return m3u_url, segments_size, segment_list", "gt": "        Give you the real URLs.\"\"\""}
{"prefix": "def get_head(repo_path):", "suffix": "    try:\n        ref = open(os.path.join(repo_path, '.git', 'HEAD'), 'r').read().strip()[5:].split('/')\n        branch = ref[-1]\n        commit = open(os.path.join(repo_path, '.git', *ref), 'r').read().strip()[:7]\n        return branch, commit\n    except:\n        return None", "gt": "    \"\"\"Get (branch, commit) from HEAD of a git repo.\"\"\""}
{"prefix": "def legitimize(text, os=detect_os()):\n    \"\"\"Converts a string to a valid filename.", "suffix": "\n    # POSIX systems\n    text = text.translate({\n        0: None,\n        ord('/'): '-',\n        ord('|'): '-',\n    })\n\n    # FIXME: do some filesystem detection\n    if os == 'windows' or os == 'cygwin' or os == 'wsl':\n        # Windows (non-POSIX namespace)\n        text = text.translate({\n            # Reserved in Windows VFAT and NTFS\n            ord(':'): '-',\n            ord('*'): '-',\n            ord('?'): '-',\n            ord('\\\\'): '-',\n            ord('\\\"'): '\\'',\n            # Reserved in Windows VFAT\n            ord('+'): '-',\n            ord('<'): '-',\n            ord('>'): '-',\n            ord('['): '(',\n            ord(']'): ')',\n            ord('\\t'): ' ',\n        })\n    else:\n        # *nix\n        if os == 'mac':\n            # Mac OS HFS+\n            text = text.translate({\n                ord(':'): '-',\n            })\n\n        # Remove leading .\n        if text.startswith(\".\"):\n            text = text[1:]\n\n    text = text[:80] # Trim to 82 Unicode characters long\n    return text", "gt": "    \"\"\""}
{"prefix": "def get_terminal_size():\n    \"\"\"Get (width, height) of the current terminal.\"\"\"", "suffix": "        import fcntl, termios, struct # fcntl module only available on Unix\n        return struct.unpack('hh', fcntl.ioctl(1, termios.TIOCGWINSZ, '1234'))\n    except:\n        return (40, 80)", "gt": "    try:"}
{"prefix": "def cbs_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads CBS videos by URL.\n    \"\"\"\n\n    html = get_content(url)\n    pid = match1(html, r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'')", "suffix": "\n    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)", "gt": "    title = match1(html, r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"')"}
{"prefix": "def download(self, **kwargs):\n        \"\"\"Override the original one\n        Ugly ugly dirty hack\"\"\"\n        if 'json_output' in kwargs and kwargs['json_output']:\n            json_output.output(self)\n        elif 'info_only' in kwargs and kwargs['info_only']:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Display the stream\n                stream_id = kwargs['stream_id']\n                if 'index' not in kwargs:\n                    self.p(stream_id)\n                else:\n                    self.p_i(stream_id)\n            else:\n                # Display all available streams\n                if 'index' not in kwargs:\n                    self.p([])\n                else:\n                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n                    self.p_i(stream_id)\n\n        else:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Download the stream\n                stream_id = kwargs['stream_id']\n            else:\n                # Download stream with the best quality\n                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n\n            if 'index' not in kwargs:\n                self.p(stream_id)\n            else:\n                self.p_i(stream_id)\n\n            if stream_id in self.streams:\n                urls = self.streams[stream_id]['src']\n                ext = self.streams[stream_id]['container']\n                total_size = self.streams[stream_id]['size']\n            else:\n                urls = self.dash_streams[stream_id]['src']\n                ext = self.dash_streams[stream_id]['container']\n                total_size = self.dash_streams[stream_id]['size']\n\n            if not urls:\n                log.wtf('[Failed] Cannot extract video source.')", "suffix": "            \n            #Here's the change!!\n            download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n\n            if not kwargs['caption']:\n                print('Skipping captions.')\n                return\n            for lang in self.caption_tracks:\n                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n                print('Saving %s ... ' % filename, end=\"\", flush=True)\n                srt = self.caption_tracks[lang]\n                with open(os.path.join(kwargs['output_dir'], filename),\n                          'w', encoding='utf-8') as x:\n                    x.write(srt)\n                print('Done.')", "gt": "            # For legacy main()"}
{"prefix": "def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"str, str, str, bool, bool ->None\n\n    Download Acfun video by vid.\n\n    Call Acfun API, decide which site to use, and pass the job to its\n    extractor.\n    \"\"\"\n\n    #first call the main parasing API\n    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid))\n\n    sourceType = info['sourceType']\n\n    #decide sourceId to know which extractor to use\n    if 'sourceId' in info: sourceId = info['sourceId']\n    # danmakuId = info['danmakuId']\n\n    #call extractor decided by sourceId\n    if sourceType == 'sina':\n        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'youku':\n        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n    elif sourceType == 'tudou':\n        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'qq':\n        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'letv':\n        letvcloud_download_by_vu(sourceId, '2d8c027396', title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'zhuzhan':\n        #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this\n#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player\n#old code removed\n        url = 'http://www.acfun.cn/v/ac' + vid\n        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)\n        seq = ['mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd']\n        for t in seq:\n            if yk_streams.get(t):\n                preferred = yk_streams[t]\n                break\n#total_size in the json could be incorrect(F.I. 0)\n        size = 0\n        for url in preferred[0]:\n            _, _, seg_size = url_info(url)\n            size += seg_size\n#fallback to flvhd is not quite possible\n        if re.search(r'fid=[0-9A-Z\\-]*.flv', preferred[0][0]):\n            ext = 'flv'\n        else:\n            ext = 'mp4'\n        print_info(site_info, title, ext, size)\n        if not info_only:\n            download_urls(preferred[0], title, ext, size, output_dir=output_dir, merge=merge)\n    else:\n        raise NotImplementedError(sourceType)\n\n    if not info_only and not dry_run:", "suffix": "            print('Skipping danmaku.')\n            return\n        try:\n            title = get_filename(title)\n            print('Downloading %s ...\\n' % (title + '.cmt.json'))\n            cmt = get_srt_json(vid)\n            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-8') as x:\n                x.write(cmt)\n        except:\n            pass", "gt": "        if not kwargs['caption']:"}
{"prefix": "def main_dev(**kwargs):\n    \"\"\"Main entry point.\n    you-get-dev\n    \"\"\"\n\n    # Get (branch, commit) if running from a git repo.\n    head = git.get_head(kwargs['repo_path'])\n", "suffix": "    try:\n        opts, args = getopt.getopt(sys.argv[1:], _short_options, _options)\n    except getopt.GetoptError as e:\n        log.wtf(\"\"\"\n    [Fatal] {}.\n    Try '{} --help' for more options.\"\"\".format(e, script_name))\n\n    if not opts and not args:\n        # Display help.\n        print(_help)\n        # Enter GUI mode.\n        #from .gui import gui_main\n        #gui_main()\n    else:\n        conf = {}\n        for opt, arg in opts:\n            if opt in ('-h', '--help'):\n                # Display help.\n                print(_help)\n\n            elif opt in ('-V', '--version'):\n                # Display version.\n                log.println(\"you-get:\", log.BOLD)\n                log.println(\"    version:  {}\".format(__version__))\n                if head is not None:\n                    log.println(\"    branch:   {}\\n    commit:   {}\".format(*head))\n                else:\n                    log.println(\"    branch:   {}\\n    commit:   {}\".format(\"(stable)\", \"(tag v{})\".format(__version__)))\n\n                log.println(\"    platform: {}\".format(platform.platform()))\n                log.println(\"    python:   {}\".format(sys.version.split('\\n')[0]))\n\n            elif opt in ('-g', '--gui'):\n                # Run using GUI.\n                conf['gui'] = True\n\n            elif opt in ('-f', '--force'):\n                # Force download.\n                conf['force'] = True\n\n            elif opt in ('-l', '--playlist', '--playlists'):\n                # Download playlist whenever possible.\n                conf['playlist'] = True\n\n        if args:\n            if 'gui' in conf and conf['gui']:\n                # Enter GUI mode.\n                from .gui import gui_main\n                gui_main(*args, **conf)\n            else:\n                # Enter console mode.\n                from .console import console_main\n                console_main(*args, **conf)", "gt": "    # Get options and arguments."}
{"prefix": "def ffmpeg_download_stream(files, title, ext, params={}, output_dir='.', stream=True):\n    \"\"\"str, str->True\n    WARNING: NOT THE SAME PARMS AS OTHER FUNCTIONS!!!!!!\n    You can basicly download anything with this function\n    but better leave it alone with\n    \"\"\"\n    output = title + '.' + ext\n\n    if not (output_dir == '.'):\n        output = output_dir + '/' + output\n\n    print('Downloading streaming content with FFmpeg, press q to stop recording...')\n    if stream:\n        ffmpeg_params = [FFMPEG] + ['-y', '-re', '-i']\n    else:", "suffix": "    ffmpeg_params.append(files)  #not the same here!!!!\n\n    if FFMPEG == 'avconv':  #who cares?\n        ffmpeg_params += ['-c', 'copy', output]\n    else:\n        ffmpeg_params += ['-c', 'copy', '-bsf:a', 'aac_adtstoasc']\n\n    if params is not None:\n        if len(params) > 0:\n            for k, v in params:\n                ffmpeg_params.append(k)\n                ffmpeg_params.append(v)\n\n    ffmpeg_params.append(output)\n\n    print(' '.join(ffmpeg_params))\n\n    try:\n        a = subprocess.Popen(ffmpeg_params, stdin= subprocess.PIPE)\n        a.communicate()\n    except KeyboardInterrupt:\n        try:\n            a.stdin.write('q'.encode('utf-8'))\n        except:\n            pass\n\n    return True", "gt": "        ffmpeg_params = [FFMPEG] + ['-y', '-i']"}
{"prefix": "", "suffix": "    \"\"\"Scans through a string for substrings matched some patterns (first-subgroups only).\n\n    Args:\n        text: A string to be scanned.\n        patterns: Arbitrary number of regex patterns.\n\n    Returns:\n        When only one pattern is given, returns a string (None if no match found).\n        When more than one pattern are given, returns a list of strings ([] if no match found).\n    \"\"\"\n\n    if len(patterns) == 1:\n        pattern = patterns[0]\n        match = re.search(pattern, text)\n        if match:\n            return match.group(1)\n        else:\n            return None\n    else:\n        ret = []\n        for pattern in patterns:\n            match = re.search(pattern, text)\n            if match:\n                ret.append(match.group(1))\n        return ret", "gt": "def match1(text, *patterns):"}
{"prefix": "", "suffix": "    \"\"\"Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.\n    \"\"\"\n\n    ret = []\n    for pattern in patterns:\n        match = re.findall(pattern, text)\n        ret += match\n\n    return ret", "gt": "def matchall(text, patterns):"}
{"prefix": "def parse_query_param(url, param):\n    \"\"\"Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.\n    \"\"\"", "suffix": "    try:\n        return parse.parse_qs(parse.urlparse(url).query)[param][0]\n    except:\n        return None", "gt": ""}
{"prefix": "def ungzip(data):\n    \"\"\"Decompresses data for Content-Encoding: gzip.\n    \"\"\"", "suffix": "    import gzip\n    buffer = BytesIO(data)\n    f = gzip.GzipFile(fileobj=buffer)\n    return f.read()", "gt": "    from io import BytesIO"}
{"prefix": "def undeflate(data):\n    \"\"\"Decompresses data for Content-Encoding: deflate.\n    (the zlib compression is used.)\n    \"\"\"\n    import zlib\n    decompressobj = zlib.decompressobj(-zlib.MAX_WBITS)", "suffix": "", "gt": "    return decompressobj.decompress(data)+decompressobj.flush()"}
{"prefix": "def get_content(url, headers={}, decoded=True):\n    \"\"\"Gets the content of a URL via sending a HTTP GET request.\n\n    Args:", "suffix": "        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n\n    logging.debug('get_content: %s' % url)\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n\n    response = urlopen_with_retry(req)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type', ''), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset, 'ignore')\n        else:\n            data = data.decode('utf-8', 'ignore')\n\n    return data", "gt": "        url: A URL."}
{"prefix": "def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n    \"\"\"Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n    if kwargs.get('post_data_raw'):\n        logging.debug('post_content: %s\\npost_data_raw: %s' % (url, kwargs['post_data_raw']))\n    else:\n        logging.debug('post_content: %s\\npost_data: %s' % (url, post_data))\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n    if kwargs.get('post_data_raw'):\n        post_data_enc = bytes(kwargs['post_data_raw'], 'utf-8')\n    else:\n        post_data_enc = bytes(parse.urlencode(post_data), 'utf-8')\n    response = urlopen_with_retry(req, data=post_data_enc)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type'), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset)\n        else:", "suffix": "\n    return data", "gt": "            data = data.decode('utf-8')"}
{"prefix": "def parse_host(host):\n    \"\"\"Parses host name and port number from a string.\n    \"\"\"", "suffix": "        return (\"0.0.0.0\", int(host))\n    if re.match(r'^(\\w+)://', host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)", "gt": "    if re.match(r'^(\\d+)$', host) is not None:"}
{"prefix": "def print_more_compatible(*args, **kwargs):\n    import builtins as __builtin__\n    \"\"\"Overload default print function as py (<3.3) does not support 'flush' keyword.\n    Although the function name can be same as print to get itself overloaded automatically,\n    I'd rather leave it with a different name and only overload it when importing to make less confusion.\n    \"\"\"\n    # nothing happens on py3.3 and later\n    if sys.version_info[:2] >= (3, 3):\n        return __builtin__.print(*args, **kwargs)\n\n    # in lower pyver (e.g. 3.2.x), remove 'flush' keyword and flush it as requested\n    doFlush = kwargs.pop('flush', False)", "suffix": "    if doFlush:\n        kwargs.get('file', sys.stdout).flush()\n    return ret", "gt": "    ret = __builtin__.print(*args, **kwargs)"}
{"prefix": "def showroom_get_roomid_by_room_url_key(room_url_key):\n    \"\"\"str->str\"\"\"\n    fake_headers_mobile = {\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n        'Accept-Charset': 'UTF-8,*;q=0.5',\n        'Accept-Encoding': 'gzip,deflate,sdch',\n        'Accept-Language': 'en-US,en;q=0.8',\n        'User-Agent': 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'\n    }\n    webpage_url = 'https://www.showroom-live.com/' + room_url_key\n    html = get_content(webpage_url, headers = fake_headers_mobile)\n    roomid = match1(html, r'room\\?room_id\\=(\\d+)')", "suffix": "    return roomid", "gt": "    assert roomid"}
{"prefix": "def showroom_download_by_room_id(room_id, output_dir = '.', merge = False, info_only = False, **kwargs):\n    '''Source: Android mobile'''\n    while True:\n        timestamp = str(int(time() * 1000))\n        api_endpoint = 'https://www.showroom-live.com/api/live/streaming_url?room_id={room_id}&_={timestamp}'.format(room_id = room_id, timestamp = timestamp)\n        html = get_content(api_endpoint)\n        html = json.loads(html)\n        #{'streaming_url_list': [{'url': 'rtmp://52.197.69.198:1935/liveedge', 'id': 1, 'label': 'original spec(low latency)', 'is_default': True, 'type': 'rtmp', 'stream_name': '7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed'}, {'url': 'http://52.197.69.198:1935/liveedge/7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed/playlist.m3u8', 'is_default': True, 'id': 2, 'type': 'hls', 'label': 'original spec'}, {'url': 'rtmp://52.197.69.198:1935/liveedge', 'id': 3, 'label': 'low spec(low latency)', 'is_default': False, 'type': 'rtmp', 'stream_name': '7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed_low'}, {'url': 'http://52.197.69.198:1935/liveedge/7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed_low/playlist.m3u8', 'is_default': False, 'id': 4, 'type': 'hls', 'label': 'low spec'}]}\n        if len(html) >= 1:\n            break\n        log.w('The live show is currently offline.')\n        sleep(1)\n\n    #This is mainly for testing the M3U FFmpeg parser so I would ignore any non-m3u ones\n    stream_url = [i['url'] for i in html['streaming_url_list'] if i['is_default'] and i['type'] == 'hls'][0]\n\n    assert stream_url\n\n    #title\n    title = ''\n    profile_api = 'https://www.showroom-live.com/api/room/profile?room_id={room_id}'.format(room_id = room_id)\n    html = loads(get_content(profile_api))\n    try:\n        title = html['main_name']\n    except KeyError:\n        title = 'Showroom_{room_id}'.format(room_id = room_id)\n\n    type_, ext, size = url_info(stream_url)\n    print_info(site_info, title, type_, size)", "suffix": "        download_url_ffmpeg(url=stream_url, title=title, ext= 'mp4', output_dir=output_dir)", "gt": "    if not info_only:"}
{"prefix": "def _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):\n    \"\"\"JSON, int, int, int->str\n    ", "suffix": "\n    return '_'.join([json_content[0]['name'],\n                    json_content[0]['Topics'][tIndex]['name'],\n                    json_content[0]['Topics'][tIndex]['Parts'][pIndex]['name']])", "gt": "    Get a proper title with courseid+topicID+partID.\"\"\""}
{"prefix": "", "suffix": "    \"\"\"int->None\n    \n    Download a WHOLE course.\n    Reuse the API call to save time.\"\"\"\n\n    for tIndex in range(len(json_api_content[0]['Topics'])):\n        for pIndex in range(len(json_api_content[0]['Topics'][tIndex]['Parts'])):\n            wanmen_download_by_course_topic_part(json_api_content,\n                                                 tIndex,\n                                                 pIndex,\n                                                 output_dir=output_dir,\n                                                 merge=merge,\n                                                 info_only=info_only,\n                                                 **kwargs)", "gt": "def wanmen_download_by_course(json_api_content, output_dir='.', merge=True, info_only=False, **kwargs):"}
{"prefix": "def wanmen_download_by_course_topic_part(json_api_content, tIndex, pIndex, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"int, int, int->None\n    \n    Download ONE PART of the course.\"\"\"\n\n    html = json_api_content\n\n    title = _wanmen_get_title_by_json_topic_part(html, \n                                                  tIndex, ", "suffix": "\n    bokeccID = _wanmen_get_boke_id_by_json_topic_part(html,\n                                                      tIndex, \n                                                     pIndex)\n\n    bokecc_download_by_id(vid = bokeccID, title = title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)", "gt": "                                                  pIndex)"}
{"prefix": "def get_streams_by_id(account_number, video_id):\n        \"\"\"\n        int, int->list\n        \n        Get the height of the videos.", "suffix": "        Since brightcove is using 3 kinds of links: rtmp, http and https,\n        we will be using the HTTPS one to make it secure.\n        \n        If somehow akamaihd.net is blocked by the Great Fucking Wall,\n        change the \"startswith https\" to http.\n        \"\"\"\n        endpoint = 'https://edge.api.brightcove.com/playback/v1/accounts/{account_number}/videos/{video_id}'.format(account_number = account_number, video_id = video_id)\n        fake_header_id = fake_headers\n        #is this somehow related to the time? Magic....\n        fake_header_id['Accept'] ='application/json;pk=BCpkADawqM1cc6wmJQC2tvoXZt4mrB7bFfi6zGt9QnOzprPZcGLE9OMGJwspQwKfuFYuCjAAJ53JdjI8zGFx1ll4rxhYJ255AXH1BQ10rnm34weknpfG-sippyQ'\n\n        html = get_content(endpoint, headers= fake_header_id)\n        html_json = json.loads(html)\n\n        link_list = []\n\n        for i in html_json['sources']:\n            if 'src' in i:  #to avoid KeyError\n                if i['src'].startswith('https'):\n                    link_list.append((str(i['height']), i['src']))\n\n        return link_list", "gt": "        "}
{"prefix": "def has_task(self, task_instance):\n        \"\"\"\n        Checks if a task is either queued or running in this executor\n\n        :param task_instance: TaskInstance\n        :return: True if the task is known to this executor\n        \"\"\"\n        if task_instance.key in self.queued_tasks or task_instance.key in self.running:", "suffix": "", "gt": "            return True"}
{"prefix": "def get_event_buffer(self, dag_ids=None):\n        \"\"\"\n        Returns and flush the event buffer. In case dag_ids is specified\n        it will only return and flush events for the given dag_ids. Otherwise", "suffix": "\n        :param dag_ids: to dag_ids to return events for, if None returns all\n        :return: a dict of events\n        \"\"\"\n        cleared_events = dict()\n        if dag_ids is None:\n            cleared_events = self.event_buffer\n            self.event_buffer = dict()\n        else:\n            for key in list(self.event_buffer.keys()):\n                dag_id, _, _, _ = key\n                if dag_id in dag_ids:\n                    cleared_events[key] = self.event_buffer.pop(key)\n\n        return cleared_events", "gt": "        it returns and flushes all"}
{"prefix": "def _get_conn_params(self):\n        \"\"\"\n        one method to fetch connection params as a dict\n        used in get_uri() and get_connection()\n        \"\"\"\n        conn = self.get_connection(self.snowflake_conn_id)\n        account = conn.extra_dejson.get('account', None)\n        warehouse = conn.extra_dejson.get('warehouse', None)\n        database = conn.extra_dejson.get('database', None)\n        region = conn.extra_dejson.get(\"region\", None)\n        role = conn.extra_dejson.get('role', None)\n\n        conn_config = {\n            \"user\": conn.login,\n            \"password\": conn.password or '',\n            \"schema\": conn.schema or '',\n            \"database\": self.database or database or '',\n            \"account\": self.account or account or '',\n            \"warehouse\": self.warehouse or warehouse or '',\n            \"region\": self.region or region or '',\n            \"role\": self.role or role or '',\n        }\n\n        \"\"\"\n        If private_key_file is specified in the extra json, load the contents of the file as a private\n        key and specify that in the connection configuration. The connection password then becomes the\n        passphrase for the private key. If your private key file is not encrypted (not recommended), then\n        leave the password empty.\n        \"\"\"\n        private_key_file = conn.extra_dejson.get('private_key_file', None)\n        if private_key_file:\n            with open(private_key_file, \"rb\") as key:\n                passphrase = None\n                if conn.password:\n                    passphrase = conn.password.strip().encode()\n\n                p_key = serialization.load_pem_private_key(\n                    key.read(),\n                    password=passphrase,\n                    backend=default_backend()\n                )\n\n            pkb = p_key.private_bytes(encoding=serialization.Encoding.DER,", "suffix": "                                      encryption_algorithm=serialization.NoEncryption())\n\n            conn_config['private_key'] = pkb\n            conn_config.pop('password', None)\n\n        return conn_config", "gt": "                                      format=serialization.PrivateFormat.PKCS8,"}
{"prefix": "def get_uri(self):\n        \"\"\"\n        override DbApiHook get_uri method for get_sqlalchemy_engine()", "suffix": "        conn_config = self._get_conn_params()\n        uri = 'snowflake://{user}:{password}@{account}/{database}/'\n        uri += '{schema}?warehouse={warehouse}&role={role}'\n        return uri.format(**conn_config)", "gt": "        \"\"\""}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Returns a snowflake.connection object", "suffix": "        conn_config = self._get_conn_params()\n        conn = snowflake.connector.connect(**conn_config)\n        return conn", "gt": "        \"\"\""}
{"prefix": "def _get_aws_credentials(self):\n        \"\"\"\n        returns aws_access_key_id, aws_secret_access_key\n        from extra\n\n        intended to be used by external import and export statements\n        \"\"\"\n        if self.snowflake_conn_id:\n            connection_object = self.get_connection(self.snowflake_conn_id)\n            if 'aws_secret_access_key' in connection_object.extra_dejson:\n                aws_access_key_id = connection_object.extra_dejson.get(\n                    'aws_access_key_id')", "suffix": "                    'aws_secret_access_key')\n        return aws_access_key_id, aws_secret_access_key", "gt": "                aws_secret_access_key = connection_object.extra_dejson.get("}
{"prefix": "def _get_field(self, field_name, default=None):\n        \"\"\"\n        Fetches a field from extras, and returns it. This is some Airflow\n        magic. The grpc hook type adds custom UI elements", "suffix": "        They get formatted as shown below.\n        \"\"\"\n        full_field_name = 'extra__grpc__{}'.format(field_name)\n        if full_field_name in self.extras:\n            return self.extras[full_field_name]\n        else:\n            return default", "gt": "        to the hook page, which allow admins to specify scopes, credential pem files, etc."}
{"prefix": "def copy_expert(self, sql, filename, open=open):\n        \"\"\"\n        Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.\n        \"\"\"\n        if not os.path.isfile(filename):\n            with open(filename, 'w'):\n                pass\n", "suffix": "            with closing(self.get_conn()) as conn:\n                with closing(conn.cursor()) as cur:\n                    cur.copy_expert(sql, f)\n                    f.truncate(f.tell())\n                    conn.commit()", "gt": "        with open(filename, 'r+') as f:"}
{"prefix": "", "suffix": "        \"\"\"\n        Loads a tab-delimited file into a database table\n        \"\"\"\n        self.copy_expert(\"COPY {table} FROM STDIN\".format(table=table), tmp_file)", "gt": "def bulk_load(self, table, tmp_file):"}
{"prefix": "", "suffix": "        \"\"\"\n        Dumps a database table into a tab-delimited file\n        \"\"\"\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)", "gt": "def bulk_dump(self, table, tmp_file):"}
{"prefix": "def execute(self, context):\n        \"\"\"\n        Uploads the file to Google cloud storage\n        \"\"\"\n        hook = GoogleCloudStorageHook(\n            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,\n            delegate_to=self.delegate_to)\n\n        hook.upload(\n            bucket_name=self.bucket,\n            object_name=self.dst,\n            mime_type=self.mime_type,", "suffix": "            gzip=self.gzip,\n        )", "gt": "            filename=self.src,"}
{"prefix": "def max_partition(\n        table, schema=\"default\", field=None, filter_map=None,\n        metastore_conn_id='metastore_default'):\n    \"\"\"\n    Gets the max partition for a table.\n\n    :param schema: The hive schema the table lives in\n    :type schema: str\n    :param table: The hive table you are interested in, supports the dot\n        notation as in \"my_database.my_table\", if a dot is found,\n        the schema param is disregarded\n    :type table: str\n    :param metastore_conn_id: The hive connection you are interested in.", "suffix": "    :type metastore_conn_id: str\n    :param filter_map: partition_key:partition_value map used for partition filtering,\n                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n                       Only partitions matching all partition_key:partition_value\n                       pairs will be considered as candidates of max partition.\n    :type filter_map: map\n    :param field: the field to get the max value from. If there's only\n        one partition field, this will be inferred\n    :type field: str\n\n    >>> max_partition('airflow.static_babynames_partitioned')\n    '2015-01-01'\n    \"\"\"\n    from airflow.hooks.hive_hooks import HiveMetastoreHook\n    if '.' in table:\n        schema, table = table.split('.')\n    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)\n    return hh.max_partition(\n        schema=schema, table_name=table, field=field, filter_map=filter_map)", "gt": "        If your default is set you don't need to use this parameter."}
{"prefix": "def _closest_date(target_dt, date_list, before_target=None):\n    \"\"\"\n    This function finds the date in a list closest to the target date.\n    An optional parameter can be given to get the closest before or after.\n\n    :param target_dt: The target date\n    :type target_dt: datetime.date\n    :param date_list: The list of dates to search\n    :type date_list: list[datetime.date]\n    :param before_target: closest before or after the target\n    :type before_target: bool or None\n    :returns: The closest date\n    :rtype: datetime.date or None\n    \"\"\"", "suffix": "    fa = lambda d: d - target_dt if d >= target_dt else datetime.timedelta.max\n    fnone = lambda d: target_dt - d if d < target_dt else d - target_dt\n    if before_target is None:\n        return min(date_list, key=fnone).date()\n    if before_target:\n        return min(date_list, key=fb).date()\n    else:\n        return min(date_list, key=fa).date()", "gt": "    fb = lambda d: target_dt - d if d <= target_dt else datetime.timedelta.max"}
{"prefix": "def closest_ds_partition(\n        table, ds, before=True, schema=\"default\",\n        metastore_conn_id='metastore_default'):\n    \"\"\"\n    This function finds the date in a list closest to the target date.\n    An optional parameter can be given to get the closest before or after.\n\n    :param table: A hive table name\n    :type table: str\n    :param ds: A datestamp ``%Y-%m-%d`` e.g. ``yyyy-mm-dd``\n    :type ds: list[datetime.date]\n    :param before: closest before (True), after (False) or either side of ds\n    :type before: bool or None\n    :returns: The closest date\n    :rtype: str or None\n\n    >>> tbl = 'airflow.static_babynames_partitioned'\n    >>> closest_ds_partition(tbl, '2015-01-02')\n    '2015-01-01'\n    \"\"\"\n    from airflow.hooks.hive_hooks import HiveMetastoreHook\n    if '.' in table:\n        schema, table = table.split('.')\n    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)\n    partitions = hh.get_partitions(schema=schema, table_name=table)\n    if not partitions:\n        return None\n    part_vals = [list(p.values())[0] for p in partitions]\n    if ds in part_vals:\n        return ds\n    else:\n        parts = [datetime.datetime.strptime(pv, '%Y-%m-%d')\n                 for pv in part_vals]\n        target_dt = datetime.datetime.strptime(ds, '%Y-%m-%d')\n        closest_ds = _closest_date(target_dt, parts, before_target=before)", "suffix": "", "gt": "        return closest_ds.isoformat()"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Returns a mysql connection object\n        \"\"\"\n        conn = self.get_connection(self.mysql_conn_id)\n        conn_config = {\n            \"user\": conn.login,\n            \"passwd\": conn.password or '',\n            \"host\": conn.host or 'localhost',\n            \"db\": self.schema or conn.schema or ''\n        }\n\n        if not conn.port:\n            conn_config[\"port\"] = 3306\n        else:\n            conn_config[\"port\"] = int(conn.port)\n\n        if conn.extra_dejson.get('charset', False):\n            conn_config[\"charset\"] = conn.extra_dejson[\"charset\"]\n            if (conn_config[\"charset\"]).lower() == 'utf8' or\\\n                    (conn_config[\"charset\"]).lower() == 'utf-8':\n                conn_config[\"use_unicode\"] = True\n        if conn.extra_dejson.get('cursor', False):\n            if (conn.extra_dejson[\"cursor\"]).lower() == 'sscursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'dictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.DictCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'ssdictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSDictCursor\n        local_infile = conn.extra_dejson.get('local_infile', False)", "suffix": "            # SSL parameter for MySQL has to be a dictionary and in case\n            # of extra/dejson we can get string if extra is passed via\n            # URL parameters\n            dejson_ssl = conn.extra_dejson['ssl']\n            if isinstance(dejson_ssl, six.string_types):\n                dejson_ssl = json.loads(dejson_ssl)\n            conn_config['ssl'] = dejson_ssl\n        if conn.extra_dejson.get('unix_socket'):\n            conn_config['unix_socket'] = conn.extra_dejson['unix_socket']\n        if local_infile:\n            conn_config[\"local_infile\"] = 1\n        conn = MySQLdb.connect(**conn_config)\n        return conn", "gt": "        if conn.extra_dejson.get('ssl', False):"}
{"prefix": "def bulk_load(self, table, tmp_file):\n        \"\"\"\n        Loads a tab-delimited file into a database table\n        \"\"\"\n        conn = self.get_conn()\n        cur = conn.cursor()", "suffix": "            LOAD DATA LOCAL INFILE '{tmp_file}'\n            INTO TABLE {table}\n            \"\"\".format(tmp_file=tmp_file, table=table))\n        conn.commit()", "gt": "        cur.execute(\"\"\""}
{"prefix": "def is_bucket_updated(self, current_num_objects):\n        \"\"\"\n        Checks whether new objects have been uploaded and the inactivity_period\n        has passed and updates the state of the sensor accordingly.\n\n        :param current_num_objects: number of objects in bucket during last poke.", "suffix": "        \"\"\"\n\n        if current_num_objects > self.previous_num_objects:\n            # When new objects arrived, reset the inactivity_seconds\n            # previous_num_objects for the next poke.\n            self.log.info(\n                '''\n                New objects found at {} resetting last_activity_time.\n                '''.format(os.path.join(self.bucket, self.prefix)))\n            self.last_activity_time = get_time()\n            self.inactivity_seconds = 0\n            self.previous_num_objects = current_num_objects\n        elif current_num_objects < self.previous_num_objects:\n            # During the last poke interval objects were deleted.\n            if self.allow_delete:\n                self.previous_num_objects = current_num_objects\n                self.last_activity_time = get_time()\n                self.log.warning(\n                    '''\n                    Objects were deleted during the last\n                    poke interval. Updating the file counter and\n                    resetting last_activity_time.\n                    '''\n                )\n            else:\n                raise RuntimeError(\n                    '''\n                    Illegal behavior: objects were deleted in {} between pokes.\n                    '''.format(os.path.join(self.bucket, self.prefix))\n                )\n        else:\n            if self.last_activity_time:\n                self.inactivity_seconds = (\n                    get_time() - self.last_activity_time).total_seconds()\n            else:\n                # Handles the first poke where last inactivity time is None.\n                self.last_activity_time = get_time()\n                self.inactivity_seconds = 0\n\n            if self.inactivity_seconds >= self.inactivity_period:\n                if current_num_objects >= self.min_objects:\n                    self.log.info(\n                        '''\n                        SUCCESS:\n                        Sensor found {} objects at {}.\n                        Waited at least {} seconds, with no new objects dropped.\n                        '''.format(\n                            current_num_objects,\n                            os.path.join(self.bucket, self.prefix),\n                            self.inactivity_period))\n                    return True\n\n                warn_msg = \\\n                    '''\n                    FAILURE:\n                    Inactivity Period passed,\n                    not enough objects found in {}\n                    '''.format(\n                        os.path.join(self.bucket, self.prefix))\n                self.log.warning(warn_msg)\n                return False\n            return False", "gt": "        :type current_num_objects: int"}
{"prefix": "def sigquit_handler(sig, frame):\n    \"\"\"Helps debug deadlocks by printing stacktraces when this gets a SIGQUIT\n    e.g. kill -s QUIT <PID> or CTRL+\\\n    \"\"\"\n    print(\"Dumping stack traces for all threads in PID {}\".format(os.getpid()))\n    id_to_name = dict([(th.ident, th.name) for th in threading.enumerate()])\n    code = []\n    for thread_id, stack in sys._current_frames().items():\n        code.append(\"\\n# Thread: {}({})\"\n                    .format(id_to_name.get(thread_id, \"\"), thread_id))\n        for filename, line_number, name, line in traceback.extract_stack(stack):\n            code.append('File: \"{}\", line {}, in {}'", "suffix": "            if line:\n                code.append(\"  {}\".format(line.strip()))\n    print(\"\\n\".join(code))", "gt": "                        .format(filename, line_number, name))"}
{"prefix": "def trigger_dag(args):\n    \"\"\"", "suffix": "    :param args:\n    :return:\n    \"\"\"\n    log = LoggingMixin().log\n    try:\n        message = api_client.trigger_dag(dag_id=args.dag_id,\n                                         run_id=args.run_id,\n                                         conf=args.conf,\n                                         execution_date=args.exec_date)\n    except IOError as err:\n        log.error(err)\n        raise AirflowException(err)\n    log.info(message)", "gt": "    Creates a dag run for the specified dag"}
{"prefix": "def delete_dag(args):\n    \"\"\"\n    Deletes all DB records related to the specified dag\n    :param args:\n    :return:\n    \"\"\"\n    log = LoggingMixin().log\n    if args.yes or input(\n            \"This will drop all existing records related to the specified DAG. \"\n            \"Proceed? (y/n)\").upper() == \"Y\":\n        try:\n            message = api_client.delete_dag(dag_id=args.dag_id)\n        except IOError as err:", "suffix": "            raise AirflowException(err)\n        log.info(message)\n    else:\n        print(\"Bail.\")", "gt": "            log.error(err)"}
{"prefix": "def task_failed_deps(args):\n    \"\"\"\n    Returns the unmet dependencies for a task instance from the perspective of the\n    scheduler (i.e. why a task instance doesn't get scheduled and then queued by the\n    scheduler, and then run by an executor).\n    >>> airflow task_failed_deps tutorial sleep 2015-01-01\n    Task instance dependencies not met:\n    Dagrun Running: Task instance's dagrun did not exist: Unknown reason", "suffix": "    to have succeeded, but found 1 non-success(es).\n    \"\"\"\n    dag = get_dag(args)\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n\n    dep_context = DepContext(deps=SCHEDULER_DEPS)\n    failed_deps = list(ti.get_failed_dep_statuses(dep_context=dep_context))\n    # TODO, Do we want to print or log this\n    if failed_deps:\n        print(\"Task instance dependencies not met:\")\n        for dep in failed_deps:\n            print(\"{}: {}\".format(dep.dep_name, dep.reason))\n    else:\n        print(\"Task instance dependencies are all met.\")", "gt": "    Trigger Rule: Task's trigger rule 'all_success' requires all upstream tasks"}
{"prefix": "def task_state(args):\n    \"\"\"\n    Returns the state of a TaskInstance at the command line.\n    >>> airflow task_state tutorial sleep 2015-01-01\n    success", "suffix": "    dag = get_dag(args)\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n    print(ti.current_state())", "gt": "    \"\"\""}
{"prefix": "def dag_state(args):\n    \"\"\"\n    Returns the state of a DagRun at the command line.\n    >>> airflow dag_state tutorial 2015-01-01T00:00:00.000000", "suffix": "    \"\"\"\n    dag = get_dag(args)\n    dr = DagRun.find(dag.dag_id, execution_date=args.execution_date)\n    print(dr[0].state if len(dr) > 0 else None)", "gt": "    running"}
{"prefix": "def next_execution(args):\n    \"\"\"\n    Returns the next execution datetime of a DAG at the command line.\n    >>> airflow next_execution tutorial\n    2018-08-31 10:38:00\n    \"\"\"\n    dag = get_dag(args)\n\n    if dag.is_paused:\n        print(\"[INFO] Please be reminded this DAG is PAUSED now.\")\n\n    if dag.latest_execution_date:\n        next_execution_dttm = dag.following_schedule(dag.latest_execution_date)\n\n        if next_execution_dttm is None:\n            print(\"[WARN] No following schedule can be found. \" +\n                  \"This DAG may have schedule interval '@once' or `None`.\")\n\n        print(next_execution_dttm)\n    else:", "suffix": "        print(None)", "gt": "        print(\"[WARN] Only applicable when there is execution record found for the DAG.\")"}
{"prefix": "def restart_workers(gunicorn_master_proc, num_workers_expected, master_timeout):\n    \"\"\"\n    Runs forever, monitoring the child processes of @gunicorn_master_proc and\n    restarting workers occasionally.\n    Each iteration of the loop traverses one edge of this state transition\n    diagram, where each state (node) represents\n    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n    The horizontal transition at ? happens after the new worker parses all the\n    dags (so it could take a while!)\n       V \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    [n / n] \u2500\u2500TTIN\u2500\u2500> [ [n, n+bs) / n + bs ]  \u2500\u2500\u2500\u2500?\u2500\u2500\u2500> [n + bs / n + bs] \u2500\u2500TTOU\u2500\u2518\n       ^                          ^\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "suffix": "       \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500 [ [0, n) / n ] <\u2500\u2500\u2500 start\n    We change the number of workers by sending TTIN and TTOU to the gunicorn\n    master process, which increases and decreases the number of child workers\n    respectively. Gunicorn guarantees that on TTOU workers are terminated\n    gracefully and that the oldest worker is terminated.\n    \"\"\"\n\n    def wait_until_true(fn, timeout=0):\n        \"\"\"\n        Sleeps until fn is true\n        \"\"\"\n        t = time.time()\n        while not fn():\n            if 0 < timeout <= time.time() - t:\n                raise AirflowWebServerTimeout(\n                    \"No response from gunicorn master within {0} seconds\"\n                    .format(timeout))\n            time.sleep(0.1)\n\n    def start_refresh(gunicorn_master_proc):\n        batch_size = conf.getint('webserver', 'worker_refresh_batch_size')\n        log.debug('%s doing a refresh of %s workers', state, batch_size)\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n        excess = 0\n        for _ in range(batch_size):\n            gunicorn_master_proc.send_signal(signal.SIGTTIN)\n            excess += 1\n            wait_until_true(lambda: num_workers_expected + excess ==\n                            get_num_workers_running(gunicorn_master_proc),\n                            master_timeout)\n\n    try:\n        wait_until_true(lambda: num_workers_expected ==\n                        get_num_workers_running(gunicorn_master_proc),\n                        master_timeout)\n        while True:\n            num_workers_running = get_num_workers_running(gunicorn_master_proc)\n            num_ready_workers_running = \\\n                get_num_ready_workers_running(gunicorn_master_proc)\n\n            state = '[{0} / {1}]'.format(num_ready_workers_running, num_workers_running)\n\n            # Whenever some workers are not ready, wait until all workers are ready\n            if num_ready_workers_running < num_workers_running:\n                log.debug('%s some workers are starting up, waiting...', state)\n                sys.stdout.flush()\n                time.sleep(1)\n\n            # Kill a worker gracefully by asking gunicorn to reduce number of workers\n            elif num_workers_running > num_workers_expected:\n                excess = num_workers_running - num_workers_expected\n                log.debug('%s killing %s workers', state, excess)\n\n                for _ in range(excess):\n                    gunicorn_master_proc.send_signal(signal.SIGTTOU)\n                    excess -= 1\n                    wait_until_true(lambda: num_workers_expected + excess ==\n                                    get_num_workers_running(gunicorn_master_proc),\n                                    master_timeout)\n\n            # Start a new worker by asking gunicorn to increase number of workers\n            elif num_workers_running == num_workers_expected:\n                refresh_interval = conf.getint('webserver', 'worker_refresh_interval')\n                log.debug(\n                    '%s sleeping for %ss starting doing a refresh...',\n                    state, refresh_interval\n                )\n                time.sleep(refresh_interval)\n                start_refresh(gunicorn_master_proc)\n\n            else:\n                # num_ready_workers_running == num_workers_running < num_workers_expected\n                log.error((\n                    \"%s some workers seem to have died and gunicorn\"\n                    \"did not restart them as expected\"\n                ), state)\n                time.sleep(10)\n                if len(\n                    psutil.Process(gunicorn_master_proc.pid).children()\n                ) < num_workers_expected:\n                    start_refresh(gunicorn_master_proc)\n    except (AirflowWebServerTimeout, OSError) as err:\n        log.error(err)\n        log.error(\"Shutting down webserver\")\n        try:\n            gunicorn_master_proc.terminate()\n            gunicorn_master_proc.wait()\n        finally:\n            sys.exit(1)", "gt": "       \u2502"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Translate\n\n        :return: Google Cloud Translate client object.\n        :rtype: Client\n        \"\"\"\n        if not self._client:\n            self._client = Client(credentials=self._get_credentials())", "suffix": "", "gt": "        return self._client"}
{"prefix": "def translate(\n        self, values, target_language, format_=None, source_language=None, model=None\n    ):\n        \"\"\"Translate a string or list of strings.\n\n        See https://cloud.google.com/translate/docs/translating-text\n\n        :type values: str or list\n        :param values: String or list of strings to translate.\n", "suffix": "        :param target_language: The language to translate results into. This\n                                is required by the API and defaults to\n                                the target language of the current instance.\n\n        :type format_: str\n        :param format_: (Optional) One of ``text`` or ``html``, to specify\n                        if the input text is plain text or HTML.\n\n        :type source_language: str or None\n        :param source_language: (Optional) The language of the text to\n                                be translated.\n\n        :type model: str or None\n        :param model: (Optional) The model used to translate the text, such\n                      as ``'base'`` or ``'nmt'``.\n\n        :rtype: str or list\n        :returns: A list of dictionaries for each queried value. Each\n                  dictionary typically contains three keys (though not\n                  all will be present in all cases)\n\n                  * ``detectedSourceLanguage``: The detected language (as an\n                    ISO 639-1 language code) of the text.\n                  * ``translatedText``: The translation of the text into the\n                    target language.\n                  * ``input``: The corresponding input value.\n                  * ``model``: The model used to translate the text.\n\n                  If only a single value is passed, then only a single\n                  dictionary will be returned.\n        :raises: :class:`~exceptions.ValueError` if the number of\n                 values and translations differ.\n        \"\"\"\n        client = self.get_conn()\n\n        return client.translate(\n            values=values,\n            target_language=target_language,\n            format_=format_,\n            source_language=source_language,\n            model=model,\n        )", "gt": "        :type target_language: str"}
{"prefix": "def execute(self, context):\n        \"\"\"\n        Execute the bash command in a temporary directory\n        which will be cleaned afterwards\n        \"\"\"\n        self.log.info('Tmp dir root location: \\n %s', gettempdir())\n\n        # Prepare env for child process.\n        if self.env is None:\n            self.env = os.environ.copy()\n\n        airflow_context_vars = context_to_airflow_vars(context, in_env_var_format=True)\n        self.log.info('Exporting the following env vars:\\n%s',\n                      '\\n'.join([\"{}={}\".format(k, v)\n                                 for k, v in\n                                 airflow_context_vars.items()]))\n        self.env.update(airflow_context_vars)\n\n        self.lineage_data = self.bash_command\n\n        with TemporaryDirectory(prefix='airflowtmp') as tmp_dir:\n            with NamedTemporaryFile(dir=tmp_dir, prefix=self.task_id) as tmp_file:\n                tmp_file.write(bytes(self.bash_command, 'utf_8'))\n                tmp_file.flush()\n                script_location = os.path.abspath(tmp_file.name)", "suffix": "\n                def pre_exec():\n                    # Restore default signal disposition and invoke setsid\n                    for sig in ('SIGPIPE', 'SIGXFZ', 'SIGXFSZ'):\n                        if hasattr(signal, sig):\n                            signal.signal(getattr(signal, sig), signal.SIG_DFL)\n                    os.setsid()\n\n                self.log.info('Running command: %s', self.bash_command)\n                sub_process = Popen(\n                    ['bash', tmp_file.name],\n                    stdout=PIPE,\n                    stderr=STDOUT,\n                    cwd=tmp_dir,\n                    env=self.env,\n                    preexec_fn=pre_exec)\n\n                self.sub_process = sub_process\n\n                self.log.info('Output:')\n                line = ''\n                for raw_line in iter(sub_process.stdout.readline, b''):\n                    line = raw_line.decode(self.output_encoding).rstrip()\n                    self.log.info(line)\n\n                sub_process.wait()\n\n                self.log.info('Command exited with return code %s', sub_process.returncode)\n\n                if sub_process.returncode:\n                    raise AirflowException('Bash command failed')\n\n        return line", "gt": "                self.log.info('Temporary script location: %s', script_location)"}
{"prefix": "def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.", "suffix": "        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().instances().get(\n            project=project_id,\n            instance=instance\n        ).execute(num_retries=self.num_retries)", "gt": "        :type instance: str"}
{"prefix": "def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set", "suffix": "        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().insert(\n            project=project_id,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)", "gt": "            to None or missing, the default project_id from the GCP connection is used."}
{"prefix": "def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().patch(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,", "suffix": "", "gt": "                                             operation_name=operation_name)"}
{"prefix": "def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"", "suffix": "            project=project_id,\n            instance=instance,\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)", "gt": "        response = self.get_conn().instances().delete("}
{"prefix": "def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().databases().get(\n            project=project_id,\n            instance=instance,\n            database=database", "suffix": "", "gt": "        ).execute(num_retries=self.num_retries)"}
{"prefix": "def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict", "suffix": "            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().insert(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)", "gt": "        :param project_id: Project ID of the project that contains the instance. If set"}
{"prefix": "def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().patch(\n            project=project_id,", "suffix": "            database=database,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)", "gt": "            instance=instance,"}
{"prefix": "def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().delete(\n            project=project_id,\n            instance=instance,\n            database=database", "suffix": "        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)", "gt": "        ).execute(num_retries=self.num_retries)"}
{"prefix": "def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.", "suffix": "        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = self.get_conn().instances().export(\n                project=project_id,\n                instance=instance,\n                body=body\n            ).execute(num_retries=self.num_retries)\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(project_id=project_id,\n                                                 operation_name=operation_name)\n        except HttpError as ex:\n            raise AirflowException(\n                'Exporting instance {} failed: {}'.format(instance, ex.content)\n            )", "gt": ""}
{"prefix": "def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = service.operations().get(\n                project=project_id,\n                operation=operation_name,\n            ).execute(num_retries=self.num_retries)\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]", "suffix": "                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)", "gt": "                    raise AirflowException(error_msg)"}
{"prefix": "def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is already running: {}\".format(\n                self.sql_proxy_process))\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\"Creating directory %s\",\n                              self.cloud_sql_proxy_socket_directory)\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(command_to_run,\n                                           stdin=PIPE, stdout=PIPE, stderr=PIPE)\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode('utf-8')", "suffix": "                if line == '' and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code))\n                if line != '':\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(\n                            line))\n                if \"Ready for new connections\" in line:\n                    return", "gt": "                return_code = self.sql_proxy_process.poll()"}
{"prefix": "def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\"Stopping the cloud_sql_proxy pid: %s\",\n                          self.sql_proxy_process.pid)\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!", "suffix": "                      self.cloud_sql_proxy_socket_directory)\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\"Skipped removing proxy - it was not downloaded: %s\",\n                          self.sql_proxy_path)\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\"Removing generated credentials file %s\",\n                          self.credentials_path)\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)", "gt": "        self.log.info(\"Removing the socket directory: %s\","}
{"prefix": "def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend(['--version'])", "suffix": "        result = subprocess.check_output(command_to_run).decode('utf-8')\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None", "gt": "        command_to_run.extend(self._get_credential_parameters())"}
{"prefix": "", "suffix": "        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()", "gt": "def create_connection(self, session=None):"}
{"prefix": "", "suffix": "        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            return connections[0]\n        return None", "gt": "def retrieve_connection(self, session=None):"}
{"prefix": "def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.", "suffix": "        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")", "gt": ""}
{"prefix": "def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner", "suffix": "        if not self.use_proxy:\n            raise AirflowException(\"Proxy runner can only be retrieved in case of use_proxy = True\")\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path\n        )", "gt": "        \"\"\""}
{"prefix": "def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == 'postgres':", "suffix": "                                        schema=self.database)\n        else:\n            self.db_hook = MySqlHook(mysql_conn_id=self.db_conn_id,\n                                     schema=self.database)\n        return self.db_hook", "gt": "            self.db_hook = PostgresHook(postgres_conn_id=self.db_conn_id,"}
{"prefix": "def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == 'postgres':\n            if hasattr(self.db_hook,\n                       'conn') and self.db_hook.conn and self.db_hook.conn.notices:\n                for output in self.db_hook.conn.notices:", "suffix": "", "gt": "                    self.log.info(output)"}
{"prefix": "def reserve_free_tcp_port(self):\n        \"\"\"", "suffix": "        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind(('127.0.0.1', 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]", "gt": "        Reserve free TCP port to be used by Cloud SQL Proxy"}
{"prefix": "def _normalize_mlengine_job_id(job_id):\n    \"\"\"\n    Replaces invalid MLEngine job_id characters with '_'.\n\n    This also adds a leading 'z' in case job_id starts with an invalid\n    character.\n\n    Args:\n        job_id: A job_id str that may have invalid characters.\n\n    Returns:\n        A valid job_id representation.\n    \"\"\"\n\n    # Add a prefix when a job_id starts with a digit or a template\n    match = re.search(r'\\d|\\{{2}', job_id)\n    if match and match.start() == 0:\n        job = 'z_{}'.format(job_id)\n    else:\n        job = job_id\n\n    # Clean up 'bad' characters except templates\n    tracker = 0\n    cleansed_job_id = ''\n    for m in re.finditer(r'\\{{2}.+?\\}{2}', job):\n        cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_',\n                                  job[tracker:m.start()])\n        cleansed_job_id += job[m.start():m.end()]\n        tracker = m.end()\n\n    # Clean up last substring or the full string if no templates\n    cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_', job[tracker:])\n", "suffix": "", "gt": "    return cleansed_job_id"}
{"prefix": "def _get_error_code(self, e):\n        \"\"\"Extract error code from ftp exception\"\"\"\n        try:\n            matches = self.error_code_pattern.match(str(e))\n            code = int(matches.group(0))\n            return code", "suffix": "            return e", "gt": "        except ValueError:"}
{"prefix": "", "suffix": "    \"\"\"Integrate plugins to the context\"\"\"\n    import sys\n    from airflow.plugins_manager import sensors_modules\n    for sensors_module in sensors_modules:\n        sys.modules[sensors_module.__name__] = sensors_module\n        globals()[sensors_module._name] = sensors_module", "gt": "def _integrate_plugins():"}
{"prefix": "def clear_dag_runs():\n    \"\"\"\n    Remove any existing DAG runs for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    drs = session.query(DagRun).filter(", "suffix": "    ).all()\n    for dr in drs:\n        logging.info('Deleting DagRun :: {}'.format(dr))\n        session.delete(dr)", "gt": "        DagRun.dag_id.in_(DAG_IDS),"}
{"prefix": "def clear_dag_task_instances():\n    \"\"\"\n    Remove any existing task instances for the perf test DAGs.\n    \"\"\"", "suffix": "    TI = TaskInstance\n    tis = (\n        session\n        .query(TI)\n        .filter(TI.dag_id.in_(DAG_IDS))\n        .all()\n    )\n    for ti in tis:\n        logging.info('Deleting TaskInstance :: {}'.format(ti))\n        session.delete(ti)\n    session.commit()", "gt": "    session = settings.Session()"}
{"prefix": "def set_dags_paused_state(is_paused):\n    \"\"\"\n    Toggle the pause state of the DAGs in the test.", "suffix": "    session = settings.Session()\n    dms = session.query(DagModel).filter(\n        DagModel.dag_id.in_(DAG_IDS))\n    for dm in dms:\n        logging.info('Setting DAG :: {} is_paused={}'.format(dm, is_paused))\n        dm.is_paused = is_paused\n    session.commit()", "gt": "    \"\"\""}
{"prefix": "def print_stats(self):\n        \"\"\"\n        Print operational metrics for the scheduler test.\n        \"\"\"\n        session = settings.Session()\n        TI = TaskInstance\n        tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .all()\n        )\n        successful_tis = [x for x in tis if x.state == State.SUCCESS]\n        ti_perf = [(ti.dag_id, ti.task_id, ti.execution_date,\n                    (ti.queued_dttm - self.start_date).total_seconds(),\n                    (ti.start_date - self.start_date).total_seconds(),\n                    (ti.end_date - self.start_date).total_seconds(),\n                    ti.duration) for ti in successful_tis]\n        ti_perf_df = pd.DataFrame(ti_perf, columns=['dag_id', 'task_id',\n                                                    'execution_date',\n                                                    'queue_delay',\n                                                    'start_delay', 'land_time',\n                                                    'duration'])", "suffix": "        print('Performance Results')\n        print('###################')\n        for dag_id in DAG_IDS:\n            print('DAG {}'.format(dag_id))\n            print(ti_perf_df[ti_perf_df['dag_id'] == dag_id])\n        print('###################')\n        if len(tis) > len(successful_tis):\n            print(\"WARNING!! The following task instances haven't completed\")\n            print(pd.DataFrame([(ti.dag_id, ti.task_id, ti.execution_date, ti.state)\n                  for ti in filter(lambda x: x.state != State.SUCCESS, tis)],\n                  columns=['dag_id', 'task_id', 'execution_date', 'state']))\n\n        session.commit()", "gt": ""}
{"prefix": "", "suffix": "        \"\"\"\n        Override the scheduler heartbeat to determine when the test is complete\n        \"\"\"\n        super(SchedulerMetricsJob, self).heartbeat()\n        session = settings.Session()\n        # Get all the relevant task instances\n        TI = TaskInstance\n        successful_tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .filter(TI.state.in_([State.SUCCESS]))\n            .all()\n        )\n        session.commit()\n\n        dagbag = DagBag(SUBDIR)\n        dags = [dagbag.dags[dag_id] for dag_id in DAG_IDS]\n        # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.\n        num_task_instances = sum([(timezone.utcnow() - task.start_date).days\n                                 for dag in dags for task in dag.tasks])\n\n        if (len(successful_tis) == num_task_instances or\n                (timezone.utcnow() - self.start_date).total_seconds() >\n                MAX_RUNTIME_SECS):\n            if len(successful_tis) == num_task_instances:\n                self.log.info(\"All tasks processed! Printing stats.\")\n            else:\n                self.log.info(\"Test timeout reached. Printing available stats.\")\n            self.print_stats()\n            set_dags_paused_state(True)\n            sys.exit()", "gt": "def heartbeat(self):"}
{"prefix": "def invoke_lambda(self, payload):\n        \"\"\"\n        Invoke Lambda Function\n        \"\"\"\n\n        awslambda_conn = self.get_conn()", "suffix": "        response = awslambda_conn.invoke(\n            FunctionName=self.function_name,\n            InvocationType=self.invocation_type,\n            LogType=self.log_type,\n            Payload=payload,\n            Qualifier=self.qualifier\n        )\n\n        return response", "gt": ""}
{"prefix": "def get_dag_run_state(dag_id, execution_date):\n    \"\"\"Return the task object identified by the given dag_id and task_id.\"\"\"\n\n    dagbag = DagBag()\n\n    # Check DAG exists.\n    if dag_id not in dagbag.dags:\n        error_message = \"Dag id {} not found\".format(dag_id)\n        raise DagNotFound(error_message)\n\n    # Get DAG object and check Task Exists\n    dag = dagbag.get_dag(dag_id)", "suffix": "    # Get DagRun object and check that it exists\n    dagrun = dag.get_dagrun(execution_date=execution_date)\n    if not dagrun:\n        error_message = ('Dag Run for date {} not found in dag {}'\n                         .format(execution_date, dag_id))\n        raise DagRunNotFound(error_message)\n\n    return {'state': dagrun.get_state()}", "gt": ""}
{"prefix": "def create_evaluate_ops(task_prefix,\n                        data_format,\n                        input_paths,\n                        prediction_path,\n                        metric_fn_and_keys,\n                        validate_fn,\n                        batch_prediction_job_id=None,\n                        project_id=None,\n                        region=None,\n                        dataflow_options=None,\n                        model_uri=None,\n                        model_name=None,\n                        version_name=None,\n                        dag=None):\n    \"\"\"\n    Creates Operators needed for model evaluation and returns.\n\n    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n    calling MLEngineBatchPredictionOperator, then summarize and validate\n    the result via Cloud Dataflow using DataFlowPythonOperator.\n\n    For details and pricing about Batch prediction, please refer to the website\n    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n\n    It returns three chained operators for prediction, summary, and validation,\n    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n    respectively.\n    (<prefix> should contain only alphanumeric characters or hyphen.)\n\n    The upstream and downstream can be set accordingly like:\n      pred, _, val = create_evaluate_ops(...)\n      pred.set_upstream(upstream_op)\n      ...\n      downstream_op.set_upstream(val)\n\n    Callers will provide two python callables, metric_fn and validate_fn, in\n    order to customize the evaluation behavior as they wish.\n    - metric_fn receives a dictionary per instance derived from json in the\n      batch prediction result. The keys might vary depending on the model.\n      It should return a tuple of metrics.\n    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n      generated over all instances.\n      The key/value of the dictionary matches to what's given by\n      metric_fn_and_keys arg.\n      The dictionary contains an additional metric, 'count' to represent the\n      total number of instances received for evaluation.\n      The function would raise an exception to mark the task as failed, in a\n      case the validation result is not okay to proceed (i.e. to set the trained\n      version as default).\n\n    Typical examples are like this:\n\n    def get_metric_fn_and_keys():\n        import math  # imports should be outside of the metric_fn below.\n        def error_and_squared_error(inst):\n            label = float(inst['input_label'])\n            classes = float(inst['classes'])  # 0 or 1\n            err = abs(classes-label)\n            squared_err = math.pow(classes-label, 2)\n            return (err, squared_err)  # returns a tuple.\n        return error_and_squared_error, ['err', 'mse']  # key order must match.\n\n    def validate_err_and_count(summary):\n        if summary['err'] > 0.2:\n            raise ValueError('Too high err>0.2; summary=%s' % summary)\n        if summary['mse'] > 0.05:\n            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n        if summary['count'] < 1000:\n            raise ValueError('Too few instances<1000; summary=%s' % summary)\n        return summary\n\n    For the details on the other BatchPrediction-related arguments (project_id,\n    job_id, region, data_format, input_paths, prediction_path, model_uri),\n    please refer to MLEngineBatchPredictionOperator too.\n\n    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and\n        hyphen are allowed (no underscores), since this will be used as dataflow\n        job name, which doesn't allow other characters.\n    :type task_prefix: str\n\n    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n    :type data_format: str\n\n    :param input_paths: a list of input paths to be sent to BatchPrediction.\n    :type input_paths: list[str]\n\n    :param prediction_path: GCS path to put the prediction results in.\n    :type prediction_path: str\n\n    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:\n        - metric_fn is a function that accepts a dictionary (for an instance),\n          and returns a tuple of metric(s) that it calculates.\n        - metric_keys is a list of strings to denote the key of each metric.\n    :type metric_fn_and_keys: tuple of a function and a list[str]\n\n    :param validate_fn: a function to validate whether the averaged metric(s) is\n        good enough to push the model.\n    :type validate_fn: function\n\n    :param batch_prediction_job_id: the id to use for the Cloud ML Batch\n        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n        the job_id argument.\n    :type batch_prediction_job_id: str\n\n    :param project_id: the Google Cloud Platform project id in which to execute\n        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['project_id']` will be used.\n    :type project_id: str\n\n    :param region: the Google Cloud Platform region in which to execute Cloud ML\n        Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['region']` will be used.\n    :type region: str\n\n    :param dataflow_options: options to run Dataflow jobs. If None, then the\n        `dag`'s `default_args['dataflow_default_options']` will be used.\n    :type dataflow_options: dictionary\n\n    :param model_uri: GCS path of the model exported by Tensorflow using\n        tensorflow.estimator.export_savedmodel(). It cannot be used with\n        model_name or version_name below. See MLEngineBatchPredictionOperator for\n        more detail.\n    :type model_uri: str\n\n    :param model_name: Used to indicate a model to use for prediction. Can be\n        used in combination with version_name, but cannot be used together with\n        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,\n        then the `dag`'s `default_args['model_name']` will be used.\n    :type model_name: str\n\n    :param version_name: Used to indicate a model version to use for prediction,\n        in combination with model_name. Cannot be used together with model_uri.\n        See MLEngineBatchPredictionOperator for more detail. If None, then the\n        `dag`'s `default_args['version_name']` will be used.\n    :type version_name: str\n\n    :param dag: The `DAG` to use for all Operators.\n    :type dag: airflow.models.DAG\n\n    :returns: a tuple of three operators, (prediction, summary, validation)\n    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n                  PythonOperator)\n    \"\"\"\n\n    # Verify that task_prefix doesn't have any special characters except hyphen\n    # '-', which is the only allowed non-alphanumeric character by Dataflow.\n    if not re.match(r\"^[a-zA-Z][-A-Za-z0-9]*$\", task_prefix):\n        raise AirflowException(\n            \"Malformed task_id for DataFlowPythonOperator (only alphanumeric \"\n            \"and hyphens are allowed but got: \" + task_prefix)\n\n    metric_fn, metric_keys = metric_fn_and_keys", "suffix": "        raise AirflowException(\"`metric_fn` param must be callable.\")\n    if not callable(validate_fn):\n        raise AirflowException(\"`validate_fn` param must be callable.\")\n\n    if dag is not None and dag.default_args is not None:\n        default_args = dag.default_args\n        project_id = project_id or default_args.get('project_id')\n        region = region or default_args.get('region')\n        model_name = model_name or default_args.get('model_name')\n        version_name = version_name or default_args.get('version_name')\n        dataflow_options = dataflow_options or \\\n            default_args.get('dataflow_default_options')\n\n    evaluate_prediction = MLEngineBatchPredictionOperator(\n        task_id=(task_prefix + \"-prediction\"),\n        project_id=project_id,\n        job_id=batch_prediction_job_id,\n        region=region,\n        data_format=data_format,\n        input_paths=input_paths,\n        output_path=prediction_path,\n        uri=model_uri,\n        model_name=model_name,\n        version_name=version_name,\n        dag=dag)\n\n    metric_fn_encoded = base64.b64encode(dill.dumps(metric_fn, recurse=True))\n    evaluate_summary = DataFlowPythonOperator(\n        task_id=(task_prefix + \"-summary\"),\n        py_options=[\"-m\"],\n        py_file=\"airflow.contrib.utils.mlengine_prediction_summary\",\n        dataflow_default_options=dataflow_options,\n        options={\n            \"prediction_path\": prediction_path,\n            \"metric_fn_encoded\": metric_fn_encoded,\n            \"metric_keys\": ','.join(metric_keys)\n        },\n        dag=dag)\n    evaluate_summary.set_upstream(evaluate_prediction)\n\n    def apply_validate_fn(*args, **kwargs):\n        prediction_path = kwargs[\"templates_dict\"][\"prediction_path\"]\n        scheme, bucket, obj, _, _ = urlsplit(prediction_path)\n        if scheme != \"gs\" or not bucket or not obj:\n            raise ValueError(\"Wrong format prediction_path: %s\",\n                             prediction_path)\n        summary = os.path.join(obj.strip(\"/\"),\n                               \"prediction.summary.json\")\n        gcs_hook = GoogleCloudStorageHook()\n        summary = json.loads(gcs_hook.download(bucket, summary))\n        return validate_fn(summary)\n\n    evaluate_validation = PythonOperator(\n        task_id=(task_prefix + \"-validation\"),\n        python_callable=apply_validate_fn,\n        provide_context=True,\n        templates_dict={\"prediction_path\": prediction_path},\n        dag=dag)\n    evaluate_validation.set_upstream(evaluate_summary)\n\n    return evaluate_prediction, evaluate_summary, evaluate_validation", "gt": "    if not callable(metric_fn):"}
{"prefix": "def mkdirs(path, mode):\n    \"\"\"", "suffix": "    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int\n    \"\"\"\n    try:\n        o_umask = os.umask(0)\n        os.makedirs(path, mode)\n    except OSError:\n        if not os.path.isdir(path):\n            raise\n    finally:\n        os.umask(o_umask)", "gt": "    Creates the directory specified by path, creating intermediate directories"}
{"prefix": "def _convert_to_float_if_possible(s):\n    \"\"\"\n    A small helper function to convert a string to a numeric value\n    if appropriate\n\n    :param s: the string to be converted", "suffix": "    \"\"\"\n    try:\n        ret = float(s)\n    except (ValueError, TypeError):\n        ret = s\n    return ret", "gt": "    :type s: str"}
{"prefix": "", "suffix": "    \"\"\"\n    Get the current date and time in UTC\n    :return:\n    \"\"\"\n\n    # pendulum utcnow() is not used as that sets a TimezoneInfo object\n    # instead of a Timezone. This is not pickable and also creates issues\n    # when using replace()\n    d = dt.datetime.utcnow()\n    d = d.replace(tzinfo=utc)\n\n    return d", "gt": "def utcnow():"}
{"prefix": "def utc_epoch():\n    \"\"\"\n    Gets the epoch in the users timezone\n    :return:\n    \"\"\"\n", "suffix": "    # instead of a Timezone. This is not pickable and also creates issues\n    # when using replace()\n    d = dt.datetime(1970, 1, 1)\n    d = d.replace(tzinfo=utc)\n\n    return d", "gt": "    # pendulum utcnow() is not used as that sets a TimezoneInfo object"}
{"prefix": "def convert_to_utc(value):\n    \"\"\"\n    Returns the datetime with the default timezone added if timezone", "suffix": "    :param value: datetime\n    :return: datetime with tzinfo\n    \"\"\"\n    if not value:\n        return value\n\n    if not is_localized(value):\n        value = pendulum.instance(value, TIMEZONE)\n\n    return value.astimezone(utc)", "gt": "    information was not associated"}
{"prefix": "def make_aware(value, timezone=None):\n    \"\"\"\n    Make a naive datetime.datetime in a given time zone aware.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: localized datetime in settings.TIMEZONE or timezone\n\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Check that we won't overwrite the timezone of an aware datetime.\n    if is_localized(value):\n        raise ValueError(", "suffix": "    if hasattr(value, 'fold'):\n        # In case of python 3.6 we want to do the same that pendulum does for python3.5\n        # i.e in case we move clock back we want to schedule the run at the time of the second\n        # instance of the same clock time rather than the first one.\n        # Fold parameter has no impact in other cases so we can safely set it to 1 here\n        value = value.replace(fold=1)\n    if hasattr(timezone, 'localize'):\n        # This method is available for pytz time zones.\n        return timezone.localize(value)\n    elif hasattr(timezone, 'convert'):\n        # For pendulum\n        return timezone.convert(value)\n    else:\n        # This may be wrong around DST changes!\n        return value.replace(tzinfo=timezone)", "gt": "            \"make_aware expects a naive datetime, got %s\" % value)"}
{"prefix": "def make_naive(value, timezone=None):\n    \"\"\"\n    Make an aware datetime.datetime naive in a given time zone.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: naive datetime\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Emulate the behavior of astimezone() on Python < 3.6.\n    if is_naive(value):\n        raise ValueError(\"make_naive() cannot be applied to a naive datetime\")\n\n    o = value.astimezone(timezone)\n\n    # cross library compatibility", "suffix": "                        o.month,\n                        o.day,\n                        o.hour,\n                        o.minute,\n                        o.second,\n                        o.microsecond)\n\n    return naive", "gt": "    naive = dt.datetime(o.year,"}
{"prefix": "def datetime(*args, **kwargs):\n    \"\"\"\n    Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified", "suffix": "    :return: datetime.datetime\n    \"\"\"\n    if 'tzinfo' not in kwargs:\n        kwargs['tzinfo'] = TIMEZONE\n\n    return dt.datetime(*args, **kwargs)", "gt": ""}
{"prefix": "def _set_env_from_extras(self, extras):\n        \"\"\"\n        Sets the environment variable `GOOGLE_APPLICATION_CREDENTIALS` with either:\n\n        - The path to the keyfile from the specified connection id\n        - A generated file's path if the user specified JSON in the connection id. The\n            file is assumed to be deleted after the process dies due to how mkstemp()\n            works.\n\n        The environment variable is used inside the gcloud command to determine correct\n        service account to use.\n        \"\"\"\n        key_path = self._get_field(extras, 'key_path', False)\n        keyfile_json_str = self._get_field(extras, 'keyfile_dict', False)\n\n        if not key_path and not keyfile_json_str:\n            self.log.info('Using gcloud with application default credentials.')\n        elif key_path:\n            os.environ[G_APP_CRED] = key_path\n        else:\n            # Write service account JSON to secure file for gcloud to reference\n            service_key = tempfile.NamedTemporaryFile(delete=False)\n            service_key.write(keyfile_json_str)", "suffix": "            # Return file object to have a pointer to close after use,\n            # thus deleting from file system.\n            return service_key", "gt": "            os.environ[G_APP_CRED] = service_key.name"}
{"prefix": "def _get_field(self, extras, field, default=None):\n        \"\"\"\n        Fetches a field from extras, and returns it. This is some Airflow\n        magic. The google_cloud_platform hook type adds custom UI elements\n        to the hook page, which allow admins to specify service_account,", "suffix": "        \"\"\"\n        long_f = 'extra__google_cloud_platform__{}'.format(field)\n        if long_f in extras:\n            return extras[long_f]\n        else:\n            self.log.info('Field %s not found in extras.', field)\n            return default", "gt": "        key_path, etc. They get formatted as shown below."}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Establish a connection to druid broker.", "suffix": "        conn = self.get_connection(self.druid_broker_conn_id)\n        druid_broker_conn = connect(\n            host=conn.host,\n            port=conn.port,\n            path=conn.extra_dejson.get('endpoint', '/druid/v2/sql'),\n            scheme=conn.extra_dejson.get('schema', 'http')\n        )\n        self.log.info('Get the connection to druid broker on %s', conn.host)\n        return druid_broker_conn", "gt": "        \"\"\""}
{"prefix": "def get_conn(self, headers=None):\n        \"\"\"\n        Returns http session for use with requests\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        \"\"\"", "suffix": "        if self.http_conn_id:\n            conn = self.get_connection(self.http_conn_id)\n\n            if \"://\" in conn.host:\n                self.base_url = conn.host\n            else:\n                # schema defaults to HTTP\n                schema = conn.schema if conn.schema else \"http\"\n                self.base_url = schema + \"://\" + conn.host\n\n            if conn.port:\n                self.base_url = self.base_url + \":\" + str(conn.port)\n            if conn.login:\n                session.auth = (conn.login, conn.password)\n            if conn.extra:\n                try:\n                    session.headers.update(conn.extra_dejson)\n                except TypeError:\n                    self.log.warn('Connection to %s has invalid extra field.', conn.host)\n        if headers:\n            session.headers.update(headers)\n\n        return session", "gt": "        session = requests.Session()"}
{"prefix": "def run(self, endpoint, data=None, headers=None, extra_options=None):\n        \"\"\"\n        Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if self.base_url and not self.base_url.endswith('/') and \\\n           endpoint and not endpoint.startswith('/'):\n            url = self.base_url + '/' + endpoint\n        else:\n            url = (self.base_url or '') + (endpoint or '')\n\n        req = None\n        if self.method == 'GET':\n            # GET uses params\n            req = requests.Request(self.method,\n                                   url,", "suffix": "                                   headers=headers)\n        elif self.method == 'HEAD':\n            # HEAD doesn't use params\n            req = requests.Request(self.method,\n                                   url,\n                                   headers=headers)\n        else:\n            # Others use data\n            req = requests.Request(self.method,\n                                   url,\n                                   data=data,\n                                   headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)", "gt": "                                   params=data,"}
{"prefix": "def check_response(self, response):\n        \"\"\"\n        Checks the status code and raise an AirflowException exception on non 2XX or 3XX", "suffix": "\n        :param response: A requests response object\n        :type response: requests.response\n        \"\"\"\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError:\n            self.log.error(\"HTTP error: %s\", response.reason)\n            if self.method not in ['GET', 'HEAD']:\n                self.log.error(response.text)\n            raise AirflowException(str(response.status_code) + \":\" + response.reason)", "gt": "        status codes"}
{"prefix": "def run_and_check(self, session, prepped_request, extra_options):\n        \"\"\"\n        Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        try:\n            response = session.send(\n                prepped_request,\n                stream=extra_options.get(\"stream\", False),\n                verify=extra_options.get(\"verify\", True),\n                proxies=extra_options.get(\"proxies\", {}),\n                cert=extra_options.get(\"cert\"),\n                timeout=extra_options.get(\"timeout\"),\n                allow_redirects=extra_options.get(\"allow_redirects\", True))\n\n            if extra_options.get('check_response', True):\n                self.check_response(response)\n            return response\n\n        except requests.exceptions.ConnectionError as ex:\n            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')", "suffix": "", "gt": "            raise ex"}
{"prefix": "def run_with_advanced_retry(self, _retry_args, *args, **kwargs):\n        \"\"\"\n        Runs Hook.run() with a Tenacity decorator attached to it. This is useful for\n        connectors which might be disturbed by intermittent issues and should not\n        instantly fail.\n\n        :param _retry_args: Arguments which define the retry behaviour.\n            See Tenacity documentation at https://github.com/jd/tenacity\n        :type _retry_args: dict\n\n", "suffix": "\n            hook = HttpHook(http_conn_id='my_conn',method='GET')\n            retry_args = dict(\n                 wait=tenacity.wait_exponential(),\n                 stop=tenacity.stop_after_attempt(10),\n                 retry=requests.exceptions.ConnectionError\n             )\n             hook.run_with_advanced_retry(\n                     endpoint='v1/test',\n                     _retry_args=retry_args\n                 )\n        \"\"\"\n        self._retry_obj = tenacity.Retrying(\n            **_retry_args\n        )\n\n        self._retry_obj(self.run, *args, **kwargs)", "gt": "        :Example::"}
{"prefix": "def create_session():", "suffix": "    Contextmanager that will create and teardown a session.\n    \"\"\"\n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()", "gt": "    \"\"\""}
{"prefix": "def provide_session(func):\n    \"\"\"\n    Function decorator that provides a session if it isn't provided.\n    If you want to reuse a session or run the function as part of a\n    database transaction, you pass it to the function, if not this wrapper\n    will create one and close it for you.\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        arg_session = 'session'\n\n        func_params = func.__code__.co_varnames\n        session_in_args = arg_session in func_params and \\\n            func_params.index(arg_session) < len(args)", "suffix": "\n        if session_in_kwargs or session_in_args:\n            return func(*args, **kwargs)\n        else:\n            with create_session() as session:\n                kwargs[arg_session] = session\n                return func(*args, **kwargs)\n\n    return wrapper", "gt": "        session_in_kwargs = arg_session in kwargs"}
{"prefix": "def resetdb():\n    \"\"\"\n    Clear out the database\n    \"\"\"\n    from airflow import models\n\n    # alembic adds significant import time, so we import it lazily\n    from alembic.migration import MigrationContext\n\n    log.info(\"Dropping tables that exist\")\n\n    models.base.Base.metadata.drop_all(settings.engine)\n    mc = MigrationContext.configure(settings.engine)\n    if mc._version.exists(settings.engine):\n        mc._version.drop(settings.engine)\n\n    from flask_appbuilder.models.sqla import Base", "suffix": "\n    initdb()", "gt": "    Base.metadata.drop_all(settings.engine)"}
{"prefix": "def execute(self, context):\n        \"\"\"Upload a file to Azure Blob Storage.\"\"\"\n        hook = WasbHook(wasb_conn_id=self.wasb_conn_id)", "suffix": "            'Uploading %s to wasb://%s '\n            'as %s'.format(self.file_path, self.container_name, self.blob_name)\n        )\n        hook.load_file(self.file_path, self.container_name,\n                       self.blob_name, **self.load_options)", "gt": "        self.log.info("}
{"prefix": "def get_conn(self):\n        \"\"\"Returns a connection object\"\"\"\n        db = self.get_connection(self.presto_conn_id)\n        reqkwargs = None\n        if db.password is not None:\n            reqkwargs = {'auth': HTTPBasicAuth(db.login, db.password)}", "suffix": "            host=db.host,\n            port=db.port,\n            username=db.login,\n            source=db.extra_dejson.get('source', 'airflow'),\n            protocol=db.extra_dejson.get('protocol', 'http'),\n            catalog=db.extra_dejson.get('catalog', 'hive'),\n            requests_kwargs=reqkwargs,\n            schema=db.schema)", "gt": "        return presto.connect("}
{"prefix": "def _get_pretty_exception_message(e):\n        \"\"\"\n        Parses some DatabaseError to provide a better error message\n        \"\"\"\n        if (hasattr(e, 'message') and", "suffix": "                'message' in e.message):\n            return ('{name}: {message}'.format(\n                    name=e.message['errorName'],\n                    message=e.message['message']))\n        else:\n            return str(e)", "gt": "            'errorName' in e.message and"}
{"prefix": "def get_records(self, hql, parameters=None):\n        \"\"\"\n        Get a set of records from Presto\n        \"\"\"\n        try:\n            return super().get_records(", "suffix": "        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))", "gt": "                self._strip_sql(hql), parameters)"}
{"prefix": "def get_pandas_df(self, hql, parameters=None):\n        \"\"\"\n        Get a pandas dataframe from a sql query.", "suffix": "        import pandas\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(self._strip_sql(hql), parameters)\n            data = cursor.fetchall()\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n        column_descriptions = cursor.description\n        if data:\n            df = pandas.DataFrame(data)\n            df.columns = [c[0] for c in column_descriptions]\n        else:\n            df = pandas.DataFrame()\n        return df", "gt": "        \"\"\""}
{"prefix": "def run(self, hql, parameters=None):", "suffix": "        Execute the statement against Presto. Can be used to create views.\n        \"\"\"\n        return super().run(self._strip_sql(hql), parameters)", "gt": "        \"\"\""}
{"prefix": "def insert_rows(self, table, rows, target_fields=None):\n        \"\"\"\n        A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples", "suffix": "        :type target_fields: iterable of strings\n        \"\"\"\n        super().insert_rows(table, rows, target_fields, 0)", "gt": "        :param target_fields: The names of the columns to fill in the table"}
{"prefix": "", "suffix": "        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(self.endpoint_uri, {'masterKey': self.master_key})\n\n        return self.cosmos_client", "gt": "def get_conn(self):"}
{"prefix": "def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {", "suffix": "                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n        if len(existing_container) == 0:\n            return False\n\n        return True", "gt": "                \"query\": \"SELECT * FROM r WHERE r.id=@id\","}
{"prefix": "def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))", "suffix": "        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name})", "gt": ""}
{"prefix": "def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]", "suffix": "        if len(existing_database) == 0:\n            return False\n\n        return True", "gt": "        }))"}
{"prefix": "def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",", "suffix": "                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})", "gt": "            \"parameters\": ["}
{"prefix": "def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:", "suffix": "\n        self.get_conn().DeleteDatabase(get_database_link(database_name))", "gt": "            raise AirflowBadRequest(\"Database name cannot be None.\")"}
{"prefix": "def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:", "suffix": "\n        self.get_conn().DeleteContainer(\n            get_collection_link(self.__get_database_name(database_name), collection_name))", "gt": "            raise AirflowBadRequest(\"Collection name cannot be None.\")"}
{"prefix": "def upsert_document(self, document, database_name=None, collection_name=None, document_id=None):\n        \"\"\"\n        Inserts a new document (or updates an existing one) into an existing\n        collection in the CosmosDB database.\n        \"\"\"\n        # Assign unique ID if one isn't provided\n        if document_id is None:\n            document_id = str(uuid.uuid4())", "suffix": "        if document is None:\n            raise AirflowBadRequest(\"You cannot insert a None document\")\n\n        # Add document id if isn't found\n        if 'id' in document:\n            if document['id'] is None:\n                document['id'] = document_id\n        else:\n            document['id'] = document_id\n\n        created_document = self.get_conn().CreateItem(\n            get_collection_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name)),\n            document)\n\n        return created_document", "gt": ""}
{"prefix": "def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n", "suffix": "        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name)),\n                    single_document))\n\n        return created_documents", "gt": "        created_documents = []"}
{"prefix": "def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),", "suffix": "                document_id))", "gt": "                self.__get_collection_name(collection_name),"}
{"prefix": "def get_document(self, document_id, database_name=None, collection_name=None):", "suffix": "        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id))\n        except HTTPFailure:\n            return None", "gt": "        \"\"\""}
{"prefix": "def get_documents(self, sql_string, database_name=None, collection_name=None, partition_key=None):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")", "suffix": "        # Query them in SQL\n        query = {'query': sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name)),\n                query,\n                partition_key)\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None", "gt": ""}
{"prefix": "def get_code(dag_id):\n    \"\"\"Return python code of a given dag_id.\"\"\"\n    session = settings.Session()\n    DM = models.DagModel\n    dag = session.query(DM).filter(DM.dag_id == dag_id).first()\n    session.close()", "suffix": "    if dag is None:\n        error_message = \"Dag id {} not found\".format(dag_id)\n        raise DagNotFound(error_message)\n\n    try:\n        with wwwutils.open_maybe_zipped(dag.fileloc, 'r') as f:\n            code = f.read()\n            return code\n    except IOError as e:\n        error_message = \"Error {} while reading Dag id {} Code\".format(str(e), dag_id)\n        raise AirflowException(error_message)", "gt": "    # Check DAG exists."}
{"prefix": "def get_function(self, name):\n        \"\"\"\n        Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.", "suffix": "        \"\"\"\n        return self.get_conn().projects().locations().functions().get(\n            name=name).execute(num_retries=self.num_retries)", "gt": "        :rtype: dict"}
{"prefix": "def create_new_function(self, location, body, project_id=None):\n        \"\"\"", "suffix": "\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().create(\n            location=self._full_location(project_id, location),\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)", "gt": "        Creates a new function in Cloud Function in the location specified in the body."}
{"prefix": "def update_function(self, name, body, update_mask):", "suffix": "        Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().patch(\n            updateMask=\",\".join(update_mask),\n            name=name,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)", "gt": "        \"\"\""}
{"prefix": "def upload_function_zip(self, location, zip_path, project_id=None):\n        \"\"\"\n        Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str", "suffix": "            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().generateUploadUrl(\n            parent=self._full_location(project_id, location)\n        ).execute(num_retries=self.num_retries)\n        upload_url = response.get('uploadUrl')\n        with open(zip_path, 'rb') as fp:\n            requests.put(\n                url=upload_url,\n                data=fp,\n                # Those two headers needs to be specified according to:\n                # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl\n                # nopep8\n                headers={\n                    'Content-type': 'application/zip',\n                    'x-goog-content-length-range': '0,104857600',\n                }\n            )\n        return upload_url", "gt": "        :param project_id: Optional, Google Cloud Project project_id where the function belongs."}
{"prefix": "def delete_function(self, name):\n        \"\"\"", "suffix": "\n        :param name: The name of the function.\n        :type name: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().delete(\n            name=name).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)", "gt": "        Deletes the specified Cloud Function."}
{"prefix": "", "suffix": "        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param operation_name: The name of the operation.\n        :type operation_name: str\n        :return: The response returned by the operation.\n        :rtype: dict\n        :exception: AirflowException in case error is returned.\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = service.operations().get(\n                name=operation_name,\n            ).execute(num_retries=self.num_retries)\n            if operation_response.get(\"done\"):\n                response = operation_response.get(\"response\")\n                error = operation_response.get(\"error\")\n                # Note, according to documentation always either response or error is\n                # set when \"done\" == True\n                if error:\n                    raise AirflowException(str(error))\n                return response\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)", "gt": "def _wait_for_operation_to_complete(self, operation_name):"}
{"prefix": "def publish(self, project, topic, messages):\n        \"\"\"Publishes messages to a Pub/Sub topic.\n\n        :param project: the GCP project ID in which to publish\n        :type project: str\n        :param topic: the Pub/Sub topic to which to publish; do not", "suffix": "        :type topic: str\n        :param messages: messages to publish; if the data field in a\n            message is set, it should already be base64 encoded.\n        :type messages: list of PubSub messages; see\n            http://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage\n        \"\"\"\n        body = {'messages': messages}\n        full_topic = _format_topic(project, topic)\n        request = self.get_conn().projects().topics().publish(\n            topic=full_topic, body=body)\n        try:\n            request.execute(num_retries=self.num_retries)\n        except HttpError as e:\n            raise PubSubException(\n                'Error publishing to topic {}'.format(full_topic), e)", "gt": "            include the ``projects/{project}/topics/`` prefix."}
{"prefix": "def create_topic(self, project, topic, fail_if_exists=False):\n        \"\"\"Creates a Pub/Sub topic, if it does not already exist.\n\n        :param project: the GCP project ID in which to create\n            the topic\n        :type project: str\n        :param topic: the Pub/Sub topic name to create; do not\n            include the ``projects/{project}/topics/`` prefix.\n        :type topic: str\n        :param fail_if_exists: if set, raise an exception if the topic\n            already exists\n        :type fail_if_exists: bool\n        \"\"\"\n        service = self.get_conn()", "suffix": "        try:\n            service.projects().topics().create(\n                name=full_topic, body={}).execute(num_retries=self.num_retries)\n        except HttpError as e:\n            # Status code 409 indicates that the topic already exists.\n            if str(e.resp['status']) == '409':\n                message = 'Topic already exists: {}'.format(full_topic)\n                self.log.warning(message)\n                if fail_if_exists:\n                    raise PubSubException(message)\n            else:\n                raise PubSubException(\n                    'Error creating topic {}'.format(full_topic), e)", "gt": "        full_topic = _format_topic(project, topic)"}
{"prefix": "def delete_topic(self, project, topic, fail_if_not_exists=False):\n        \"\"\"Deletes a Pub/Sub topic if it exists.\n\n        :param project: the GCP project ID in which to delete the topic\n        :type project: str\n        :param topic: the Pub/Sub topic name to delete; do not\n            include the ``projects/{project}/topics/`` prefix.\n        :type topic: str\n        :param fail_if_not_exists: if set, raise an exception if the topic\n            does not exist\n        :type fail_if_not_exists: bool\n        \"\"\"\n        service = self.get_conn()\n        full_topic = _format_topic(project, topic)\n        try:\n            service.projects().topics().delete(topic=full_topic).execute(num_retries=self.num_retries)", "suffix": "            # Status code 409 indicates that the topic was not found\n            if str(e.resp['status']) == '404':\n                message = 'Topic does not exist: {}'.format(full_topic)\n                self.log.warning(message)\n                if fail_if_not_exists:\n                    raise PubSubException(message)\n            else:\n                raise PubSubException(\n                    'Error deleting topic {}'.format(full_topic), e)", "gt": "        except HttpError as e:"}
{"prefix": "def create_subscription(self, topic_project, topic, subscription=None,\n                            subscription_project=None, ack_deadline_secs=10,\n                            fail_if_exists=False):\n        \"\"\"Creates a Pub/Sub subscription, if it does not already exist.\n\n        :param topic_project: the GCP project ID of the topic that the\n            subscription will be bound to.\n        :type topic_project: str\n        :param topic: the Pub/Sub topic name that the subscription will be bound\n            to create; do not include the ``projects/{project}/subscriptions/``", "suffix": "        :type topic: str\n        :param subscription: the Pub/Sub subscription name. If empty, a random\n            name will be generated using the uuid module\n        :type subscription: str\n        :param subscription_project: the GCP project ID where the subscription\n            will be created. If unspecified, ``topic_project`` will be used.\n        :type subscription_project: str\n        :param ack_deadline_secs: Number of seconds that a subscriber has to\n            acknowledge each message pulled from the subscription\n        :type ack_deadline_secs: int\n        :param fail_if_exists: if set, raise an exception if the topic\n            already exists\n        :type fail_if_exists: bool\n        :return: subscription name which will be the system-generated value if\n            the ``subscription`` parameter is not supplied\n        :rtype: str\n        \"\"\"\n        service = self.get_conn()\n        full_topic = _format_topic(topic_project, topic)\n        if not subscription:\n            subscription = 'sub-{}'.format(uuid4())\n        if not subscription_project:\n            subscription_project = topic_project\n        full_subscription = _format_subscription(subscription_project,\n                                                 subscription)\n        body = {\n            'topic': full_topic,\n            'ackDeadlineSeconds': ack_deadline_secs\n        }\n        try:\n            service.projects().subscriptions().create(\n                name=full_subscription, body=body).execute(num_retries=self.num_retries)\n        except HttpError as e:\n            # Status code 409 indicates that the subscription already exists.\n            if str(e.resp['status']) == '409':\n                message = 'Subscription already exists: {}'.format(\n                    full_subscription)\n                self.log.warning(message)\n                if fail_if_exists:\n                    raise PubSubException(message)\n            else:\n                raise PubSubException(\n                    'Error creating subscription {}'.format(full_subscription),\n                    e)\n        return subscription", "gt": "            prefix."}
{"prefix": "def delete_subscription(self, project, subscription,\n                            fail_if_not_exists=False):\n        \"\"\"Deletes a Pub/Sub subscription, if it exists.\n\n        :param project: the GCP project ID where the subscription exists\n        :type project: str", "suffix": "            include the ``projects/{project}/subscriptions/`` prefix.\n        :type subscription: str\n        :param fail_if_not_exists: if set, raise an exception if the topic\n            does not exist\n        :type fail_if_not_exists: bool\n        \"\"\"\n        service = self.get_conn()\n        full_subscription = _format_subscription(project, subscription)\n        try:\n            service.projects().subscriptions().delete(\n                subscription=full_subscription).execute(num_retries=self.num_retries)\n        except HttpError as e:\n            # Status code 404 indicates that the subscription was not found\n            if str(e.resp['status']) == '404':\n                message = 'Subscription does not exist: {}'.format(\n                    full_subscription)\n                self.log.warning(message)\n                if fail_if_not_exists:\n                    raise PubSubException(message)\n            else:\n                raise PubSubException(\n                    'Error deleting subscription {}'.format(full_subscription),\n                    e)", "gt": "        :param subscription: the Pub/Sub subscription name to delete; do not"}
{"prefix": "", "suffix": "             return_immediately=False):\n        \"\"\"Pulls up to ``max_messages`` messages from Pub/Sub subscription.\n\n        :param project: the GCP project ID where the subscription exists\n        :type project: str\n        :param subscription: the Pub/Sub subscription name to pull from; do not\n            include the 'projects/{project}/topics/' prefix.\n        :type subscription: str\n        :param max_messages: The maximum number of messages to return from\n            the Pub/Sub API.\n        :type max_messages: int\n        :param return_immediately: If set, the Pub/Sub API will immediately\n            return if no messages are available. Otherwise, the request will\n            block for an undisclosed, but bounded period of time\n        :type return_immediately: bool\n        :return: A list of Pub/Sub ReceivedMessage objects each containing\n            an ``ackId`` property and a ``message`` property, which includes\n            the base64-encoded message content. See\n            https://cloud.google.com/pubsub/docs/reference/rest/v1/projects.subscriptions/pull#ReceivedMessage\n        \"\"\"\n        service = self.get_conn()\n        full_subscription = _format_subscription(project, subscription)\n        body = {\n            'maxMessages': max_messages,\n            'returnImmediately': return_immediately\n        }\n        try:\n            response = service.projects().subscriptions().pull(\n                subscription=full_subscription, body=body).execute(num_retries=self.num_retries)\n            return response.get('receivedMessages', [])\n        except HttpError as e:\n            raise PubSubException(\n                'Error pulling messages from subscription {}'.format(\n                    full_subscription), e)", "gt": "def pull(self, project, subscription, max_messages,"}
{"prefix": "def acknowledge(self, project, subscription, ack_ids):\n        \"\"\"Pulls up to ``max_messages`` messages from Pub/Sub subscription.\n\n        :param project: the GCP project name or ID in which to create\n            the topic\n        :type project: str\n        :param subscription: the Pub/Sub subscription name to delete; do not\n            include the 'projects/{project}/topics/' prefix.\n        :type subscription: str\n        :param ack_ids: List of ReceivedMessage ackIds from a previous pull\n            response\n        :type ack_ids: list\n        \"\"\"\n        service = self.get_conn()\n        full_subscription = _format_subscription(project, subscription)\n        try:\n            service.projects().subscriptions().acknowledge(\n                subscription=full_subscription, body={'ackIds': ack_ids}\n            ).execute(num_retries=self.num_retries)\n        except HttpError as e:", "suffix": "                'Error acknowledging {} messages pulled from subscription {}'\n                .format(len(ack_ids), full_subscription), e)", "gt": "            raise PubSubException("}
{"prefix": "def get_dep_statuses(self, ti, session, dep_context=None):\n        \"\"\"\n        Wrapper around the private _get_dep_statuses method that contains some global\n        checks for all dependencies.\n\n        :param ti: the task instance to get the dependency status for\n        :type ti: airflow.models.TaskInstance\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        :param dep_context: the context for which this dependency should be evaluated for\n        :type dep_context: DepContext\n        \"\"\"\n        # this avoids a circular dependency\n        from airflow.ti_deps.dep_context import DepContext", "suffix": "        if dep_context is None:\n            dep_context = DepContext()\n\n        if self.IGNOREABLE and dep_context.ignore_all_deps:\n            yield self._passing_status(\n                reason=\"Context specified all dependencies should be ignored.\")\n            return\n\n        if self.IS_TASK_DEP and dep_context.ignore_task_deps:\n            yield self._passing_status(\n                reason=\"Context specified all task dependencies should be ignored.\")\n            return\n\n        for dep_status in self._get_dep_statuses(ti, session, dep_context):\n            yield dep_status", "gt": ""}
{"prefix": "def is_met(self, ti, session, dep_context=None):", "suffix": "        Returns whether or not this dependency is met for a given task instance. A\n        dependency is considered met if all of the dependency statuses it reports are\n        passing.\n\n        :param ti: the task instance to see if this dependency is met for\n        :type ti: airflow.models.TaskInstance\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        :param dep_context: The context this dependency is being checked under that stores\n            state that can be used by this dependency.\n        :type dep_context: BaseDepContext\n        \"\"\"\n        return all(status.passed for status in\n                   self.get_dep_statuses(ti, session, dep_context))", "gt": "        \"\"\""}
{"prefix": "def get_failure_reasons(self, ti, session, dep_context=None):\n        \"\"\"\n        Returns an iterable of strings that explain why this dependency wasn't met.", "suffix": "        :param ti: the task instance to see if this dependency is met for\n        :type ti: airflow.models.TaskInstance\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        :param dep_context: The context this dependency is being checked under that stores\n            state that can be used by this dependency.\n        :type dep_context: BaseDepContext\n        \"\"\"\n        for dep_status in self.get_dep_statuses(ti, session, dep_context):\n            if not dep_status.passed:\n                yield dep_status.reason", "gt": ""}
{"prefix": "def _parse_s3_config(config_file_name, config_format='boto', profile=None):\n    \"\"\"\n    Parses a config file for s3 credentials. Can currently\n    parse boto, s3cmd.conf and AWS SDK config formats\n\n    :param config_file_name: path to the config file\n    :type config_file_name: str\n    :param config_format: config type. One of \"boto\", \"s3cmd\" or \"aws\".\n        Defaults to \"boto\"\n    :type config_format: str\n    :param profile: profile name in AWS type config file\n    :type profile: str\n    \"\"\"\n    config = configparser.ConfigParser()\n    if config.read(config_file_name):  # pragma: no cover\n        sections = config.sections()\n    else:\n        raise AirflowException(\"Couldn't read {0}\".format(config_file_name))\n    # Setting option names depending on file format\n    if config_format is None:\n        config_format = 'boto'\n    conf_format = config_format.lower()\n    if conf_format == 'boto':  # pragma: no cover\n        if profile is not None and 'profile ' + profile in sections:\n            cred_section = 'profile ' + profile\n        else:\n            cred_section = 'Credentials'\n    elif conf_format == 'aws' and profile is not None:\n        cred_section = profile\n    else:", "suffix": "    # Option names\n    if conf_format in ('boto', 'aws'):  # pragma: no cover\n        key_id_option = 'aws_access_key_id'\n        secret_key_option = 'aws_secret_access_key'\n        # security_token_option = 'aws_security_token'\n    else:\n        key_id_option = 'access_key'\n        secret_key_option = 'secret_key'\n    # Actual Parsing\n    if cred_section not in sections:\n        raise AirflowException(\"This config file format is not recognized\")\n    else:\n        try:\n            access_key = config.get(cred_section, key_id_option)\n            secret_key = config.get(cred_section, secret_key_option)\n        except Exception:\n            logging.warning(\"Option Error in parsing s3 config file\")\n            raise\n        return access_key, secret_key", "gt": "        cred_section = 'default'"}
{"prefix": "def get_credentials(self, region_name=None):\n        \"\"\"Get the underlying `botocore.Credentials` object.\n\n        This contains the following authentication attributes: access_key, secret_key and token.\n        \"\"\"\n        session, _ = self._get_credentials(region_name)\n        # Credentials are refreshable, so accessing your access key and", "suffix": "        # See https://stackoverflow.com/a/36291428/8283373\n        return session.get_credentials().get_frozen_credentials()", "gt": "        # secret key separately can lead to a race condition."}
{"prefix": "def expand_role(self, role):\n        \"\"\"\n        If the IAM role is a role name, get the Amazon Resource Name (ARN) for the role.", "suffix": "\n        :param role: IAM role name or ARN\n        :return: IAM role ARN\n        \"\"\"\n        if '/' in role:\n            return role\n        else:\n            return self.get_client_type('iam').get_role(RoleName=role)['Role']['Arn']", "gt": "        If IAM role is already an IAM role ARN, no change is made."}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Returns verticaql connection object\n        \"\"\"\n        conn = self.get_connection(self.vertica_conn_id)\n        conn_config = {", "suffix": "            \"password\": conn.password or '',\n            \"database\": conn.schema,\n            \"host\": conn.host or 'localhost'\n        }\n\n        if not conn.port:\n            conn_config[\"port\"] = 5433\n        else:\n            conn_config[\"port\"] = int(conn.port)\n\n        conn = connect(**conn_config)\n        return conn", "gt": "            \"user\": conn.login,"}
{"prefix": "def set_context(logger, value):\n    \"\"\"\n    Walks the tree of loggers and tries to set the context for each handler\n    :param logger: logger\n    :param value: value to set\n    \"\"\"\n    _logger = logger\n    while _logger:\n        for handler in _logger.handlers:\n            try:\n                handler.set_context(value)\n            except AttributeError:\n                # Not all handlers need to have context passed in so we ignore\n                # the error when handlers do not have set_context defined.\n                pass\n        if _logger.propagate is True:\n            _logger = _logger.parent\n        else:", "suffix": "", "gt": "            _logger = None"}
{"prefix": "def write(self, message):\n        \"\"\"", "suffix": "        :param message: message to log\n        \"\"\"\n        if not message.endswith(\"\\n\"):\n            self._buffer += message\n        else:\n            self._buffer += message\n            self.logger.log(self.level, self._buffer.rstrip())\n            self._buffer = str()", "gt": "        Do whatever it takes to actually log the specified logging record"}
{"prefix": "def flush(self):\n        \"\"\"\n        Ensure all logging output has been flushed\n        \"\"\"\n        if len(self._buffer) > 0:", "suffix": "            self._buffer = str()", "gt": "            self.logger.log(self.level, self._buffer)"}
{"prefix": "def correct_maybe_zipped(fileloc):\n    \"\"\"\n    If the path contains a folder with a .zip suffix, then\n    the folder is treated as a zip archive and path to zip is returned.\n    \"\"\"\n\n    _, archive, filename = re.search(", "suffix": "    if archive and zipfile.is_zipfile(archive):\n        return archive\n    else:\n        return fileloc", "gt": "        r'((.*\\.zip){})?(.*)'.format(re.escape(os.sep)), fileloc).groups()"}
{"prefix": "def list_py_file_paths(directory, safe_mode=True,\n                       include_examples=None):\n    \"\"\"\n    Traverse a directory and look for Python files.\n\n    :param directory: the directory to traverse\n    :type directory: unicode\n    :param safe_mode: whether to use a heuristic to determine whether a file\n        contains Airflow DAG definitions\n    :return: a list of paths to Python files in the specified directory\n    :rtype: list[unicode]\n    \"\"\"\n    if include_examples is None:\n        include_examples = conf.getboolean('core', 'LOAD_EXAMPLES')\n    file_paths = []\n    if directory is None:\n        return []\n    elif os.path.isfile(directory):\n        return [directory]\n    elif os.path.isdir(directory):\n        patterns_by_dir = {}\n        for root, dirs, files in os.walk(directory, followlinks=True):\n            patterns = patterns_by_dir.get(root, [])\n            ignore_file = os.path.join(root, '.airflowignore')\n            if os.path.isfile(ignore_file):\n                with open(ignore_file, 'r') as f:\n                    # If we have new patterns create a copy so we don't change\n                    # the previous list (which would affect other subdirs)\n                    patterns += [re.compile(p) for p in f.read().split('\\n') if p]\n\n            # If we can ignore any subdirs entirely we should - fewer paths\n            # to walk is better. We have to modify the ``dirs`` array in\n            # place for this to affect os.walk\n            dirs[:] = [\n                d\n                for d in dirs\n                if not any(p.search(os.path.join(root, d)) for p in patterns)\n            ]\n\n            # We want patterns defined in a parent folder's .airflowignore to\n            # apply to subdirs too\n            for d in dirs:\n                patterns_by_dir[os.path.join(root, d)] = patterns\n\n            for f in files:\n                try:\n                    file_path = os.path.join(root, f)\n                    if not os.path.isfile(file_path):\n                        continue\n                    mod_name, file_ext = os.path.splitext(\n                        os.path.split(file_path)[-1])\n                    if file_ext != '.py' and not zipfile.is_zipfile(file_path):\n                        continue\n                    if any([re.findall(p, file_path) for p in patterns]):\n                        continue\n\n                    # Heuristic that guesses whether a Python file contains an\n                    # Airflow DAG definition.\n                    might_contain_dag = True\n                    if safe_mode and not zipfile.is_zipfile(file_path):\n                        with open(file_path, 'rb') as fp:\n                            content = fp.read()", "suffix": "                                [s in content for s in (b'DAG', b'airflow')])\n\n                    if not might_contain_dag:\n                        continue\n\n                    file_paths.append(file_path)\n                except Exception:\n                    log = LoggingMixin().log\n                    log.exception(\"Error while examining %s\", f)\n    if include_examples:\n        import airflow.example_dags\n        example_dag_folder = airflow.example_dags.__path__[0]\n        file_paths.extend(list_py_file_paths(example_dag_folder, safe_mode, False))\n    return file_paths", "gt": "                            might_contain_dag = all("}
{"prefix": "def construct_task_instance(self, session=None, lock_for_update=False):\n        \"\"\"\n        Construct a TaskInstance from the database based on the primary key", "suffix": "        :param session: DB session.\n        :param lock_for_update: if True, indicates that the database should\n            lock the TaskInstance (issuing a FOR UPDATE clause) until the\n            session is committed.\n        \"\"\"\n        TI = airflow.models.TaskInstance\n\n        qry = session.query(TI).filter(\n            TI.dag_id == self._dag_id,\n            TI.task_id == self._task_id,\n            TI.execution_date == self._execution_date)\n\n        if lock_for_update:\n            ti = qry.with_for_update().first()\n        else:\n            ti = qry.first()\n        return ti", "gt": ""}
{"prefix": "def get_dag(self, dag_id):\n        \"\"\"\n        :param dag_id: DAG ID\n        :type dag_id: unicode\n        :return: if the given DAG ID exists in the bag, return the BaseDag\n        corresponding to that ID. Otherwise, throw an Exception\n        :rtype: airflow.utils.dag_processing.SimpleDag\n        \"\"\"\n        if dag_id not in self.dag_id_to_simple_dag:", "suffix": "        return self.dag_id_to_simple_dag[dag_id]", "gt": "            raise AirflowException(\"Unknown DAG ID {}\".format(dag_id))"}
{"prefix": "", "suffix": "        \"\"\"\n        Launch DagFileProcessorManager processor and start DAG parsing loop in manager.\n        \"\"\"\n        self._process = self._launch_process(self._dag_directory,\n                                             self._file_paths,\n                                             self._max_runs,\n                                             self._processor_factory,\n                                             self._child_signal_conn,\n                                             self._stat_queue,\n                                             self._result_queue,\n                                             self._async_mode)\n        self.log.info(\"Launched DagFileProcessorManager with pid: %s\", self._process.pid)", "gt": "def start(self):"}
{"prefix": "def harvest_simple_dags(self):\n        \"\"\"\n        Harvest DAG parsing results from result queue and sync metadata from stat queue.\n        :return: List of parsing result in SimpleDag format.\n        \"\"\"\n        # Metadata and results to be harvested can be inconsistent,\n        # but it should not be a big problem.\n        self._sync_metadata()\n        # Heartbeating after syncing metadata so we do not restart manager\n        # if it processed all files for max_run times and exit normally.\n        self._heartbeat_manager()\n        simple_dags = []\n        # multiprocessing.Queue().qsize will not work on MacOS.\n        if sys.platform == \"darwin\":\n            qsize = self._result_count\n        else:\n            qsize = self._result_queue.qsize()\n        for _ in range(qsize):\n            simple_dags.append(self._result_queue.get())", "suffix": "        self._result_count = 0\n\n        return simple_dags", "gt": ""}
{"prefix": "", "suffix": "        \"\"\"\n        Heartbeat DAG file processor and start it if it is not alive.\n        :return:\n        \"\"\"\n        if self._process and not self._process.is_alive() and not self.done:\n            self.start()", "gt": "def _heartbeat_manager(self):"}
{"prefix": "def _sync_metadata(self):\n        \"\"\"", "suffix": "        :return:\n        \"\"\"\n        while not self._stat_queue.empty():\n            stat = self._stat_queue.get()\n            self._file_paths = stat.file_paths\n            self._all_pids = stat.all_pids\n            self._done = stat.done\n            self._all_files_processed = stat.all_files_processed\n            self._result_count += stat.result_count", "gt": "        Sync metadata from stat queue and only keep the latest stat."}
{"prefix": "def terminate(self):\n        \"\"\"\n        Send termination signal to DAG parsing processor manager\n        and expect it to terminate all DAG file processors.\n        \"\"\"\n        self.log.info(\"Sending termination message to manager.\")", "suffix": "", "gt": "        self._child_signal_conn.send(DagParsingSignal.TERMINATE_MANAGER)"}
{"prefix": "def end(self):\n        \"\"\"\n        Terminate (and then kill) the manager process launched.\n        :return:\n        \"\"\"\n        if not self._process:\n            self.log.warn('Ending without manager process.')\n            return\n        this_process = psutil.Process(os.getpid())\n        try:\n            manager_process = psutil.Process(self._process.pid)\n        except psutil.NoSuchProcess:\n            self.log.info(\"Manager process not running.\")\n            return\n\n        # First try SIGTERM\n        if manager_process.is_running() \\\n                and manager_process.pid in [x.pid for x in this_process.children()]:\n            self.log.info(\"Terminating manager process: %s\", manager_process.pid)\n            manager_process.terminate()\n            # TODO: Remove magic number\n            timeout = 5\n            self.log.info(\"Waiting up to %ss for manager process to exit...\", timeout)\n            try:\n                psutil.wait_procs({manager_process}, timeout)\n            except psutil.TimeoutExpired:\n                self.log.debug(\"Ran out of time while waiting for \"\n                               \"processes to exit\")", "suffix": "        # Then SIGKILL\n        if manager_process.is_running() \\\n                and manager_process.pid in [x.pid for x in this_process.children()]:\n            self.log.info(\"Killing manager process: %s\", manager_process.pid)\n            manager_process.kill()\n            manager_process.wait()", "gt": ""}
{"prefix": "def _exit_gracefully(self, signum, frame):\n        \"\"\"", "suffix": "        \"\"\"\n        self.log.info(\"Exiting gracefully upon receiving signal %s\", signum)\n        self.terminate()\n        self.end()\n        self.log.debug(\"Finished terminating DAG processors.\")\n        sys.exit(os.EX_OK)", "gt": "        Helper method to clean up DAG file processors to avoid leaving orphan processes."}
{"prefix": "def start(self):\n        \"\"\"\n        Use multiple processes to parse and generate tasks for the\n        DAGs in parallel. By processing them in separate processes,\n        we can get parallelism and isolation from potentially harmful\n        user code.\n        \"\"\"\n\n        self.log.info(\"Processing files using up to %s processes at a time \", self._parallelism)\n        self.log.info(\"Process each file at most once every %s seconds\", self._file_process_interval)\n        self.log.info(\n            \"Checking for new files in %s every %s seconds\", self._dag_directory, self.dag_dir_list_interval\n        )\n\n        if self._async_mode:\n            self.log.debug(\"Starting DagFileProcessorManager in async mode\")\n            self.start_in_async()\n        else:", "suffix": "            self.start_in_sync()", "gt": "            self.log.debug(\"Starting DagFileProcessorManager in sync mode\")"}
{"prefix": "def start_in_async(self):\n        \"\"\"\n        Parse DAG files repeatedly in a standalone loop.\n        \"\"\"\n        while True:\n            loop_start_time = time.time()\n\n            if self._signal_conn.poll():\n                agent_signal = self._signal_conn.recv()\n                if agent_signal == DagParsingSignal.TERMINATE_MANAGER:\n                    self.terminate()\n                    break\n                elif agent_signal == DagParsingSignal.END_MANAGER:\n                    self.end()\n                    sys.exit(os.EX_OK)\n\n            self._refresh_dag_dir()\n\n            simple_dags = self.heartbeat()\n            for simple_dag in simple_dags:\n                self._result_queue.put(simple_dag)\n\n            self._print_stat()\n\n            all_files_processed = all(self.get_last_finish_time(x) is not None\n                                      for x in self.file_paths)\n            max_runs_reached = self.max_runs_reached()\n\n            dag_parsing_stat = DagParsingStat(self._file_paths,\n                                              self.get_all_pids(),", "suffix": "                                              all_files_processed,\n                                              len(simple_dags))\n            self._stat_queue.put(dag_parsing_stat)\n\n            if max_runs_reached:\n                self.log.info(\"Exiting dag parsing loop as all files \"\n                              \"have been processed %s times\", self._max_runs)\n                break\n\n            loop_duration = time.time() - loop_start_time\n            if loop_duration < 1:\n                sleep_length = 1 - loop_duration\n                self.log.debug(\"Sleeping for %.2f seconds to prevent excessive logging\", sleep_length)\n                time.sleep(sleep_length)", "gt": "                                              max_runs_reached,"}
{"prefix": "def start_in_sync(self):\n        \"\"\"\n        Parse DAG files in a loop controlled by DagParsingSignal.\n        Actual DAG parsing loop will run once upon receiving one\n        agent heartbeat message and will report done when finished the loop.\n        \"\"\"\n        while True:\n            agent_signal = self._signal_conn.recv()\n            if agent_signal == DagParsingSignal.TERMINATE_MANAGER:\n                self.terminate()\n                break\n            elif agent_signal == DagParsingSignal.END_MANAGER:\n                self.end()\n                sys.exit(os.EX_OK)\n            elif agent_signal == DagParsingSignal.AGENT_HEARTBEAT:\n\n                self._refresh_dag_dir()\n\n                simple_dags = self.heartbeat()\n                for simple_dag in simple_dags:\n                    self._result_queue.put(simple_dag)\n\n                self._print_stat()\n\n                all_files_processed = all(self.get_last_finish_time(x) is not None\n                                          for x in self.file_paths)\n                max_runs_reached = self.max_runs_reached()\n\n                dag_parsing_stat = DagParsingStat(self._file_paths,\n                                                  self.get_all_pids(),", "suffix": "                                                  all_files_processed,\n                                                  len(simple_dags))\n                self._stat_queue.put(dag_parsing_stat)\n\n                self.wait_until_finished()\n                self._signal_conn.send(DagParsingSignal.MANAGER_DONE)\n\n                if max_runs_reached:\n                    self.log.info(\"Exiting dag parsing loop as all files \"\n                                  \"have been processed %s times\", self._max_runs)\n                    self._signal_conn.send(DagParsingSignal.MANAGER_DONE)\n                    break", "gt": "                                                  self.max_runs_reached(),"}
{"prefix": "def _refresh_dag_dir(self):\n        \"\"\"\n        Refresh file paths from dag dir if we haven't done it for too long.\n        \"\"\"\n        elapsed_time_since_refresh = (timezone.utcnow() -\n                                      self.last_dag_dir_refresh_time).total_seconds()\n        if elapsed_time_since_refresh > self.dag_dir_list_interval:", "suffix": "            self.log.info(\"Searching for files in %s\", self._dag_directory)\n            self._file_paths = list_py_file_paths(self._dag_directory)\n            self.last_dag_dir_refresh_time = timezone.utcnow()\n            self.log.info(\"There are %s files in %s\", len(self._file_paths), self._dag_directory)\n            self.set_file_paths(self._file_paths)\n\n            try:\n                self.log.debug(\"Removing old import errors\")\n                self.clear_nonexistent_import_errors()\n            except Exception:\n                self.log.exception(\"Error removing old import errors\")", "gt": "            # Build up a list of Python files that could contain DAGs"}
{"prefix": "def _print_stat(self):\n        \"\"\"\n        Occasionally print out stats about how fast the files are getting processed\n        \"\"\"", "suffix": "                self.print_stats_interval):\n            if len(self._file_paths) > 0:\n                self._log_file_processing_stats(self._file_paths)\n            self.last_stat_print_time = timezone.utcnow()", "gt": "        if ((timezone.utcnow() - self.last_stat_print_time).total_seconds() >"}
{"prefix": "def clear_nonexistent_import_errors(self, session):\n        \"\"\"\n        Clears import errors for files that no longer exist.\n\n        :param session: session for ORM operations\n        :type session: sqlalchemy.orm.session.Session\n        \"\"\"\n        query = session.query(errors.ImportError)\n        if self._file_paths:\n            query = query.filter(\n                ~errors.ImportError.filename.in_(self._file_paths)", "suffix": "        query.delete(synchronize_session='fetch')\n        session.commit()", "gt": "            )"}
{"prefix": "def _log_file_processing_stats(self, known_file_paths):\n        \"\"\"\n        Print out stats about how files are getting processed.\n\n        :param known_file_paths: a list of file paths that may contain Airflow\n            DAG definitions\n        :type known_file_paths: list[unicode]\n        :return: None\n        \"\"\"\n\n        # File Path: Path to the file containing the DAG definition\n        # PID: PID associated with the process that's processing the file. May\n        # be empty.\n        # Runtime: If the process is currently running, how long it's been\n        # running for in seconds.\n        # Last Runtime: If the process ran before, how long did it take to\n        # finish in seconds\n        # Last Run: When the file finished processing in the previous run.\n        headers = [\"File Path\",\n                   \"PID\",\n                   \"Runtime\",\n                   \"Last Runtime\",\n                   \"Last Run\"]\n\n        rows = []\n        for file_path in known_file_paths:\n            last_runtime = self.get_last_runtime(file_path)\n            file_name = os.path.basename(file_path)", "suffix": "            if last_runtime:\n                Stats.gauge(\n                    'dag_processing.last_runtime.{}'.format(file_name),\n                    last_runtime\n                )\n\n            processor_pid = self.get_pid(file_path)\n            processor_start_time = self.get_start_time(file_path)\n            runtime = ((timezone.utcnow() - processor_start_time).total_seconds()\n                       if processor_start_time else None)\n            last_run = self.get_last_finish_time(file_path)\n            if last_run:\n                seconds_ago = (timezone.utcnow() - last_run).total_seconds()\n                Stats.gauge(\n                    'dag_processing.last_run.seconds_ago.{}'.format(file_name),\n                    seconds_ago\n                )\n\n            rows.append((file_path,\n                         processor_pid,\n                         runtime,\n                         last_runtime,\n                         last_run))\n\n        # Sort by longest last runtime. (Can't sort None values in python3)\n        rows = sorted(rows, key=lambda x: x[3] or 0.0)\n\n        formatted_rows = []\n        for file_path, pid, runtime, last_runtime, last_run in rows:\n            formatted_rows.append((file_path,\n                                   pid,\n                                   \"{:.2f}s\".format(runtime)\n                                   if runtime else None,\n                                   \"{:.2f}s\".format(last_runtime)\n                                   if last_runtime else None,\n                                   last_run.strftime(\"%Y-%m-%dT%H:%M:%S\")\n                                   if last_run else None))\n        log_str = (\"\\n\" +\n                   \"=\" * 80 +\n                   \"\\n\" +\n                   \"DAG File Processing Stats\\n\\n\" +\n                   tabulate(formatted_rows, headers=headers) +\n                   \"\\n\" +\n                   \"=\" * 80)\n\n        self.log.info(log_str)", "gt": "            file_name = os.path.splitext(file_name)[0].replace(os.sep, '.')"}
{"prefix": "def get_pid(self, file_path):\n        \"\"\"\n        :param file_path: the path to the file that's being processed\n        :type file_path: unicode\n        :return: the PID of the process processing the given file or None if\n            the specified file is not being processed\n        :rtype: int", "suffix": "        if file_path in self._processors:\n            return self._processors[file_path].pid\n        return None", "gt": "        \"\"\""}
{"prefix": "def get_runtime(self, file_path):\n        \"\"\"\n        :param file_path: the path to the file that's being processed\n        :type file_path: unicode\n        :return: the current runtime (in seconds) of the process that's", "suffix": "            being processed\n        \"\"\"\n        if file_path in self._processors:\n            return (timezone.utcnow() - self._processors[file_path].start_time)\\\n                .total_seconds()\n        return None", "gt": "            processing the specified file or None if the file is not currently"}
{"prefix": "def get_start_time(self, file_path):\n        \"\"\"\n        :param file_path: the path to the file that's being processed\n        :type file_path: unicode\n        :return: the start time of the process that's processing the", "suffix": "        :rtype: datetime\n        \"\"\"\n        if file_path in self._processors:\n            return self._processors[file_path].start_time\n        return None", "gt": "            specified file or None if the file is not currently being processed"}
{"prefix": "def set_file_paths(self, new_file_paths):\n        \"\"\"\n        Update this with a new set of paths to DAG definition files.", "suffix": "        :param new_file_paths: list of paths to DAG definition files\n        :type new_file_paths: list[unicode]\n        :return: None\n        \"\"\"\n        self._file_paths = new_file_paths\n        self._file_path_queue = [x for x in self._file_path_queue\n                                 if x in new_file_paths]\n        # Stop processors that are working on deleted files\n        filtered_processors = {}\n        for file_path, processor in self._processors.items():\n            if file_path in new_file_paths:\n                filtered_processors[file_path] = processor\n            else:\n                self.log.warning(\"Stopping processor for %s\", file_path)\n                processor.terminate()\n        self._processors = filtered_processors", "gt": ""}
{"prefix": "def wait_until_finished(self):\n        \"\"\"\n        Sleeps until all the processors are done.", "suffix": "        for file_path, processor in self._processors.items():\n            while not processor.done:\n                time.sleep(0.1)", "gt": "        \"\"\""}
{"prefix": "def heartbeat(self):\n        \"\"\"\n        This should be periodically called by the manager loop. This method will\n        kick off new processes to process DAG definition files and read the\n        results from the finished processors.\n\n        :return: a list of SimpleDags that were produced by processors that\n            have finished since the last time this was called\n        :rtype: list[airflow.utils.dag_processing.SimpleDag]\n        \"\"\"\n        finished_processors = {}\n        \"\"\":type : dict[unicode, AbstractDagFileProcessor]\"\"\"\n        running_processors = {}\n        \"\"\":type : dict[unicode, AbstractDagFileProcessor]\"\"\"\n\n        for file_path, processor in self._processors.items():\n            if processor.done:\n                self.log.debug(\"Processor for %s finished\", file_path)\n                now = timezone.utcnow()\n                finished_processors[file_path] = processor\n                self._last_runtime[file_path] = (now -\n                                                 processor.start_time).total_seconds()\n                self._last_finish_time[file_path] = now\n                self._run_count[file_path] += 1\n            else:\n                running_processors[file_path] = processor\n        self._processors = running_processors\n\n        self.log.debug(\"%s/%s DAG parsing processes running\",\n                       len(self._processors), self._parallelism)\n\n        self.log.debug(\"%s file paths queued for processing\",\n                       len(self._file_path_queue))\n\n        # Collect all the DAGs that were found in the processed files\n        simple_dags = []\n        for file_path, processor in finished_processors.items():\n            if processor.result is None:\n                self.log.warning(\n                    \"Processor for %s exited with return code %s.\",\n                    processor.file_path, processor.exit_code\n                )\n            else:\n                for simple_dag in processor.result:\n                    simple_dags.append(simple_dag)\n\n        # Generate more file paths to process if we processed all the files\n        # already.\n        if len(self._file_path_queue) == 0:\n            # If the file path is already being processed, or if a file was\n            # processed recently, wait until the next batch\n            file_paths_in_progress = self._processors.keys()\n            now = timezone.utcnow()", "suffix": "            for file_path in self._file_paths:\n                last_finish_time = self.get_last_finish_time(file_path)\n                if (last_finish_time is not None and\n                    (now - last_finish_time).total_seconds() <\n                        self._file_process_interval):\n                    file_paths_recently_processed.append(file_path)\n\n            files_paths_at_run_limit = [file_path\n                                        for file_path, num_runs in self._run_count.items()\n                                        if num_runs == self._max_runs]\n\n            files_paths_to_queue = list(set(self._file_paths) -\n                                        set(file_paths_in_progress) -\n                                        set(file_paths_recently_processed) -\n                                        set(files_paths_at_run_limit))\n\n            for file_path, processor in self._processors.items():\n                self.log.debug(\n                    \"File path %s is still being processed (started: %s)\",\n                    processor.file_path, processor.start_time.isoformat()\n                )\n\n            self.log.debug(\n                \"Queuing the following files for processing:\\n\\t%s\",\n                \"\\n\\t\".join(files_paths_to_queue)\n            )\n\n            self._file_path_queue.extend(files_paths_to_queue)\n\n        zombies = self._find_zombies()\n\n        # Start more processors if we have enough slots and files to process\n        while (self._parallelism - len(self._processors) > 0 and\n               len(self._file_path_queue) > 0):\n            file_path = self._file_path_queue.pop(0)\n            processor = self._processor_factory(file_path, zombies)\n\n            processor.start()\n            self.log.debug(\n                \"Started a process (PID: %s) to generate tasks for %s\",\n                processor.pid, file_path\n            )\n            self._processors[file_path] = processor\n\n        # Update heartbeat count.\n        self._run_count[self._heart_beat_key] += 1\n\n        return simple_dags", "gt": "            file_paths_recently_processed = []"}
{"prefix": "def _find_zombies(self, session):\n        \"\"\"\n        Find zombie task instances, which are tasks haven't heartbeated for too long.\n        :return: Zombie task instances in SimpleTaskInstance format.\n        \"\"\"\n        now = timezone.utcnow()\n        zombies = []\n        if (now - self._last_zombie_query_time).total_seconds() \\\n                > self._zombie_query_interval:\n            # to avoid circular imports\n            from airflow.jobs import LocalTaskJob as LJ\n            self.log.info(\"Finding 'running' jobs without a recent heartbeat\")\n            TI = airflow.models.TaskInstance\n            limit_dttm = timezone.utcnow() - timedelta(\n                seconds=self._zombie_threshold_secs)\n            self.log.info(\"Failing jobs without heartbeat after %s\", limit_dttm)\n\n            tis = (\n                session.query(TI)", "suffix": "                .filter(TI.state == State.RUNNING)\n                .filter(\n                    or_(\n                        LJ.state != State.RUNNING,\n                        LJ.latest_heartbeat < limit_dttm,\n                    )\n                ).all()\n            )\n            self._last_zombie_query_time = timezone.utcnow()\n            for ti in tis:\n                zombies.append(SimpleTaskInstance(ti))\n\n        return zombies", "gt": "                .join(LJ, TI.job_id == LJ.id)"}
{"prefix": "def max_runs_reached(self):\n        \"\"\"\n        :return: whether all file paths have been processed max_runs times\n        \"\"\"\n        if self._max_runs == -1:  # Unlimited runs.\n            return False\n        for file_path in self._file_paths:\n            if self._run_count[file_path] < self._max_runs:\n                return False\n        if self._run_count[self._heart_beat_key] < self._max_runs:", "suffix": "        return True", "gt": "            return False"}
{"prefix": "def end(self):\n        \"\"\"\n        Kill all child processes on exit since we don't want to leave", "suffix": "        \"\"\"\n        pids_to_kill = self.get_all_pids()\n        if len(pids_to_kill) > 0:\n            # First try SIGTERM\n            this_process = psutil.Process(os.getpid())\n            # Only check child processes to ensure that we don't have a case\n            # where we kill the wrong process because a child process died\n            # but the PID got reused.\n            child_processes = [x for x in this_process.children(recursive=True)\n                               if x.is_running() and x.pid in pids_to_kill]\n            for child in child_processes:\n                self.log.info(\"Terminating child PID: %s\", child.pid)\n                child.terminate()\n            # TODO: Remove magic number\n            timeout = 5\n            self.log.info(\"Waiting up to %s seconds for processes to exit...\", timeout)\n            try:\n                psutil.wait_procs(\n                    child_processes, timeout=timeout,\n                    callback=lambda x: self.log.info('Terminated PID %s', x.pid))\n            except psutil.TimeoutExpired:\n                self.log.debug(\"Ran out of time while waiting for processes to exit\")\n\n            # Then SIGKILL\n            child_processes = [x for x in this_process.children(recursive=True)\n                               if x.is_running() and x.pid in pids_to_kill]\n            if len(child_processes) > 0:\n                self.log.info(\"SIGKILL processes that did not terminate gracefully\")\n                for child in child_processes:\n                    self.log.info(\"Killing child PID: %s\", child.pid)\n                    child.kill()\n                    child.wait()", "gt": "        them as orphaned."}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Opens a ssh connection to the remote host.\n\n        :rtype: paramiko.client.SSHClient\n        \"\"\"\n\n        self.log.debug('Creating SSH client for conn_id: %s', self.ssh_conn_id)\n        client = paramiko.SSHClient()\n        if not self.allow_host_key_change:\n            self.log.warning('Remote Identification Change is not verified. '\n                             'This wont protect against Man-In-The-Middle attacks')\n            client.load_system_host_keys()\n        if self.no_host_key_check:\n            self.log.warning('No Host Key Verification. This wont protect '\n                             'against Man-In-The-Middle attacks')\n            # Default is RejectPolicy\n            client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n        if self.password and self.password.strip():\n            client.connect(hostname=self.remote_host,\n                           username=self.username,\n                           password=self.password,\n                           key_filename=self.key_file,", "suffix": "                           compress=self.compress,\n                           port=self.port,\n                           sock=self.host_proxy)\n        else:\n            client.connect(hostname=self.remote_host,\n                           username=self.username,\n                           key_filename=self.key_file,\n                           timeout=self.timeout,\n                           compress=self.compress,\n                           port=self.port,\n                           sock=self.host_proxy)\n\n        if self.keepalive_interval:\n            client.get_transport().set_keepalive(self.keepalive_interval)\n\n        self.client = client\n        return client", "gt": "                           timeout=self.timeout,"}
{"prefix": "def get_tunnel(self, remote_port, remote_host=\"localhost\", local_port=None):\n        \"\"\"\n        Creates a tunnel between two hosts. Like ssh -L <LOCAL_PORT>:host:<REMOTE_PORT>.\n\n        :param remote_port: The remote port to create a tunnel to\n        :type remote_port: int\n        :param remote_host: The remote host to create a tunnel to (default localhost)\n        :type remote_host: str\n        :param local_port:  The local port to attach the tunnel to\n        :type local_port: int\n\n        :return: sshtunnel.SSHTunnelForwarder object\n        \"\"\"\n\n        if local_port:\n            local_bind_address = ('localhost', local_port)\n        else:\n            local_bind_address = ('localhost',)\n\n        if self.password and self.password.strip():\n            client = SSHTunnelForwarder(self.remote_host,\n                                        ssh_port=self.port,\n                                        ssh_username=self.username,\n                                        ssh_password=self.password,\n                                        ssh_pkey=self.key_file,\n                                        ssh_proxy=self.host_proxy,\n                                        local_bind_address=local_bind_address,\n                                        remote_bind_address=(remote_host, remote_port),\n                                        logger=self.log)", "suffix": "            client = SSHTunnelForwarder(self.remote_host,\n                                        ssh_port=self.port,\n                                        ssh_username=self.username,\n                                        ssh_pkey=self.key_file,\n                                        ssh_proxy=self.host_proxy,\n                                        local_bind_address=local_bind_address,\n                                        remote_bind_address=(remote_host, remote_port),\n                                        host_pkey_directories=[],\n                                        logger=self.log)\n\n        return client", "gt": "        else:"}
{"prefix": "def create_transfer_job(self, body):\n        \"\"\"\n        Creates a transfer job that runs periodically.\n\n        :param body: (Required) A request body, as described in\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body\n        :type body: dict\n        :return: transfer job.\n            See:", "suffix": "        :rtype: dict\n        \"\"\"\n        body = self._inject_project_id(body, BODY, PROJECT_ID)\n        return self.get_conn().transferJobs().create(body=body).execute(num_retries=self.num_retries)", "gt": "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob"}
{"prefix": "def get_transfer_job(self, job_name, project_id=None):\n        \"\"\"\n        Gets the latest state of a long-running operation in Google Storage\n        Transfer Service.\n\n        :param job_name: (Required) Name of the job to be fetched\n        :type job_name: str\n        :param project_id: (Optional) the ID of the project that owns the Transfer\n            Job. If set to None or missing, the default project_id from the GCP\n            connection is used.\n        :type project_id: str\n        :return: Transfer Job\n        :rtype: dict\n        \"\"\"\n        return (", "suffix": "            .transferJobs()\n            .get(jobName=job_name, projectId=project_id)\n            .execute(num_retries=self.num_retries)\n        )", "gt": "            self.get_conn()"}
{"prefix": "def list_transfer_job(self, filter):\n        \"\"\"\n        Lists long-running operations in Google Storage Transfer\n        Service that match the specified filter.\n\n        :param filter: (Required) A request filter, as described in\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter\n        :type filter: dict\n        :return: List of Transfer Jobs\n        :rtype: list[dict]\n        \"\"\"\n        conn = self.get_conn()\n        filter = self._inject_project_id(filter, FILTER, FILTER_PROJECT_ID)\n        request = conn.transferJobs().list(filter=json.dumps(filter))\n        jobs = []", "suffix": "        while request is not None:\n            response = request.execute(num_retries=self.num_retries)\n            jobs.extend(response[TRANSFER_JOBS])\n\n            request = conn.transferJobs().list_next(previous_request=request, previous_response=response)\n\n        return jobs", "gt": ""}
{"prefix": "def update_transfer_job(self, job_name, body):\n        \"\"\"\n        Updates a transfer job that runs periodically.\n\n        :param job_name: (Required) Name of the job to be updated\n        :type job_name: str\n        :param body: A request body, as described in\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body", "suffix": "        :return: If successful, TransferJob.\n        :rtype: dict\n        \"\"\"\n        body = self._inject_project_id(body, BODY, PROJECT_ID)\n        return (\n            self.get_conn()\n            .transferJobs()\n            .patch(jobName=job_name, body=body)\n            .execute(num_retries=self.num_retries)\n        )", "gt": "        :type body: dict"}
{"prefix": "def delete_transfer_job(self, job_name, project_id):\n        \"\"\"\n        Deletes a transfer job. This is a soft delete. After a transfer job is\n        deleted, the job and all the transfer executions are subject to garbage\n        collection. Transfer jobs become eligible for garbage collection\n        30 days after soft delete.\n", "suffix": "        :type job_name: str\n        :param project_id: (Optional) the ID of the project that owns the Transfer\n            Job. If set to None or missing, the default project_id from the GCP\n            connection is used.\n        :type project_id: str\n        :rtype: None\n        \"\"\"\n\n        return (\n            self.get_conn()\n            .transferJobs()\n            .patch(\n                jobName=job_name,\n                body={\n                    PROJECT_ID: project_id,\n                    TRANSFER_JOB: {STATUS1: GcpTransferJobsStatus.DELETED},\n                    TRANSFER_JOB_FIELD_MASK: STATUS1,\n                },\n            )\n            .execute(num_retries=self.num_retries)\n        )", "gt": "        :param job_name: (Required) Name of the job to be deleted"}
{"prefix": "def cancel_transfer_operation(self, operation_name):\n        \"\"\"", "suffix": "\n        :param operation_name: Name of the transfer operation.\n        :type operation_name: str\n        :rtype: None\n        \"\"\"\n        self.get_conn().transferOperations().cancel(name=operation_name).execute(num_retries=self.num_retries)", "gt": "        Cancels an transfer operation in Google Storage Transfer Service."}
{"prefix": "def get_transfer_operation(self, operation_name):", "suffix": "        Gets an transfer operation in Google Storage Transfer Service.\n\n        :param operation_name: (Required) Name of the transfer operation.\n        :type operation_name: str\n        :return: transfer operation\n            See:\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/Operation\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .transferOperations()\n            .get(name=operation_name)\n            .execute(num_retries=self.num_retries)\n        )", "gt": "        \"\"\""}
{"prefix": "def list_transfer_operations(self, filter):\n        \"\"\"\n        Gets an transfer operation in Google Storage Transfer Service.\n\n        :param filter: (Required) A request filter, as described in\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter\n            With one additional improvement:\n\n            * project_id is optional if you have a project id defined\n              in the connection\n              See: :ref:`howto/connection:gcp`\n\n        :type filter: dict\n        :return: transfer operation\n        :rtype: list[dict]\n        \"\"\"\n        conn = self.get_conn()\n\n        filter = self._inject_project_id(filter, FILTER, FILTER_PROJECT_ID)\n\n        operations = []\n\n        request = conn.transferOperations().list(name=TRANSFER_OPERATIONS, filter=json.dumps(filter))\n\n        while request is not None:\n            response = request.execute(num_retries=self.num_retries)\n            if OPERATIONS in response:\n                operations.extend(response[OPERATIONS])\n\n            request = conn.transferOperations().list_next(\n                previous_request=request, previous_response=response", "suffix": "\n        return operations", "gt": "            )"}
{"prefix": "def pause_transfer_operation(self, operation_name):\n        \"\"\"\n        Pauses an transfer operation in Google Storage Transfer Service.\n", "suffix": "        :type operation_name: str\n        :rtype: None\n        \"\"\"\n        self.get_conn().transferOperations().pause(name=operation_name).execute(num_retries=self.num_retries)", "gt": "        :param operation_name: (Required) Name of the transfer operation."}
{"prefix": "def resume_transfer_operation(self, operation_name):\n        \"\"\"\n        Resumes an transfer operation in Google Storage Transfer Service.\n\n        :param operation_name: (Required) Name of the transfer operation.\n        :type operation_name: str", "suffix": "        \"\"\"\n        self.get_conn().transferOperations().resume(name=operation_name).execute(num_retries=self.num_retries)", "gt": "        :rtype: None"}
{"prefix": "def wait_for_transfer_job(self, job, expected_statuses=(GcpTransferOperationStatus.SUCCESS,), timeout=60):\n        \"\"\"\n        Waits until the job reaches the expected state.\n\n        :param job: Transfer job\n            See:\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob\n        :type job: dict\n        :param expected_statuses: State that is expected\n            See:", "suffix": "        :type expected_statuses: set[str]\n        :param timeout:\n        :type timeout: time in which the operation must end in seconds\n        :rtype: None\n        \"\"\"\n        while timeout > 0:\n            operations = self.list_transfer_operations(\n                filter={FILTER_PROJECT_ID: job[PROJECT_ID], FILTER_JOB_NAMES: [job[NAME]]}\n            )\n\n            if GCPTransferServiceHook.operations_contain_expected_statuses(operations, expected_statuses):\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n            timeout -= TIME_TO_SLEEP_IN_SECONDS\n        raise AirflowException(\"Timeout. The operation could not be completed within the allotted time.\")", "gt": "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status"}
{"prefix": "def operations_contain_expected_statuses(operations, expected_statuses):\n        \"\"\"\n        Checks whether the operation list has an operation with the\n        expected status, then returns true\n        If it encounters operations in FAILED or ABORTED state\n        throw :class:`airflow.exceptions.AirflowException`.\n\n        :param operations: (Required) List of transfer operations to check.\n        :type operations: list[dict]\n        :param expected_statuses: (Required) status that is expected\n            See:\n            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status\n        :type expected_statuses: set[str]\n        :return: If there is an operation with the expected state\n            in the operation list, returns true,\n        :raises: airflow.exceptions.AirflowException If it encounters operations\n            with a state in the list,\n        :rtype: bool\n        \"\"\"\n        expected_statuses = (\n            {expected_statuses} if isinstance(expected_statuses, six.string_types) else set(expected_statuses)\n        )\n        if len(operations) == 0:\n            return False", "suffix": "        current_statuses = {operation[METADATA][STATUS] for operation in operations}\n\n        if len(current_statuses - set(expected_statuses)) != len(current_statuses):\n            return True\n\n        if len(NEGATIVE_STATUSES - current_statuses) != len(NEGATIVE_STATUSES):\n            raise AirflowException(\n                'An unexpected operation status was encountered. Expected: {}'.format(\n                    \", \".join(expected_statuses)\n                )\n            )\n        return False", "gt": ""}
{"prefix": "def find_for_task_instance(task_instance, session):\n        \"\"\"\n        Returns all task reschedules for the task instance and try number,\n        in ascending order.\n\n        :param task_instance: the task instance to find task reschedules for\n        :type task_instance: airflow.models.TaskInstance", "suffix": "        TR = TaskReschedule\n        return (\n            session\n            .query(TR)\n            .filter(TR.dag_id == task_instance.dag_id,\n                    TR.task_id == task_instance.task_id,\n                    TR.execution_date == task_instance.execution_date,\n                    TR.try_number == task_instance.try_number)\n            .order_by(asc(TR.id))\n            .all()\n        )", "gt": "        \"\"\""}
{"prefix": "def _strip_unsafe_kubernetes_special_chars(string):\n        \"\"\"\n        Kubernetes only supports lowercase alphanumeric characters and \"-\" and \".\" in\n        the pod name\n        However, there are special rules about how \"-\" and \".\" can be used so let's\n        only keep", "suffix": "        https://kubernetes.io/docs/concepts/overview/working-with-objects/names/\n\n        :param string: The requested Pod name\n        :return: ``str`` Pod name stripped of any unsafe characters\n        \"\"\"\n        return ''.join(ch.lower() for ind, ch in enumerate(string) if ch.isalnum())", "gt": "        alphanumeric chars  see here for detail:"}
{"prefix": "def _make_safe_pod_id(safe_dag_id, safe_task_id, safe_uuid):\n        \"\"\"\n        Kubernetes pod names must be <= 253 chars and must pass the following regex for\n        validation\n        \"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$\"\n\n        :param safe_dag_id: a dag_id with only alphanumeric characters\n        :param safe_task_id: a task_id with only alphanumeric characters\n        :param random_uuid: a uuid\n        :return: ``str`` valid Pod name of appropriate length\n        \"\"\"\n        MAX_POD_ID_LEN = 253\n\n        safe_key = safe_dag_id + safe_task_id", "suffix": "        safe_pod_id = safe_key[:MAX_POD_ID_LEN - len(safe_uuid) - 1] + \"-\" + safe_uuid\n\n        return safe_pod_id", "gt": ""}
{"prefix": "", "suffix": "        \"\"\"\n        Valid label values must be 63 characters or less and must be empty or begin and\n        end with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (_),\n        dots (.), and alphanumerics between.\n\n        If the label value is then greater than 63 chars once made safe, or differs in any\n        way from the original value sent to this function, then we need to truncate to\n        53chars, and append it with a unique hash.\n        \"\"\"\n        MAX_LABEL_LEN = 63\n\n        safe_label = re.sub(r'^[^a-z0-9A-Z]*|[^a-zA-Z0-9_\\-\\.]|[^a-z0-9A-Z]*$', '', string)\n\n        if len(safe_label) > MAX_LABEL_LEN or string != safe_label:\n            safe_hash = hashlib.md5(string.encode()).hexdigest()[:9]\n            safe_label = safe_label[:MAX_LABEL_LEN - len(safe_hash) - 1] + \"-\" + safe_hash\n\n        return safe_label", "gt": "def _make_safe_label_value(string):"}
{"prefix": "def clear_not_launched_queued_tasks(self, session=None):\n        \"\"\"\n        If the airflow scheduler restarts with pending \"Queued\" tasks, the tasks may or\n        may not\n        have been launched Thus, on starting up the scheduler let's check every\n        \"Queued\" task to\n        see if it has been launched (ie: if there is a corresponding pod on kubernetes)\n\n        If it has been launched then do nothing, otherwise reset the state to \"None\" so\n        the task\n        will be rescheduled\n\n        This will not be necessary in a future version of airflow in which there is\n        proper support\n        for State.LAUNCHED\n        \"\"\"\n        queued_tasks = session\\\n            .query(TaskInstance)\\\n            .filter(TaskInstance.state == State.QUEUED).all()\n        self.log.info(\n            'When executor started up, found %s queued task instances',\n            len(queued_tasks)\n        )\n\n        for task in queued_tasks:\n            dict_string = (", "suffix": "                    AirflowKubernetesScheduler._make_safe_label_value(task.dag_id),\n                    AirflowKubernetesScheduler._make_safe_label_value(task.task_id),\n                    AirflowKubernetesScheduler._datetime_to_label_safe_datestring(\n                        task.execution_date\n                    ),\n                    self.worker_uuid\n                )\n            )\n            kwargs = dict(label_selector=dict_string)\n            pod_list = self.kube_client.list_namespaced_pod(\n                self.kube_config.kube_namespace, **kwargs)\n            if len(pod_list.items) == 0:\n                self.log.info(\n                    'TaskInstance: %s found in queued state but was not launched, '\n                    'rescheduling', task\n                )\n                session.query(TaskInstance).filter(\n                    TaskInstance.dag_id == task.dag_id,\n                    TaskInstance.task_id == task.task_id,\n                    TaskInstance.execution_date == task.execution_date\n                ).update({TaskInstance.state: State.NONE})", "gt": "                \"dag_id={},task_id={},execution_date={},airflow-worker={}\".format("}
{"prefix": "def open_slots(self, session):\n        \"\"\"\n        Returns the number of slots open at the moment", "suffix": "        from airflow.models.taskinstance import \\\n            TaskInstance as TI  # Avoid circular import\n\n        used_slots = session.query(func.count()).filter(TI.pool == self.pool).filter(\n            TI.state.in_([State.RUNNING, State.QUEUED])).scalar()\n        return self.slots - used_slots", "gt": "        \"\"\""}
{"prefix": "def expand_env_var(env_var):\n    \"\"\"\n    Expands (potentially nested) env vars by repeatedly applying\n    `expandvars` and `expanduser` until interpolation stops having", "suffix": "    \"\"\"\n    if not env_var:\n        return env_var\n    while True:\n        interpolated = os.path.expanduser(os.path.expandvars(str(env_var)))\n        if interpolated == env_var:\n            return interpolated\n        else:\n            env_var = interpolated", "gt": "    any effect."}
{"prefix": "def run_command(command):\n    \"\"\"\n    Runs command and returns stdout\n    \"\"\"\n    process = subprocess.Popen(\n        shlex.split(command),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,", "suffix": "    output, stderr = [stream.decode(sys.getdefaultencoding(), 'ignore')\n                      for stream in process.communicate()]\n\n    if process.returncode != 0:\n        raise AirflowConfigException(\n            \"Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}\"\n            .format(command, process.returncode, output, stderr)\n        )\n\n    return output", "gt": "        close_fds=True)"}
{"prefix": "def parameterized_config(template):\n    \"\"\"", "suffix": "    current scope\n    :param template: a config content templated with {{variables}}\n    \"\"\"\n    all_vars = {k: v for d in [globals(), locals()] for k, v in d.items()}\n    return template.format(**all_vars)", "gt": "    Generates a configuration from the provided template + variables defined in"}
{"prefix": "def remove_option(self, section, option, remove_default=True):", "suffix": "        Remove an option if it exists in config from a file or\n        default config. If both of config have the same option, this removes\n        the option in both configs unless remove_default=False.\n        \"\"\"\n        if super().has_option(section, option):\n            super().remove_option(section, option)\n\n        if self.airflow_defaults.has_option(section, option) and remove_default:\n            self.airflow_defaults.remove_option(section, option)", "gt": "        \"\"\""}
{"prefix": "def getsection(self, section):\n        \"\"\"\n        Returns the section as a dict. Values are converted to int, float, bool\n        as required.\n\n        :param section: section from the config\n        :rtype: dict\n        \"\"\"\n        if (section not in self._sections and", "suffix": "            return None\n\n        _section = copy.deepcopy(self.airflow_defaults._sections[section])\n\n        if section in self._sections:\n            _section.update(copy.deepcopy(self._sections[section]))\n\n        section_prefix = 'AIRFLOW__{S}__'.format(S=section.upper())\n        for env_var in sorted(os.environ.keys()):\n            if env_var.startswith(section_prefix):\n                key = env_var.replace(section_prefix, '').lower()\n                _section[key] = self._get_env_var_option(section, key)\n\n        for key, val in iteritems(_section):\n            try:\n                val = int(val)\n            except ValueError:\n                try:\n                    val = float(val)\n                except ValueError:\n                    if val.lower() in ('t', 'true'):\n                        val = True\n                    elif val.lower() in ('f', 'false'):\n                        val = False\n            _section[key] = val\n        return _section", "gt": "                section not in self.airflow_defaults._sections):"}
{"prefix": "def as_dict(\n            self, display_source=False, display_sensitive=False, raw=False):\n        \"\"\"\n        Returns the current configuration as an OrderedDict of OrderedDicts.\n        :param display_source: If False, the option value is returned. If True,\n            a tuple of (option_value, source) is returned. Source is either\n            'airflow.cfg', 'default', 'env var', or 'cmd'.\n        :type display_source: bool\n        :param display_sensitive: If True, the values of options set by env\n            vars and bash commands will be displayed. If False, those options\n            are shown as '< hidden >'\n        :type display_sensitive: bool\n        :param raw: Should the values be output as interpolated values, or the\n            \"raw\" form that can be fed back in to ConfigParser\n        :type raw: bool\n        \"\"\"\n        cfg = {}\n        configs = [\n            ('default', self.airflow_defaults),\n            ('airflow.cfg', self),\n        ]\n\n        for (source_name, config) in configs:\n            for section in config.sections():\n                sect = cfg.setdefault(section, OrderedDict())\n                for (k, val) in config.items(section=section, raw=raw):\n                    if display_source:\n                        val = (val, source_name)\n                    sect[k] = val\n\n        # add env vars and overwrite because they have priority\n        for ev in [ev for ev in os.environ if ev.startswith('AIRFLOW__')]:\n            try:\n                _, section, key = ev.split('__')\n                opt = self._get_env_var_option(section, key)\n            except ValueError:\n                continue\n            if not display_sensitive and ev != 'AIRFLOW__CORE__UNIT_TEST_MODE':\n                opt = '< hidden >'\n            elif raw:\n                opt = opt.replace('%', '%%')\n            if display_source:\n                opt = (opt, 'env var')\n            cfg.setdefault(section.lower(), OrderedDict()).update(\n                {key.lower(): opt})\n\n        # add bash commands\n        for (section, key) in self.as_command_stdout:\n            opt = self._get_cmd_option(section, key)\n            if opt:\n                if not display_sensitive:\n                    opt = '< hidden >'\n                if display_source:\n                    opt = (opt, 'cmd')\n                elif raw:", "suffix": "                cfg.setdefault(section, OrderedDict()).update({key: opt})\n                del cfg[section][key + '_cmd']\n\n        return cfg", "gt": "                    opt = opt.replace('%', '%%')"}
{"prefix": "", "suffix": "        \"\"\"\n        Allocate IDs for incomplete keys.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds\n\n        :param partial_keys: a list of partial keys.\n        :type partial_keys: list\n        :return: a list of full keys.\n        :rtype: list\n        \"\"\"\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .allocateIds(projectId=self.project_id, body={'keys': partial_keys})\n                .execute(num_retries=self.num_retries))\n\n        return resp['keys']", "gt": "def allocate_ids(self, partial_keys):"}
{"prefix": "def begin_transaction(self):\n        \"\"\"\n        Begins a new transaction.\n", "suffix": "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/beginTransaction\n\n        :return: a transaction handle.\n        :rtype: str\n        \"\"\"\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .beginTransaction(projectId=self.project_id, body={})\n                .execute(num_retries=self.num_retries))\n\n        return resp['transaction']", "gt": "        .. seealso::"}
{"prefix": "def commit(self, body):\n        \"\"\"\n        Commit a transaction, optionally creating, deleting or modifying some entities.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/commit\n\n        :param body: the body of the commit request.\n        :type body: dict\n        :return: the response body of the commit request.\n        :rtype: dict\n        \"\"\"\n        conn = self.get_conn()", "suffix": "        resp = (conn\n                .projects()\n                .commit(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp", "gt": ""}
{"prefix": "def lookup(self, keys, read_consistency=None, transaction=None):\n        \"\"\"\n        Lookup some entities by key.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/lookup\n\n        :param keys: the keys to lookup.\n        :type keys: list\n        :param read_consistency: the read consistency to use. default, strong or eventual.\n                                 Cannot be used with a transaction.\n        :type read_consistency: str\n        :param transaction: the transaction to use, if any.\n        :type transaction: str\n        :return: the response body of the lookup request.\n        :rtype: dict\n        \"\"\"\n        conn = self.get_conn()\n\n        body = {'keys': keys}\n        if read_consistency:\n            body['readConsistency'] = read_consistency\n        if transaction:\n            body['transaction'] = transaction", "suffix": "                .projects()\n                .lookup(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp", "gt": "        resp = (conn"}
{"prefix": "def rollback(self, transaction):\n        \"\"\"\n        Roll back a transaction.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/rollback\n\n        :param transaction: the transaction to roll back.\n        :type transaction: str\n        \"\"\"\n        conn = self.get_conn()", "suffix": "        conn.projects().rollback(\n            projectId=self.project_id, body={'transaction': transaction}\n        ).execute(num_retries=self.num_retries)", "gt": ""}
{"prefix": "def run_query(self, body):\n        \"\"\"\n        Run a query for entities.", "suffix": "        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/runQuery\n\n        :param body: the body of the query request.\n        :type body: dict\n        :return: the batch of query results.\n        :rtype: dict\n        \"\"\"\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .runQuery(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp['batch']", "gt": ""}
{"prefix": "def get_operation(self, name):\n        \"\"\"\n        Gets the latest state of a long-running operation.\n", "suffix": "            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/get\n\n        :param name: the name of the operation resource.\n        :type name: str\n        :return: a resource operation instance.\n        :rtype: dict\n        \"\"\"\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .operations()\n                .get(name=name)\n                .execute(num_retries=self.num_retries))\n\n        return resp", "gt": "        .. seealso::"}
{"prefix": "def delete_operation(self, name):\n        \"\"\"\n        Deletes the long-running operation.", "suffix": "        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/delete\n\n        :param name: the name of the operation resource.\n        :type name: str\n        :return: none if successful.\n        :rtype: dict\n        \"\"\"\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .operations()\n                .delete(name=name)\n                .execute(num_retries=self.num_retries))\n\n        return resp", "gt": ""}
{"prefix": "def poll_operation_until_done(self, name, polling_interval_in_seconds):\n        \"\"\"\n        Poll backup operation state until it's completed.\n\n        :param name: the name of the operation resource\n        :type name: str\n        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.\n        :type polling_interval_in_seconds: int\n        :return: a resource operation instance.", "suffix": "        \"\"\"\n        while True:\n            result = self.get_operation(name)\n\n            state = result['metadata']['common']['state']\n            if state == 'PROCESSING':\n                self.log.info('Operation is processing. Re-polling state in {} seconds'\n                              .format(polling_interval_in_seconds))\n                time.sleep(polling_interval_in_seconds)\n            else:\n                return result", "gt": "        :rtype: dict"}
{"prefix": "def export_to_storage_bucket(self, bucket, namespace=None, entity_filter=None, labels=None):\n        \"\"\"\n        Export entities from Cloud Datastore to Cloud Storage for backup.\n\n        .. note::\n            Keep in mind that this requests the Admin API not the Data API.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/export\n\n        :param bucket: The name of the Cloud Storage bucket.\n        :type bucket: str\n        :param namespace: The Cloud Storage namespace path.\n        :type namespace: str\n        :param entity_filter: Description of what data from the project is included in the export.\n        :type entity_filter: dict\n        :param labels: Client-assigned labels.\n        :type labels: dict of str\n        :return: a resource operation instance.\n        :rtype: dict\n        \"\"\"\n        admin_conn = self.get_conn()\n\n        output_uri_prefix = 'gs://' + '/'.join(filter(None, [bucket, namespace]))\n        if not entity_filter:\n            entity_filter = {}\n        if not labels:\n            labels = {}\n        body = {\n            'outputUrlPrefix': output_uri_prefix,\n            'entityFilter': entity_filter,\n            'labels': labels,", "suffix": "        resp = (admin_conn\n                .projects()\n                .export(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp", "gt": "        }"}
{"prefix": "def import_from_storage_bucket(self, bucket, file, namespace=None, entity_filter=None, labels=None):\n        \"\"\"\n        Import a backup from Cloud Storage to Cloud Datastore.\n\n        .. note::\n            Keep in mind that this requests the Admin API not the Data API.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/import\n\n        :param bucket: The name of the Cloud Storage bucket.\n        :type bucket: str\n        :param file: the metadata file written by the projects.export operation.\n        :type file: str\n        :param namespace: The Cloud Storage namespace path.\n        :type namespace: str\n        :param entity_filter: specify which kinds/namespaces are to be imported.\n        :type entity_filter: dict\n        :param labels: Client-assigned labels.\n        :type labels: dict of str\n        :return: a resource operation instance.\n        :rtype: dict\n        \"\"\"\n        admin_conn = self.get_conn()\n\n        input_url = 'gs://' + '/'.join(filter(None, [bucket, namespace, file]))\n        if not entity_filter:\n            entity_filter = {}\n        if not labels:\n            labels = {}\n        body = {\n            'inputUrl': input_url,\n            'entityFilter': entity_filter,", "suffix": "        }\n        resp = (admin_conn\n                .projects()\n                .import_(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp", "gt": "            'labels': labels,"}
{"prefix": "def publish_to_target(self, target_arn, message):\n        \"\"\"\n        Publish a message to a topic or an endpoint.\n\n        :param target_arn: either a TopicArn or an EndpointArn\n        :type target_arn: str\n        :param message: the default message you want to send\n        :param message: str\n        \"\"\"", "suffix": "        conn = self.get_conn()\n\n        messages = {\n            'default': message\n        }\n\n        return conn.publish(\n            TargetArn=target_arn,\n            Message=json.dumps(messages),\n            MessageStructure='json'\n        )", "gt": ""}
{"prefix": "def get_hostname():\n    \"\"\"\n    Fetch the hostname using the callable from the config or using\n    `socket.getfqdn` as a fallback.\n    \"\"\"\n    # First we attempt to fetch the callable path from the config.\n    try:\n        callable_path = conf.get('core', 'hostname_callable')\n    except AirflowConfigException:\n        callable_path = None\n\n    # Then we handle the case when the config is missing or empty. This is the\n    # default behavior.\n    if not callable_path:\n        return socket.getfqdn()\n\n    # Since we have a callable path, we try to import and run it next.\n    module_path, attr_name = callable_path.split(':')", "suffix": "    callable = getattr(module, attr_name)\n    return callable()", "gt": "    module = importlib.import_module(module_path)"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Natural Language service.\n\n        :return: Cloud Natural Language service object\n        :rtype: google.cloud.language_v1.LanguageServiceClient", "suffix": "        if not self._conn:\n            self._conn = LanguageServiceClient(credentials=self._get_credentials())\n        return self._conn", "gt": "        \"\"\""}
{"prefix": "def analyze_entities(self, document, encoding_type=None, retry=None, timeout=None, metadata=None):\n        \"\"\"\n        Finds named entities in the text along with entity types,\n        salience, mentions for each entity, and other properties.\n\n        :param document: Input document.\n            If a dict is provided, it must be of the same form as the protobuf message Document\n        :type document: dict or class google.cloud.language_v1.types.Document\n        :param encoding_type: The encoding type used by the API to calculate offsets.\n        :type encoding_type: google.cloud.language_v1.types.EncodingType\n        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n            retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n            retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        :param metadata: Additional metadata that is provided to the method.\n        :type metadata: sequence[tuple[str, str]]]\n        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse\n        \"\"\"", "suffix": "\n        return client.analyze_entities(\n            document=document, encoding_type=encoding_type, retry=retry, timeout=timeout, metadata=metadata\n        )", "gt": "        client = self.get_conn()"}
{"prefix": "def annotate_text(self, document, features, encoding_type=None, retry=None, timeout=None, metadata=None):\n        \"\"\"\n        A convenience method that provides all the features that analyzeSentiment,\n        analyzeEntities, and analyzeSyntax provide in one call.\n\n        :param document: Input document.\n            If a dict is provided, it must be of the same form as the protobuf message Document\n        :type document: dict or google.cloud.language_v1.types.Document\n        :param features: The enabled features.\n            If a dict is provided, it must be of the same form as the protobuf message Features\n        :type features: dict or google.cloud.language_v1.enums.Features\n        :param encoding_type: The encoding type used by the API to calculate offsets.\n        :type encoding_type: google.cloud.language_v1.types.EncodingType\n        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n            retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n            retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        :param metadata: Additional metadata that is provided to the method.\n        :type metadata: sequence[tuple[str, str]]]\n        :rtype: google.cloud.language_v1.types.AnnotateTextResponse\n        \"\"\"\n        client = self.get_conn()", "suffix": "        return client.annotate_text(\n            document=document,\n            features=features,\n            encoding_type=encoding_type,\n            retry=retry,\n            timeout=timeout,\n            metadata=metadata,\n        )", "gt": ""}
{"prefix": "def classify_text(self, document, retry=None, timeout=None, metadata=None):\n        \"\"\"\n        Classifies a document into categories.\n\n        :param document: Input document.", "suffix": "        :type document: dict or class google.cloud.language_v1.types.Document\n        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n            retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n            retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        :param metadata: Additional metadata that is provided to the method.\n        :type metadata: sequence[tuple[str, str]]]\n        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse\n        \"\"\"\n        client = self.get_conn()\n\n        return client.classify_text(document=document, retry=retry, timeout=timeout, metadata=metadata)", "gt": "            If a dict is provided, it must be of the same form as the protobuf message Document"}
{"prefix": "def get_task(dag_id, task_id):\n    \"\"\"Return the task object identified by the given dag_id and task_id.\"\"\"", "suffix": "\n    # Check DAG exists.\n    if dag_id not in dagbag.dags:\n        error_message = \"Dag id {} not found\".format(dag_id)\n        raise DagNotFound(error_message)\n\n    # Get DAG object and check Task Exists\n    dag = dagbag.get_dag(dag_id)\n    if not dag.has_task(task_id):\n        error_message = 'Task {} not found in dag {}'.format(task_id, dag_id)\n        raise TaskNotFound(error_message)\n\n    # Return the task.\n    return dag.get_task(task_id)", "gt": "    dagbag = DagBag()"}
{"prefix": "def get_template_field(env, fullname):\n    \"\"\"\n    Gets template fields for specific operator class.\n\n    :param fullname: Full path to operator class.\n        For example: ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetCreateOperator``\n    :return: List of template field\n    :rtype: list[str]\n    \"\"\"\n    modname, classname = fullname.rsplit(\".\", 1)\n\n    try:", "suffix": "            mod = import_module(modname)\n    except ImportError:\n        raise RoleException(\"Error loading %s module.\" % (modname, ))\n\n    clazz = getattr(mod, classname)\n    if not clazz:\n        raise RoleException(\"Error finding %s class in %s module.\" % (classname, modname))\n\n    template_fields = getattr(clazz, \"template_fields\")\n\n    if not template_fields:\n        raise RoleException(\n            \"Could not find the template fields for %s class in %s module.\" % (classname, modname)\n        )\n\n    return list(template_fields)", "gt": "        with mock(env.config.autodoc_mock_imports):"}
{"prefix": "def template_field_role(app, typ, rawtext, text, lineno, inliner, options={}, content=[]):\n    \"\"\"\n    A role that allows you to include a list of template fields in the middle of the text. This is especially\n    useful when writing guides describing how to use the operator.\n    The result is a list of fields where each field is shorted in the literal block.\n\n    Sample usage::\n\n    :template-fields:`airflow.contrib.operators.gcp_natural_language_operator.CloudLanguageAnalyzeSentimentOperator`\n\n    For further information look at:", "suffix": "    * [http://docutils.sourceforge.net/docs/howto/rst-roles.html](Creating reStructuredText Interpreted\n      Text Roles)\n    \"\"\"\n    text = utils.unescape(text)\n\n    try:\n        template_fields = get_template_field(app.env, text)\n    except RoleException as e:\n        msg = inliner.reporter.error(\"invalid class name %s \\n%s\" % (text, e, ), line=lineno)\n        prb = inliner.problematic(rawtext, rawtext, msg)\n        return [prb], [msg]\n\n    node = nodes.inline(rawtext=rawtext)\n    for i, field in enumerate(template_fields):\n        if i != 0:\n            node += nodes.Text(\", \")\n        node += nodes.literal(field, \"\", nodes.Text(field))\n\n    return [node], []", "gt": ""}
{"prefix": "def dispose_orm():", "suffix": "    log.debug(\"Disposing DB connection pool (PID %s)\", os.getpid())\n    global engine\n    global Session\n\n    if Session:\n        Session.remove()\n        Session = None\n    if engine:\n        engine.dispose()\n        engine = None", "gt": "    \"\"\" Properly close pooled database connections \"\"\""}
{"prefix": "", "suffix": "    \"\"\"\n    Ensures that certain subfolders of AIRFLOW_HOME are on the classpath\n    \"\"\"\n\n    if DAGS_FOLDER not in sys.path:\n        sys.path.append(DAGS_FOLDER)\n\n    # Add ./config/ for loading custom log parsers etc, or\n    # airflow_local_settings etc.\n    config_path = os.path.join(AIRFLOW_HOME, 'config')\n    if config_path not in sys.path:\n        sys.path.append(config_path)\n\n    if PLUGINS_FOLDER not in sys.path:\n        sys.path.append(PLUGINS_FOLDER)", "gt": "def prepare_classpath():"}
{"prefix": "", "suffix": "        \"\"\"\n        Gets the returned Celery result from the Airflow task\n        ID provided to the sensor, and returns True if the\n        celery result has been finished execution.\n\n        :param context: Airflow's execution context\n        :type context: dict\n        :return: True if task has been executed, otherwise False\n        :rtype: bool\n        \"\"\"\n        ti = context['ti']\n        celery_result = ti.xcom_pull(task_ids=self.target_task_id)\n        return celery_result.ready()", "gt": "def _check_task_id(self, context):"}
{"prefix": "def detect_conf_var():\n    \"\"\"Return true if the ticket cache contains \"conf\" information as is found\n    in ticket caches of Kerberos 1.8.1 or later. This is incompatible with the\n    Sun Java Krb5LoginModule in Java6, so we need to take an action to work\n    around it.", "suffix": "    ticket_cache = configuration.conf.get('kerberos', 'ccache')\n\n    with open(ticket_cache, 'rb') as f:\n        # Note: this file is binary, so we check against a bytearray.\n        return b'X-CACHECONF:' in f.read()", "gt": "    \"\"\""}
{"prefix": "def alchemy_to_dict(obj):\n    \"\"\"\n    Transforms a SQLAlchemy model instance into a dictionary\n    \"\"\"\n    if not obj:\n        return None", "suffix": "    for c in obj.__table__.columns:\n        value = getattr(obj, c.name)\n        if type(value) == datetime:\n            value = value.isoformat()\n        d[c.name] = value\n    return d", "gt": "    d = {}"}
{"prefix": "", "suffix": "    \"\"\"\n    Yield successive chunks of a given size from a list of items\n    \"\"\"\n    if chunk_size <= 0:\n        raise ValueError('Chunk size must be a positive integer')\n    for i in range(0, len(items), chunk_size):\n        yield items[i:i + chunk_size]", "gt": "def chunks(items, chunk_size):"}
{"prefix": "def reduce_in_chunks(fn, iterable, initializer, chunk_size=0):\n    \"\"\"\n    Reduce the given list of items by splitting it into chunks\n    of the given size and passing each chunk through the reducer", "suffix": "    if len(iterable) == 0:\n        return initializer\n    if chunk_size == 0:\n        chunk_size = len(iterable)\n    return reduce(fn, chunks(iterable, chunk_size), initializer)", "gt": "    \"\"\""}
{"prefix": "def chain(*tasks):\n    \"\"\"\n    Given a number of tasks, builds a dependency chain.\n", "suffix": "\n    is equivalent to\n\n    task_1.set_downstream(task_2)\n    task_2.set_downstream(task_3)\n    task_3.set_downstream(task_4)\n    \"\"\"\n    for up_task, down_task in zip(tasks[:-1], tasks[1:]):\n        up_task.set_downstream(down_task)", "gt": "    chain(task_1, task_2, task_3, task_4)"}
{"prefix": "def pprinttable(rows):\n    \"\"\"Returns a pretty ascii table from tuples\n\n    If namedtuple are used, the table will have headers\n    \"\"\"\n    if not rows:\n        return\n    if hasattr(rows[0], '_fields'):  # if namedtuple", "suffix": "    else:\n        headers = [\"col{}\".format(i) for i in range(len(rows[0]))]\n    lens = [len(s) for s in headers]\n\n    for row in rows:\n        for i in range(len(rows[0])):\n            slenght = len(\"{}\".format(row[i]))\n            if slenght > lens[i]:\n                lens[i] = slenght\n    formats = []\n    hformats = []\n    for i in range(len(rows[0])):\n        if isinstance(rows[0][i], int):\n            formats.append(\"%%%dd\" % lens[i])\n        else:\n            formats.append(\"%%-%ds\" % lens[i])\n        hformats.append(\"%%-%ds\" % lens[i])\n    pattern = \" | \".join(formats)\n    hpattern = \" | \".join(hformats)\n    separator = \"-+-\".join(['-' * n for n in lens])\n    s = \"\"\n    s += separator + '\\n'\n    s += (hpattern % tuple(headers)) + '\\n'\n    s += separator + '\\n'\n\n    def f(t):\n        return \"{}\".format(t) if isinstance(t, basestring) else t\n\n    for line in rows:\n        s += pattern % tuple(f(t) for t in line) + '\\n'\n    s += separator + '\\n'\n    return s", "gt": "        headers = rows[0]._fields"}
{"prefix": "def reap_process_group(pid, log, sig=signal.SIGTERM,\n                       timeout=DEFAULT_TIME_TO_WAIT_AFTER_SIGTERM):\n    \"\"\"\n    Tries really hard to terminate all children (including grandchildren). Will send\n    sig (SIGTERM) to the process group of pid. If any process is alive after timeout\n    a SIGKILL will be send.\n\n    :param log: log handler\n    :param pid: pid to kill\n    :param sig: signal type\n    :param timeout: how much time a process has to terminate\n    \"\"\"\n\n    def on_terminate(p):\n        log.info(\"Process %s (%s) terminated with exit code %s\", p, p.pid, p.returncode)\n\n    if pid == os.getpid():\n        raise RuntimeError(\"I refuse to kill myself\")\n\n    parent = psutil.Process(pid)\n\n    children = parent.children(recursive=True)\n    children.append(parent)\n\n    try:\n        pg = os.getpgid(pid)\n    except OSError as err:\n        # Skip if not such process - we experience a race and it just terminated\n        if err.errno == errno.ESRCH:\n            return\n        raise\n\n    log.info(\"Sending %s to GPID %s\", sig, pg)\n    os.killpg(os.getpgid(pid), sig)\n\n    gone, alive = psutil.wait_procs(children, timeout=timeout, callback=on_terminate)\n\n    if alive:\n        for p in alive:\n            log.warn(\"process %s (%s) did not respond to SIGTERM. Trying SIGKILL\", p, pid)\n\n        os.killpg(os.getpgid(pid), signal.SIGKILL)\n\n        gone, alive = psutil.wait_procs(alive, timeout=timeout, callback=on_terminate)\n        if alive:", "suffix": "                log.error(\"Process %s (%s) could not be killed. Giving up.\", p, p.pid)", "gt": "            for p in alive:"}
{"prefix": "def render_log_filename(ti, try_number, filename_template):\n    \"\"\"\n    Given task instance, try_number, filename_template, return the rendered log\n    filename\n\n    :param ti: task instance\n    :param try_number: try_number of the task", "suffix": "        python string template\n    \"\"\"\n    filename_template, filename_jinja_template = parse_template_string(filename_template)\n    if filename_jinja_template:\n        jinja_context = ti.get_template_context()\n        jinja_context['try_number'] = try_number\n        return filename_jinja_template.render(**jinja_context)\n\n    return filename_template.format(dag_id=ti.dag_id,\n                                    task_id=ti.task_id,\n                                    execution_date=ti.execution_date.isoformat(),\n                                    try_number=try_number)", "gt": "    :param filename_template: filename template, which can be jinja template or"}
{"prefix": "def get_task_instance(dag_id, task_id, execution_date):\n    \"\"\"Return the task object identified by the given dag_id and task_id.\"\"\"\n\n    dagbag = DagBag()\n\n    # Check DAG exists.\n    if dag_id not in dagbag.dags:\n        error_message = \"Dag id {} not found\".format(dag_id)\n        raise DagNotFound(error_message)\n\n    # Get DAG object and check Task Exists\n    dag = dagbag.get_dag(dag_id)\n    if not dag.has_task(task_id):\n        error_message = 'Task {} not found in dag {}'.format(task_id, dag_id)\n        raise TaskNotFound(error_message)\n", "suffix": "    dagrun = dag.get_dagrun(execution_date=execution_date)\n    if not dagrun:\n        error_message = ('Dag Run for date {} not found in dag {}'\n                         .format(execution_date, dag_id))\n        raise DagRunNotFound(error_message)\n\n    # Get task instance object and check that it exists\n    task_instance = dagrun.get_task_instance(task_id)\n    if not task_instance:\n        error_message = ('Task {} instance for date {} not found'\n                         .format(task_id, execution_date))\n        raise TaskInstanceNotFound(error_message)\n\n    return task_instance", "gt": "    # Get DagRun object and check that it exists"}
{"prefix": "def _integrate_plugins():", "suffix": "    import sys\n    from airflow.plugins_manager import operators_modules\n    for operators_module in operators_modules:\n        sys.modules[operators_module.__name__] = operators_module\n        globals()[operators_module._name] = operators_module", "gt": "    \"\"\"Integrate plugins to the context\"\"\""}
{"prefix": "def get_conn(self):\n        \"\"\"Returns a Google Cloud Dataproc service object.\"\"\"\n        http_authorized = self._authorize()\n        return build(", "suffix": "            cache_discovery=False)", "gt": "            'dataproc', self.api_version, http=http_authorized,"}
{"prefix": "def wait(self, operation):\n        \"\"\"Awaits for Google Cloud Dataproc Operation to complete.\"\"\"\n        submitted = _DataProcOperation(self.get_conn(), operation,\n                                       self.num_retries)", "suffix": "", "gt": "        submitted.wait_for_done()"}
{"prefix": "def _deep_string_coerce(content, json_path='json'):\n    \"\"\"\n    Coerces content or all values of content if it is a dict to a string. The\n    function will throw if content contains non-string or non-numeric types.\n\n    The reason why we have this function is because the ``self.json`` field must be a\n    dict with only string values. This is because ``render_template`` will fail\n    for numerical values.", "suffix": "    c = _deep_string_coerce\n    if isinstance(content, six.string_types):\n        return content\n    elif isinstance(content, six.integer_types + (float,)):\n        # Databricks can tolerate either numeric or string types in the API backend.\n        return str(content)\n    elif isinstance(content, (list, tuple)):\n        return [c(e, '{0}[{1}]'.format(json_path, i)) for i, e in enumerate(content)]\n    elif isinstance(content, dict):\n        return {k: c(v, '{0}[{1}]'.format(json_path, k))\n                for k, v in list(content.items())}\n    else:\n        param_type = type(content)\n        msg = 'Type {0} used for parameter {1} is not a number or a string' \\\n            .format(param_type, json_path)\n        raise AirflowException(msg)", "gt": "    \"\"\""}
{"prefix": "def _handle_databricks_operator_execution(operator, hook, log, context):\n    \"\"\"\n    Handles the Airflow + Databricks lifecycle logic for a Databricks operator\n\n    :param operator: Databricks operator being handled\n    :param context: Airflow context\n    \"\"\"\n    if operator.do_xcom_push:\n        context['ti'].xcom_push(key=XCOM_RUN_ID_KEY, value=operator.run_id)\n    log.info('Run submitted with run_id: %s', operator.run_id)\n    run_page_url = hook.get_run_page_url(operator.run_id)\n    if operator.do_xcom_push:\n        context['ti'].xcom_push(key=XCOM_RUN_PAGE_URL_KEY, value=run_page_url)\n\n    log.info('View run status, Spark UI, and logs at %s', run_page_url)\n    while True:\n        run_state = hook.get_run_state(operator.run_id)\n        if run_state.is_terminal:\n            if run_state.is_successful:\n                log.info('%s completed successfully.', operator.task_id)\n                log.info('View run status, Spark UI, and logs at %s', run_page_url)", "suffix": "            else:\n                error_message = '{t} failed with terminal state: {s}'.format(\n                    t=operator.task_id,\n                    s=run_state)\n                raise AirflowException(error_message)\n        else:\n            log.info('%s in run state: %s', operator.task_id, run_state)\n            log.info('View run status, Spark UI, and logs at %s', run_page_url)\n            log.info('Sleeping for %s seconds.', operator.polling_period_seconds)\n            time.sleep(operator.polling_period_seconds)", "gt": "                return"}
{"prefix": "def run_cli(self, pig, verbose=True):\n        \"\"\"\n        Run an pig script using the pig cli\n\n        >>> ph = PigCliHook()\n        >>> result = ph.run_cli(\"ls /;\")\n        >>> (\"hdfs://\" in result)\n        True\n        \"\"\"\n\n        with TemporaryDirectory(prefix='airflow_pigop_') as tmp_dir:\n            with NamedTemporaryFile(dir=tmp_dir) as f:\n                f.write(pig.encode('utf-8'))\n                f.flush()\n                fname = f.name\n                pig_bin = 'pig'\n                cmd_extra = []\n\n                pig_cmd = [pig_bin, '-f', fname] + cmd_extra\n\n                if self.pig_properties:\n                    pig_properties_list = self.pig_properties.split()\n                    pig_cmd.extend(pig_properties_list)\n                if verbose:\n                    self.log.info(\"%s\", \" \".join(pig_cmd))\n                sp = subprocess.Popen(\n                    pig_cmd,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.STDOUT,\n                    cwd=tmp_dir,\n                    close_fds=True)\n                self.sp = sp\n                stdout = ''\n                for line in iter(sp.stdout.readline, b''):\n                    stdout += line.decode('utf-8')\n                    if verbose:\n                        self.log.info(line.strip())", "suffix": "\n                if sp.returncode:\n                    raise AirflowException(stdout)\n\n                return stdout", "gt": "                sp.wait()"}
{"prefix": "def fetch_celery_task_state(celery_task):\n    \"\"\"\n    Fetch and return the state of the given celery task. The scope of this function is\n    global so that it can be called by subprocesses in the pool.\n\n    :param celery_task: a tuple of the Celery task key and the async Celery object used\n        to fetch the task's state\n    :type celery_task: tuple(str, celery.result.AsyncResult)\n    :return: a tuple of the Celery task key and the Celery state of the task\n    :rtype: tuple[str, str]\n    \"\"\"\n\n    try:\n        with timeout(seconds=2):\n            # Accessing state property of celery task will make actual network request\n            # to get the current state of the task.", "suffix": "    except Exception as e:\n        exception_traceback = \"Celery Task ID: {}\\n{}\".format(celery_task[0],\n                                                              traceback.format_exc())\n        res = ExceptionWithTraceback(e, exception_traceback)\n    return res", "gt": "            res = (celery_task[0], celery_task[1].state)"}
{"prefix": "", "suffix": "        \"\"\"\n        How many Celery tasks should each worker process send.\n\n        :return: Number of tasks that should be sent per process\n        :rtype: int\n        \"\"\"\n        return max(1,\n                   int(math.ceil(1.0 * to_send_count / self._sync_parallelism)))", "gt": "def _num_tasks_per_send_process(self, to_send_count):"}
{"prefix": "def _num_tasks_per_fetch_process(self):", "suffix": "        How many Celery tasks should be sent to each worker process.\n\n        :return: Number of tasks that should be used per process\n        :rtype: int\n        \"\"\"\n        return max(1,\n                   int(math.ceil(1.0 * len(self.tasks) / self._sync_parallelism)))", "gt": "        \"\"\""}
{"prefix": "def setdefault(cls, key, default, deserialize_json=False):\n        \"\"\"\n        Like a Python builtin dict object, setdefault returns the current value\n        for a key, and if it isn't there, stores the default value and returns it.\n\n        :param key: Dict key for this Variable\n        :type key: str\n        :param default: Default value to set and return if the variable\n            isn't already in the DB\n        :type default: Mixed\n        :param deserialize_json: Store this as a JSON encoded value in the DB\n            and un-encode it when retrieving a value\n        :return: Mixed\n        \"\"\"\n        obj = Variable.get(key, default_var=None,\n                           deserialize_json=deserialize_json)\n        if obj is None:\n            if default is not None:\n                Variable.set(key, default, serialize_json=deserialize_json)", "suffix": "            else:\n                raise ValueError('Default Value must be set')\n        else:\n            return obj", "gt": "                return default"}
{"prefix": "def get_conn(self):", "suffix": "        Returns a Google MLEngine service object.\n        \"\"\"\n        authed_http = self._authorize()\n        return build('ml', 'v1', http=authed_http, cache_discovery=False)", "gt": "        \"\"\""}
{"prefix": "def create_job(self, project_id, job, use_existing_job_fn=None):\n        \"\"\"\n        Launches a MLEngine job and wait for it to reach a terminal state.\n\n        :param project_id: The Google Cloud project id within which MLEngine\n            job will be launched.\n        :type project_id: str\n\n        :param job: MLEngine Job object that should be provided to the MLEngine\n            API, such as: ::\n\n                {\n                  'jobId': 'my_job_id',\n                  'trainingInput': {\n                    'scaleTier': 'STANDARD_1',\n                    ...\n                  }\n                }\n\n        :type job: dict\n\n        :param use_existing_job_fn: In case that a MLEngine job with the same\n            job_id already exist, this method (if provided) will decide whether\n            we should use this existing job, continue waiting for it to finish\n            and returning the job object. It should accepts a MLEngine job\n            object, and returns a boolean value indicating whether it is OK to\n            reuse the existing job. If 'use_existing_job_fn' is not provided,\n            we by default reuse the existing MLEngine job.\n        :type use_existing_job_fn: function", "suffix": "        :return: The MLEngine job object if the job successfully reach a\n            terminal state (which might be FAILED or CANCELLED state).\n        :rtype: dict\n        \"\"\"\n        request = self._mlengine.projects().jobs().create(\n            parent='projects/{}'.format(project_id),\n            body=job)\n        job_id = job['jobId']\n\n        try:\n            request.execute()\n        except HttpError as e:\n            # 409 means there is an existing job with the same job ID.\n            if e.resp.status == 409:\n                if use_existing_job_fn is not None:\n                    existing_job = self._get_job(project_id, job_id)\n                    if not use_existing_job_fn(existing_job):\n                        self.log.error(\n                            'Job with job_id %s already exist, but it does '\n                            'not match our expectation: %s',\n                            job_id, existing_job\n                        )\n                        raise\n                self.log.info(\n                    'Job with job_id %s already exist. Will waiting for it to finish',\n                    job_id\n                )\n            else:\n                self.log.error('Failed to create MLEngine job: {}'.format(e))\n                raise\n\n        return self._wait_for_job_done(project_id, job_id)", "gt": ""}
{"prefix": "def _get_job(self, project_id, job_id):\n        \"\"\"\n        Gets a MLEngine job based on the job name.\n\n        :return: MLEngine job object if succeed.\n        :rtype: dict\n\n        Raises:\n            googleapiclient.errors.HttpError: if HTTP error is returned from server\n        \"\"\"\n        job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n        request = self._mlengine.projects().jobs().get(name=job_name)\n        while True:\n            try:\n                return request.execute()\n            except HttpError as e:\n                if e.resp.status == 429:\n                    # polling after 30 seconds when quota failure occurs\n                    time.sleep(30)\n                else:\n                    self.log.error('Failed to get MLEngine job: {}'.format(e))", "suffix": "", "gt": "                    raise"}
{"prefix": "def _wait_for_job_done(self, project_id, job_id, interval=30):\n        \"\"\"\n        Waits for the Job to reach a terminal state.\n\n        This method will periodically check the job state until the job reach\n        a terminal state.\n\n        Raises:\n            googleapiclient.errors.HttpError: if HTTP error is returned when getting\n            the job\n        \"\"\"", "suffix": "            raise ValueError(\"Interval must be > 0\")\n        while True:\n            job = self._get_job(project_id, job_id)\n            if job['state'] in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n                return job\n            time.sleep(interval)", "gt": "        if interval <= 0:"}
{"prefix": "def create_version(self, project_id, model_name, version_spec):\n        \"\"\"\n        Creates the Version on Google Cloud ML Engine.\n\n        Returns the operation if the version was created successfully and\n        raises an error otherwise.\n        \"\"\"\n        parent_name = 'projects/{}/models/{}'.format(project_id, model_name)\n        create_request = self._mlengine.projects().models().versions().create(\n            parent=parent_name, body=version_spec)", "suffix": "        get_request = self._mlengine.projects().operations().get(\n            name=response['name'])\n\n        return _poll_with_exponential_delay(\n            request=get_request,\n            max_n=9,\n            is_done_func=lambda resp: resp.get('done', False),\n            is_error_func=lambda resp: resp.get('error', None) is not None)", "gt": "        response = create_request.execute()"}
{"prefix": "def set_default_version(self, project_id, model_name, version_name):\n        \"\"\"", "suffix": "        \"\"\"\n        full_version_name = 'projects/{}/models/{}/versions/{}'.format(\n            project_id, model_name, version_name)\n        request = self._mlengine.projects().models().versions().setDefault(\n            name=full_version_name, body={})\n\n        try:\n            response = request.execute()\n            self.log.info('Successfully set version: %s to default', response)\n            return response\n        except HttpError as e:\n            self.log.error('Something went wrong: %s', e)\n            raise", "gt": "        Sets a version to be the default. Blocks until finished."}
{"prefix": "def list_versions(self, project_id, model_name):\n        \"\"\"\n        Lists all available versions of a model. Blocks until finished.\n        \"\"\"\n        result = []\n        full_parent_name = 'projects/{}/models/{}'.format(\n            project_id, model_name)\n        request = self._mlengine.projects().models().versions().list(\n            parent=full_parent_name, pageSize=100)\n\n        response = request.execute()\n        next_page_token = response.get('nextPageToken', None)\n        result.extend(response.get('versions', []))\n        while next_page_token is not None:\n            next_request = self._mlengine.projects().models().versions().list(\n                parent=full_parent_name,\n                pageToken=next_page_token,\n                pageSize=100)\n            response = next_request.execute()\n            next_page_token = response.get('nextPageToken', None)\n            result.extend(response.get('versions', []))\n            time.sleep(5)", "suffix": "", "gt": "        return result"}
{"prefix": "def delete_version(self, project_id, model_name, version_name):\n        \"\"\"\n        Deletes the given version of a model. Blocks until finished.\n        \"\"\"\n        full_name = 'projects/{}/models/{}/versions/{}'.format(\n            project_id, model_name, version_name)\n        delete_request = self._mlengine.projects().models().versions().delete(\n            name=full_name)\n        response = delete_request.execute()\n        get_request = self._mlengine.projects().operations().get(\n            name=response['name'])\n\n        return _poll_with_exponential_delay(\n            request=get_request,\n            max_n=9,", "suffix": "            is_error_func=lambda resp: resp.get('error', None) is not None)", "gt": "            is_done_func=lambda resp: resp.get('done', False),"}
{"prefix": "def create_model(self, project_id, model):\n        \"\"\"\n        Create a Model. Blocks until finished.\n        \"\"\"\n        if not model['name']:\n            raise ValueError(\"Model name must be provided and \"\n                             \"could not be an empty string\")\n        project = 'projects/{}'.format(project_id)\n\n        request = self._mlengine.projects().models().create(", "suffix": "        return request.execute()", "gt": "            parent=project, body=model)"}
{"prefix": "def get_model(self, project_id, model_name):\n        \"\"\"\n        Gets a Model. Blocks until finished.\n        \"\"\"\n        if not model_name:\n            raise ValueError(\"Model name must be provided and \"\n                             \"it could not be an empty string\")\n        full_model_name = 'projects/{}/models/{}'.format(\n            project_id, model_name)", "suffix": "        try:\n            return request.execute()\n        except HttpError as e:\n            if e.resp.status == 404:\n                self.log.error('Model was not found: %s', e)\n                return None\n            raise", "gt": "        request = self._mlengine.projects().models().get(name=full_model_name)"}
{"prefix": "def execute_work(self, key, command):\n        \"\"\"\n        Executes command received and stores result state in queue.\n        :param key: the key to identify the TI\n        :type key: tuple(dag_id, task_id, execution_date)\n        :param command: the command to execute\n        :type command: str\n        \"\"\"\n        if key is None:\n            return\n        self.log.info(\"%s running %s\", self.__class__.__name__, command)\n        try:\n            subprocess.check_call(command, close_fds=True)\n            state = State.SUCCESS\n        except subprocess.CalledProcessError as e:\n            state = State.FAILED\n            self.log.error(\"Failed to execute task %s.\", str(e))", "suffix": "            # raise e\n        self.result_queue.put((key, state))", "gt": "            # TODO: Why is this commented out?"}
{"prefix": "", "suffix": "        \"\"\"\n        Write batch items to dynamodb table with provisioned throughout capacity.\n        \"\"\"\n\n        dynamodb_conn = self.get_conn()\n\n        try:\n            table = dynamodb_conn.Table(self.table_name)\n\n            with table.batch_writer(overwrite_by_pkeys=self.table_keys) as batch:\n                for item in items:\n                    batch.put_item(Item=item)\n            return True\n        except Exception as general_error:\n            raise AirflowException(\n                'Failed to insert items in dynamodb, error: {error}'.format(\n                    error=str(general_error)\n                )\n            )", "gt": "def write_batch_data(self, items):"}
{"prefix": "def _integrate_plugins():\n    \"\"\"Integrate plugins to the context.\"\"\"\n    from airflow.plugins_manager import executors_modules\n    for executors_module in executors_modules:", "suffix": "        globals()[executors_module._name] = executors_module", "gt": "        sys.modules[executors_module.__name__] = executors_module"}
{"prefix": "def get_default_executor():\n    \"\"\"Creates a new instance of the configured executor if none exists and returns it\"\"\"\n    global DEFAULT_EXECUTOR\n\n    if DEFAULT_EXECUTOR is not None:\n        return DEFAULT_EXECUTOR\n\n    executor_name = configuration.conf.get('core', 'EXECUTOR')\n\n    DEFAULT_EXECUTOR = _get_executor(executor_name)\n\n    log = LoggingMixin().log\n    log.info(\"Using executor %s\", executor_name)", "suffix": "    return DEFAULT_EXECUTOR", "gt": ""}
{"prefix": "def _get_executor(executor_name):\n    \"\"\"\n    Creates a new instance of the named executor.\n    In case the executor name is not know in airflow,\n    look for it in the plugins\n    \"\"\"\n    if executor_name == Executors.LocalExecutor:\n        return LocalExecutor()\n    elif executor_name == Executors.SequentialExecutor:", "suffix": "    elif executor_name == Executors.CeleryExecutor:\n        from airflow.executors.celery_executor import CeleryExecutor\n        return CeleryExecutor()\n    elif executor_name == Executors.DaskExecutor:\n        from airflow.executors.dask_executor import DaskExecutor\n        return DaskExecutor()\n    elif executor_name == Executors.KubernetesExecutor:\n        from airflow.contrib.executors.kubernetes_executor import KubernetesExecutor\n        return KubernetesExecutor()\n    else:\n        # Loading plugins\n        _integrate_plugins()\n        executor_path = executor_name.split('.')\n        if len(executor_path) != 2:\n            raise AirflowException(\n                \"Executor {0} not supported: \"\n                \"please specify in format plugin_module.executor\".format(executor_name))\n\n        if executor_path[0] in globals():\n            return globals()[executor_path[0]].__dict__[executor_path[1]]()\n        else:\n            raise AirflowException(\"Executor {0} not supported.\".format(executor_name))", "gt": "        return SequentialExecutor()"}
{"prefix": "def on_error(self, error, items):\n        \"\"\"\n        Handles error callbacks when using Segment with segment_debug_mode set to True\n        \"\"\"\n        self.log.error('Encountered Segment error: {segment_error} with '\n                       'items: {with_items}'.format(segment_error=error,", "suffix": "        raise AirflowException('Segment error: {}'.format(error))", "gt": "                                                    with_items=items))"}
{"prefix": "def run_pod(self, pod, startup_timeout=120, get_logs=True):\n        # type: (Pod, int, bool) -> Tuple[State, Optional[str]]\n        \"\"\"\n        Launches the pod synchronously and waits for completion.\n        Args:\n            pod (Pod):\n            startup_timeout (int): Timeout for startup of the pod (if pod is pending for\n             too long, considers task a failure\n        \"\"\"\n        resp = self.run_pod_async(pod)\n        curr_time = dt.now()\n        if resp.status.start_time is None:\n            while self.pod_not_started(pod):\n                delta = dt.now() - curr_time\n                if delta.seconds >= startup_timeout:\n                    raise AirflowException(\"Pod took too long to start\")", "suffix": "            self.log.debug('Pod not yet started')\n\n        return self._monitor_pod(pod, get_logs)", "gt": "                time.sleep(1)"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Returns a mssql connection object\n        \"\"\"\n        conn = self.get_connection(self.mssql_conn_id)\n        conn = pymssql.connect(\n            server=conn.host,", "suffix": "            password=conn.password,\n            database=self.schema or conn.schema,\n            port=conn.port)\n        return conn", "gt": "            user=conn.login,"}
{"prefix": "def execute(self, context):\n        \"\"\"\n        Call the SparkSubmitHook to run the provided spark job\n        \"\"\"\n        self._hook = SparkSubmitHook(\n            conf=self._conf,\n            conn_id=self._conn_id,\n            files=self._files,\n            py_files=self._py_files,\n            archives=self._archives,\n            driver_class_path=self._driver_class_path,\n            jars=self._jars,\n            java_class=self._java_class,\n            packages=self._packages,\n            exclude_packages=self._exclude_packages,\n            repositories=self._repositories,\n            total_executor_cores=self._total_executor_cores,\n            executor_cores=self._executor_cores,\n            executor_memory=self._executor_memory,\n            driver_memory=self._driver_memory,\n            keytab=self._keytab,\n            principal=self._principal,\n            name=self._name,\n            num_executors=self._num_executors,\n            application_args=self._application_args,\n            env_vars=self._env_vars,", "suffix": "            spark_binary=self._spark_binary\n        )\n        self._hook.submit(self._application)", "gt": "            verbose=self._verbose,"}
{"prefix": "def trigger_dag(dag_id):\n    \"\"\"\n    Trigger a new dag run for a Dag with an execution date of now unless\n    specified in the data.\n    \"\"\"\n    data = request.get_json(force=True)\n\n    run_id = None\n    if 'run_id' in data:\n        run_id = data['run_id']\n\n    conf = None", "suffix": "        conf = data['conf']\n\n    execution_date = None\n    if 'execution_date' in data and data['execution_date'] is not None:\n        execution_date = data['execution_date']\n\n        # Convert string datetime into actual datetime\n        try:\n            execution_date = timezone.parse(execution_date)\n        except ValueError:\n            error_message = (\n                'Given execution date, {}, could not be identified '\n                'as a date. Example date format: 2015-11-16T14:34:15+00:00'\n                .format(execution_date))\n            _log.info(error_message)\n            response = jsonify({'error': error_message})\n            response.status_code = 400\n\n            return response\n\n    try:\n        dr = trigger.trigger_dag(dag_id, run_id, conf, execution_date)\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n\n    if getattr(g, 'user', None):\n        _log.info(\"User %s created %s\", g.user, dr)\n\n    response = jsonify(message=\"Created {}\".format(dr))\n    return response", "gt": "    if 'conf' in data:"}
{"prefix": "def delete_dag(dag_id):\n    \"\"\"\n    Delete all DB records related to the specified Dag.\n    \"\"\"\n    try:\n        count = delete.delete_dag(dag_id)\n    except AirflowException as err:", "suffix": "        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n    return jsonify(message=\"Removed {} record(s)\".format(count), count=count)", "gt": "        _log.error(err)"}
{"prefix": "", "suffix": "    \"\"\"\n    Returns a list of Dag Runs for a specific DAG ID.\n    :query param state: a query string parameter '?state=queued|running|success...'\n    :param dag_id: String identifier of a DAG\n    :return: List of DAG runs of a DAG with requested state,\n    or all runs if the state is not specified\n    \"\"\"\n    try:\n        state = request.args.get('state')\n        dagruns = get_dag_runs(dag_id, state)\n    except AirflowException as err:\n        _log.info(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = 400\n        return response\n\n    return jsonify(dagruns)", "gt": "def dag_runs(dag_id):"}
{"prefix": "def get_dag_code(dag_id):\n    \"\"\"Return python code of a given dag_id.\"\"\"\n    try:\n        return get_code(dag_id)", "suffix": "        _log.info(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response", "gt": "    except AirflowException as err:"}
{"prefix": "def task_info(dag_id, task_id):\n    \"\"\"Returns a JSON with a task's public instance variables. \"\"\"\n    try:\n        info = get_task(dag_id, task_id)\n    except AirflowException as err:\n        _log.info(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n", "suffix": "    fields = {k: str(v)\n              for k, v in vars(info).items()\n              if not k.startswith('_')}\n    return jsonify(fields)", "gt": "    # JSONify and return."}
{"prefix": "def dag_paused(dag_id, paused):\n    \"\"\"(Un)pauses a dag\"\"\"\n\n    DagModel = models.DagModel\n    with create_session() as session:\n        orm_dag = (", "suffix": "                   .filter(DagModel.dag_id == dag_id).first()\n        )\n        if paused == 'true':\n            orm_dag.is_paused = True\n        else:\n            orm_dag.is_paused = False\n        session.merge(orm_dag)\n        session.commit()\n\n    return jsonify({'response': 'ok'})", "gt": "            session.query(DagModel)"}
{"prefix": "def task_instance_info(dag_id, execution_date, task_id):\n    \"\"\"\n    Returns a JSON with a task instance's public instance variables.\n    The format for the exec_date is expected to be\n    \"YYYY-mm-DDTHH:MM:SS\", for example: \"2016-11-16T11:34:15\". This will\n    of course need to have been encoded for URL in the request.\n    \"\"\"\n\n    # Convert string datetime into actual datetime\n    try:\n        execution_date = timezone.parse(execution_date)\n    except ValueError:\n        error_message = (\n            'Given execution date, {}, could not be identified '\n            'as a date. Example date format: 2015-11-16T14:34:15+00:00'\n            .format(execution_date))\n        _log.info(error_message)\n        response = jsonify({'error': error_message})\n        response.status_code = 400\n\n        return response\n\n    try:\n        info = get_task_instance(dag_id, task_id, execution_date)\n    except AirflowException as err:\n        _log.info(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code", "suffix": "\n    # JSONify and return.\n    fields = {k: str(v)\n              for k, v in vars(info).items()\n              if not k.startswith('_')}\n    return jsonify(fields)", "gt": "        return response"}
{"prefix": "def dag_run_status(dag_id, execution_date):\n    \"\"\"\n    Returns a JSON with a dag_run's public instance variables.\n    The format for the exec_date is expected to be\n    \"YYYY-mm-DDTHH:MM:SS\", for example: \"2016-11-16T11:34:15\". This will\n    of course need to have been encoded for URL in the request.\n    \"\"\"\n\n    # Convert string datetime into actual datetime\n    try:\n        execution_date = timezone.parse(execution_date)\n    except ValueError:", "suffix": "            'Given execution date, {}, could not be identified '\n            'as a date. Example date format: 2015-11-16T14:34:15+00:00'.format(\n                execution_date))\n        _log.info(error_message)\n        response = jsonify({'error': error_message})\n        response.status_code = 400\n\n        return response\n\n    try:\n        info = get_dag_run_state(dag_id, execution_date)\n    except AirflowException as err:\n        _log.info(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n\n    return jsonify(info)", "gt": "        error_message = ("}
{"prefix": "def get_pools():\n    \"\"\"Get all pools.\"\"\"", "suffix": "        pools = pool_api.get_pools()\n    except AirflowException as err:\n        _log.error(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n    else:\n        return jsonify([p.to_json() for p in pools])", "gt": "    try:"}
{"prefix": "def create_pool():\n    \"\"\"Create a pool.\"\"\"\n    params = request.get_json(force=True)\n    try:\n        pool = pool_api.create_pool(**params)\n    except AirflowException as err:\n        _log.error(err)", "suffix": "        response.status_code = err.status_code\n        return response\n    else:\n        return jsonify(pool.to_json())", "gt": "        response = jsonify(error=\"{}\".format(err))"}
{"prefix": "def delete_pool(name):\n    \"\"\"Delete pool.\"\"\"\n    try:\n        pool = pool_api.delete_pool(name=name)\n    except AirflowException as err:", "suffix": "        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n    else:\n        return jsonify(pool.to_json())", "gt": "        _log.error(err)"}
{"prefix": "def create_or_update(self, resource_group, name, container_group):\n        \"\"\"\n        Create a new container group\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str", "suffix": "        :type name: str\n        :param container_group: the properties of the container group\n        :type container_group: azure.mgmt.containerinstance.models.ContainerGroup\n        \"\"\"\n        self.connection.container_groups.create_or_update(resource_group,\n                                                          name,\n                                                          container_group)", "gt": "        :param name: the name of the container group"}
{"prefix": "def get_state_exitcode_details(self, resource_group, name):\n        \"\"\"\n        Get the state and exitcode of a container group\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str\n        :return: A tuple with the state, exitcode, and details.\n            If the exitcode is unknown 0 is returned.", "suffix": "        \"\"\"\n        current_state = self._get_instance_view(resource_group, name).current_state\n        return (current_state.state,\n                current_state.exit_code,\n                current_state.detail_status)", "gt": "        :rtype: tuple(state,exitcode,details)"}
{"prefix": "def get_messages(self, resource_group, name):\n        \"\"\"\n        Get the messages of a container group\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group", "suffix": "        :return: A list of the event messages\n        :rtype: list[str]\n        \"\"\"\n        instance_view = self._get_instance_view(resource_group, name)\n\n        return [event.message for event in instance_view.events]", "gt": "        :type name: str"}
{"prefix": "def get_logs(self, resource_group, name, tail=1000):\n        \"\"\"\n        Get the tail from logs of a container group\n", "suffix": "        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str\n        :param tail: the size of the tail\n        :type tail: int\n        :return: A list of log messages\n        :rtype: list[str]\n        \"\"\"\n        logs = self.connection.container.list_logs(resource_group, name, name, tail=tail)\n        return logs.content.splitlines(True)", "gt": "        :param resource_group: the name of the resource group"}
{"prefix": "", "suffix": "        \"\"\"\n        Delete a container group\n\n        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str\n        \"\"\"\n        self.connection.container_groups.delete(resource_group, name)", "gt": "def delete(self, resource_group, name):"}
{"prefix": "def exists(self, resource_group, name):\n        \"\"\"\n        Test if a container group exists", "suffix": "        :param resource_group: the name of the resource group\n        :type resource_group: str\n        :param name: the name of the container group\n        :type name: str\n        \"\"\"\n        for container in self.connection.container_groups.list_by_resource_group(resource_group):\n            if container.name == name:\n                return True\n        return False", "gt": ""}
{"prefix": "def apply_defaults(func):\n    \"\"\"\n    Function decorator that Looks for an argument named \"default_args\", and\n    fills the unspecified arguments from it.\n\n    Since python2.* isn't clear about which arguments are missing when\n    calling a function, and that this can be quite confusing with multi-level\n    inheritance and argument defaults, this decorator also alerts with\n    specific information about the missing arguments.\n    \"\"\"\n\n    # Cache inspect.signature for the wrapper closure to avoid calling it\n    # at every decorated invocation. This is separate sig_cache created\n    # per decoration, i.e. each function decorated using apply_defaults will\n    # have a different sig_cache.\n    sig_cache = signature(func)\n    non_optional_args = {\n        name for (name, param) in sig_cache.parameters.items()\n        if param.default == param.empty and\n        param.name != 'self' and\n        param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)}\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if len(args) > 1:\n            raise AirflowException(\n                \"Use keyword arguments when initializing operators\")\n        dag_args = {}\n        dag_params = {}\n\n        dag = kwargs.get('dag', None) or settings.CONTEXT_MANAGER_DAG\n        if dag:\n            dag_args = copy(dag.default_args) or {}\n            dag_params = copy(dag.params) or {}\n\n        params = {}", "suffix": "            params = kwargs['params']\n        dag_params.update(params)\n\n        default_args = {}\n        if 'default_args' in kwargs:\n            default_args = kwargs['default_args']\n            if 'params' in default_args:\n                dag_params.update(default_args['params'])\n                del default_args['params']\n\n        dag_args.update(default_args)\n        default_args = dag_args\n\n        for arg in sig_cache.parameters:\n            if arg not in kwargs and arg in default_args:\n                kwargs[arg] = default_args[arg]\n        missing_args = list(non_optional_args - set(kwargs))\n        if missing_args:\n            msg = \"Argument {0} is required\".format(missing_args)\n            raise AirflowException(msg)\n\n        kwargs['params'] = dag_params\n\n        result = func(*args, **kwargs)\n        return result\n    return wrapper", "gt": "        if 'params' in kwargs:"}
{"prefix": "def construct_ingest_query(self, static_path, columns):\n        \"\"\"\n        Builds an ingest query for an HDFS TSV load.\n\n        :param static_path: The path on hdfs where the data is\n        :type static_path: str\n        :param columns: List of all the columns that are available\n        :type columns: list\n        \"\"\"\n\n        # backward compatibility for num_shards,\n        # but target_partition_size is the default setting\n        # and overwrites the num_shards\n        num_shards = self.num_shards\n        target_partition_size = self.target_partition_size\n        if self.target_partition_size == -1:\n            if self.num_shards == -1:\n                target_partition_size = DEFAULT_TARGET_PARTITION_SIZE\n        else:\n            num_shards = -1\n\n        metric_names = [m['fieldName'] for m in self.metric_spec if m['type'] != 'count']\n\n        # Take all the columns, which are not the time dimension\n        # or a metric, as the dimension columns\n        dimensions = [c for c in columns if c not in metric_names and c != self.ts_dim]\n\n        ingest_query_dict = {\n            \"type\": \"index_hadoop\",\n            \"spec\": {", "suffix": "                    \"metricsSpec\": self.metric_spec,\n                    \"granularitySpec\": {\n                        \"queryGranularity\": self.query_granularity,\n                        \"intervals\": self.intervals,\n                        \"type\": \"uniform\",\n                        \"segmentGranularity\": self.segment_granularity,\n                    },\n                    \"parser\": {\n                        \"type\": \"string\",\n                        \"parseSpec\": {\n                            \"columns\": columns,\n                            \"dimensionsSpec\": {\n                                \"dimensionExclusions\": [],\n                                \"dimensions\": dimensions,  # list of names\n                                \"spatialDimensions\": []\n                            },\n                            \"timestampSpec\": {\n                                \"column\": self.ts_dim,\n                                \"format\": \"auto\"\n                            },\n                            \"format\": \"tsv\"\n                        }\n                    },\n                    \"dataSource\": self.druid_datasource\n                },\n                \"tuningConfig\": {\n                    \"type\": \"hadoop\",\n                    \"jobProperties\": {\n                        \"mapreduce.job.user.classpath.first\": \"false\",\n                        \"mapreduce.map.output.compress\": \"false\",\n                        \"mapreduce.output.fileoutputformat.compress\": \"false\",\n                    },\n                    \"partitionsSpec\": {\n                        \"type\": \"hashed\",\n                        \"targetPartitionSize\": target_partition_size,\n                        \"numShards\": num_shards,\n                    },\n                },\n                \"ioConfig\": {\n                    \"inputSpec\": {\n                        \"paths\": static_path,\n                        \"type\": \"static\"\n                    },\n                    \"type\": \"hadoop\"\n                }\n            }\n        }\n\n        if self.job_properties:\n            ingest_query_dict['spec']['tuningConfig']['jobProperties'] \\\n                .update(self.job_properties)\n\n        if self.hadoop_dependency_coordinates:\n            ingest_query_dict['hadoopDependencyCoordinates'] \\\n                = self.hadoop_dependency_coordinates\n\n        return ingest_query_dict", "gt": "                \"dataSchema\": {"}
{"prefix": "def execute(self, context):\n        \"\"\"\n        This function executes the transfer from the email server (via imap) into s3.\n\n        :param context: The context while executing.\n        :type context: dict\n        \"\"\"\n        self.log.info(\n            'Transferring mail attachment %s from mail server via imap to s3 key %s...',\n            self.imap_attachment_name, self.s3_key\n        )\n\n        with ImapHook(imap_conn_id=self.imap_conn_id) as imap_hook:\n            imap_mail_attachments = imap_hook.retrieve_mail_attachments(\n                name=self.imap_attachment_name,\n                mail_folder=self.imap_mail_folder,\n                check_regex=self.imap_check_regex,", "suffix": "            )\n\n        s3_hook = S3Hook(aws_conn_id=self.s3_conn_id)\n        s3_hook.load_bytes(bytes_data=imap_mail_attachments[0][1], key=self.s3_key)", "gt": "                latest_only=True"}
{"prefix": "def poke(self, context):\n        \"\"\"\n        Check for message on subscribed channels and write to xcom the message with key ``message``\n\n        An example of message ``{'type': 'message', 'pattern': None, 'channel': b'test', 'data': b'hello'}``\n\n        :param context: the context object\n        :type context: dict\n        :return: ``True`` if message (with type 'message') is available or ``False`` if not\n        \"\"\"\n        self.log.info('RedisPubSubSensor checking for message on channels: %s', self.channels)\n\n        message = self.pubsub.get_message()\n        self.log.info('Message %s from channel %s', message, self.channels)\n\n        # Process only message types", "suffix": "\n            context['ti'].xcom_push(key='message', value=message)\n            self.pubsub.unsubscribe(self.channels)\n\n            return True\n\n        return False", "gt": "        if message and message['type'] == 'message':"}
{"prefix": "def refresh_from_db(self, session=None):\n        \"\"\"\n        Reloads the current dagrun from the database\n        :param session: database session\n        \"\"\"\n        DR = DagRun\n\n        exec_date = func.cast(self.execution_date, DateTime)\n\n        dr = session.query(DR).filter(\n            DR.dag_id == self.dag_id,\n            func.cast(DR.execution_date, DateTime) == exec_date,\n            DR.run_id == self.run_id\n        ).one()\n\n        self.id = dr.id", "suffix": "", "gt": "        self.state = dr.state"}
{"prefix": "def find(dag_id=None, run_id=None, execution_date=None,\n             state=None, external_trigger=None, no_backfills=False,\n             session=None):\n        \"\"\"\n        Returns a set of dag runs for the given search criteria.\n\n        :param dag_id: the dag_id to find dag runs for\n        :type dag_id: int, list\n        :param run_id: defines the the run id for this dag run\n        :type run_id: str\n        :param execution_date: the execution date\n        :type execution_date: datetime.datetime\n        :param state: the state of the dag run\n        :type state: airflow.utils.state.State\n        :param external_trigger: whether this dag run is externally triggered\n        :type external_trigger: bool\n        :param no_backfills: return no backfills (True), return all (False).\n            Defaults to False\n        :type no_backfills: bool\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        \"\"\"\n        DR = DagRun\n\n        qry = session.query(DR)\n        if dag_id:\n            qry = qry.filter(DR.dag_id == dag_id)\n        if run_id:\n            qry = qry.filter(DR.run_id == run_id)\n        if execution_date:\n            if isinstance(execution_date, list):\n                qry = qry.filter(DR.execution_date.in_(execution_date))\n            else:\n                qry = qry.filter(DR.execution_date == execution_date)\n        if state:\n            qry = qry.filter(DR.state == state)\n        if external_trigger is not None:\n            qry = qry.filter(DR.external_trigger == external_trigger)\n        if no_backfills:", "suffix": "            from airflow.jobs import BackfillJob\n            qry = qry.filter(DR.run_id.notlike(BackfillJob.ID_PREFIX + '%'))\n\n        dr = qry.order_by(DR.execution_date).all()\n\n        return dr", "gt": "            # in order to prevent a circular dependency"}
{"prefix": "def get_task_instances(self, state=None, session=None):\n        \"\"\"\n        Returns the task instances for this dag run\n        \"\"\"\n        from airflow.models.taskinstance import TaskInstance  # Avoid circular import\n        tis = session.query(TaskInstance).filter(\n            TaskInstance.dag_id == self.dag_id,\n            TaskInstance.execution_date == self.execution_date,\n        )\n        if state:\n            if isinstance(state, six.string_types):\n                tis = tis.filter(TaskInstance.state == state)\n            else:\n                # this is required to deal with NULL values\n                if None in state:\n                    tis = tis.filter(\n                        or_(TaskInstance.state.in_(state),\n                            TaskInstance.state.is_(None))\n                    )\n                else:\n                    tis = tis.filter(TaskInstance.state.in_(state))\n\n        if self.dag and self.dag.partial:\n            tis = tis.filter(TaskInstance.task_id.in_(self.dag.task_ids))\n", "suffix": "", "gt": "        return tis.all()"}
{"prefix": "def get_task_instance(self, task_id, session=None):\n        \"\"\"\n        Returns the task instance specified by task_id for this dag run\n\n        :param task_id: the task id", "suffix": "\n        from airflow.models.taskinstance import TaskInstance  # Avoid circular import\n        TI = TaskInstance\n        ti = session.query(TI).filter(\n            TI.dag_id == self.dag_id,\n            TI.execution_date == self.execution_date,\n            TI.task_id == task_id\n        ).first()\n\n        return ti", "gt": "        \"\"\""}
{"prefix": "def get_previous_dagrun(self, session=None):\n        \"\"\"The previous DagRun, if there is one\"\"\"\n", "suffix": "            DagRun.dag_id == self.dag_id,\n            DagRun.execution_date < self.execution_date\n        ).order_by(\n            DagRun.execution_date.desc()\n        ).first()", "gt": "        return session.query(DagRun).filter("}
{"prefix": "def get_previous_scheduled_dagrun(self, session=None):\n        \"\"\"The previous, SCHEDULED DagRun, if there is one\"\"\"\n        dag = self.get_dag()", "suffix": "        return session.query(DagRun).filter(\n            DagRun.dag_id == self.dag_id,\n            DagRun.execution_date == dag.previous_schedule(self.execution_date)\n        ).first()", "gt": ""}
{"prefix": "def update_state(self, session=None):\n        \"\"\"\n        Determines the overall state of the DagRun based on the state\n        of its TaskInstances.\n\n        :return: State\n        \"\"\"\n\n        dag = self.get_dag()\n\n        tis = self.get_task_instances(session=session)\n        self.log.debug(\"Updating state for %s considering %s task(s)\", self, len(tis))\n\n        for ti in list(tis):\n            # skip in db?\n            if ti.state == State.REMOVED:\n                tis.remove(ti)\n            else:\n                ti.task = dag.get_task(ti.task_id)\n\n        # pre-calculate\n        # db is faster\n        start_dttm = timezone.utcnow()\n        unfinished_tasks = self.get_task_instances(\n            state=State.unfinished(),\n            session=session\n        )\n        none_depends_on_past = all(not t.task.depends_on_past for t in unfinished_tasks)\n        none_task_concurrency = all(t.task.task_concurrency is None\n                                    for t in unfinished_tasks)\n        # small speed up\n        if unfinished_tasks and none_depends_on_past and none_task_concurrency:", "suffix": "            no_dependencies_met = True\n            for ut in unfinished_tasks:\n                # We need to flag upstream and check for changes because upstream\n                # failures/re-schedules can result in deadlock false positives\n                old_state = ut.state\n                deps_met = ut.are_dependencies_met(\n                    dep_context=DepContext(\n                        flag_upstream_failed=True,\n                        ignore_in_retry_period=True,\n                        ignore_in_reschedule_period=True),\n                    session=session)\n                if deps_met or old_state != ut.current_state(session=session):\n                    no_dependencies_met = False\n                    break\n\n        duration = (timezone.utcnow() - start_dttm).total_seconds() * 1000\n        Stats.timing(\"dagrun.dependency-check.{}\".format(self.dag_id), duration)\n\n        root_ids = [t.task_id for t in dag.roots]\n        roots = [t for t in tis if t.task_id in root_ids]\n\n        # if all roots finished and at least one failed, the run failed\n        if (not unfinished_tasks and\n                any(r.state in (State.FAILED, State.UPSTREAM_FAILED) for r in roots)):\n            self.log.info('Marking run %s failed', self)\n            self.set_state(State.FAILED)\n            dag.handle_callback(self, success=False, reason='task_failure',\n                                session=session)\n\n        # if all roots succeeded and no unfinished tasks, the run succeeded\n        elif not unfinished_tasks and all(r.state in (State.SUCCESS, State.SKIPPED)\n                                          for r in roots):\n            self.log.info('Marking run %s successful', self)\n            self.set_state(State.SUCCESS)\n            dag.handle_callback(self, success=True, reason='success', session=session)\n\n        # if *all tasks* are deadlocked, the run failed\n        elif (unfinished_tasks and none_depends_on_past and\n              none_task_concurrency and no_dependencies_met):\n            self.log.info('Deadlock; marking run %s failed', self)\n            self.set_state(State.FAILED)\n            dag.handle_callback(self, success=False, reason='all_tasks_deadlocked',\n                                session=session)\n\n        # finally, if the roots aren't done, the dag is still running\n        else:\n            self.set_state(State.RUNNING)\n\n        self._emit_duration_stats_for_finished_state()\n\n        # todo: determine we want to use with_for_update to make sure to lock the run\n        session.merge(self)\n        session.commit()\n\n        return self.state", "gt": "            # todo: this can actually get pretty slow: one task costs between 0.01-015s"}
{"prefix": "def verify_integrity(self, session=None):\n        \"\"\"\n        Verifies the DagRun by checking for removed tasks or tasks that are not in the\n        database yet. It will set state to removed or add the task if required.\n        \"\"\"\n        from airflow.models.taskinstance import TaskInstance  # Avoid circular import\n\n        dag = self.get_dag()\n        tis = self.get_task_instances(session=session)\n\n        # check for removed or restored tasks\n        task_ids = []\n        for ti in tis:\n            task_ids.append(ti.task_id)", "suffix": "            try:\n                task = dag.get_task(ti.task_id)\n            except AirflowException:\n                if ti.state == State.REMOVED:\n                    pass  # ti has already been removed, just ignore it\n                elif self.state is not State.RUNNING and not dag.partial:\n                    self.log.warning(\"Failed to get task '{}' for dag '{}'. \"\n                                     \"Marking it as removed.\".format(ti, dag))\n                    Stats.incr(\n                        \"task_removed_from_dag.{}\".format(dag.dag_id), 1, 1)\n                    ti.state = State.REMOVED\n\n            is_task_in_dag = task is not None\n            should_restore_task = is_task_in_dag and ti.state == State.REMOVED\n            if should_restore_task:\n                self.log.info(\"Restoring task '{}' which was previously \"\n                              \"removed from DAG '{}'\".format(ti, dag))\n                Stats.incr(\"task_restored_to_dag.{}\".format(dag.dag_id), 1, 1)\n                ti.state = State.NONE\n\n        # check for missing tasks\n        for task in six.itervalues(dag.task_dict):\n            if task.start_date > self.execution_date and not self.is_backfill:\n                continue\n\n            if task.task_id not in task_ids:\n                Stats.incr(\n                    \"task_instance_created-{}\".format(task.__class__.__name__),\n                    1, 1)\n                ti = TaskInstance(task, self.execution_date)\n                session.add(ti)\n\n        session.commit()", "gt": "            task = None"}
{"prefix": "def get_run(session, dag_id, execution_date):\n        \"\"\"\n        :param dag_id: DAG ID\n        :type dag_id: unicode\n        :param execution_date: execution date", "suffix": "        :return: DagRun corresponding to the given dag_id and execution date\n            if one exists. None otherwise.\n        :rtype: airflow.models.DagRun\n        \"\"\"\n        qry = session.query(DagRun).filter(\n            DagRun.dag_id == dag_id,\n            DagRun.external_trigger == False, # noqa\n            DagRun.execution_date == execution_date,\n        )\n        return qry.first()", "gt": "        :type execution_date: datetime"}
{"prefix": "def jenkins_request_with_headers(jenkins_server, req):\n    \"\"\"\n    We need to get the headers in addition to the body answer\n    to get the location from them\n    This function uses jenkins_request method from python-jenkins library\n    with just the return call changed\n\n    :param jenkins_server: The server to query\n    :param req: The request to execute\n    :return: Dict containing the response body (key body)\n        and the headers coming along (headers)\n    \"\"\"\n    try:\n        response = jenkins_server.jenkins_request(req)\n        response_body = response.content\n        response_headers = response.headers\n        if response_body is None:\n            raise jenkins.EmptyResponseException(\n                \"Error communicating with server[%s]: \"\n                \"empty response\" % jenkins_server.server)\n        return {'body': response_body.decode('utf-8'), 'headers': response_headers}", "suffix": "        # Jenkins's funky authentication means its nigh impossible to\n        # distinguish errors.\n        if e.code in [401, 403, 500]:\n            # six.moves.urllib.error.HTTPError provides a 'reason'\n            # attribute for all python version except for ver 2.6\n            # Falling back to HTTPError.msg since it contains the\n            # same info as reason\n            raise JenkinsException(\n                'Error in request. ' +\n                'Possibly authentication failed [%s]: %s' % (\n                    e.code, e.msg)\n            )\n        elif e.code == 404:\n            raise jenkins.NotFoundException('Requested item could not be found')\n        else:\n            raise\n    except socket.timeout as e:\n        raise jenkins.TimeoutException('Error in request: %s' % e)\n    except URLError as e:\n        # python 2.6 compatibility to ensure same exception raised\n        # since URLError wraps a socket timeout on python 2.6.\n        if str(e.reason) == \"timed out\":\n            raise jenkins.TimeoutException('Error in request: %s' % e.reason)\n        raise JenkinsException('Error in request: %s' % e.reason)", "gt": "    except HTTPError as e:"}
{"prefix": "def build_job(self, jenkins_server):\n        \"\"\"\n        This function makes an API call to Jenkins to trigger a build for 'job_name'", "suffix": "        headers contains also a dict-like object which can be queried to get\n        the location to poll in the queue.\n\n        :param jenkins_server: The jenkins server where the job should be triggered\n        :return: Dict containing the response body (key body)\n            and the headers coming along (headers)\n        \"\"\"\n        # Warning if the parameter is too long, the URL can be longer than\n        # the maximum allowed size\n        if self.parameters and isinstance(self.parameters, six.string_types):\n            import ast\n            self.parameters = ast.literal_eval(self.parameters)\n\n        if not self.parameters:\n            # We need a None to call the non parametrized jenkins api end point\n            self.parameters = None\n\n        request = Request(jenkins_server.build_job_url(self.job_name,\n                                                       self.parameters, None))\n        return jenkins_request_with_headers(jenkins_server, request)", "gt": "        It returned a dict with 2 keys : body and headers."}
{"prefix": "def poll_job_in_queue(self, location, jenkins_server):\n        \"\"\"\n        This method poll the jenkins queue until the job is executed.\n        When we trigger a job through an API call,\n        the job is first put in the queue without having a build number assigned.\n        Thus we have to wait the job exit the queue to know its build number.\n        To do so, we have to add /api/json (or /api/xml) to the location\n        returned by the build_job call and poll this file.\n        When a 'executable' block appears in the json, it means the job execution started\n        and the field 'number' then contains the build number.\n\n        :param location: Location to poll, returned in the header of the build_job call", "suffix": "        :return: The build_number corresponding to the triggered job\n        \"\"\"\n        try_count = 0\n        location = location + '/api/json'\n        # TODO Use get_queue_info instead\n        # once it will be available in python-jenkins (v > 0.4.15)\n        self.log.info('Polling jenkins queue at the url %s', location)\n        while try_count < self.max_try_before_job_appears:\n            location_answer = jenkins_request_with_headers(jenkins_server,\n                                                           Request(location))\n            if location_answer is not None:\n                json_response = json.loads(location_answer['body'])\n                if 'executable' in json_response:\n                    build_number = json_response['executable']['number']\n                    self.log.info('Job executed on Jenkins side with the build number %s',\n                                  build_number)\n                    return build_number\n            try_count += 1\n            time.sleep(self.sleep_time)\n        raise AirflowException(\"The job hasn't been executed\"\n                               \" after polling the queue %d times\",\n                               self.max_try_before_job_appears)", "gt": "        :param jenkins_server: The jenkins server to poll"}
{"prefix": "def context_to_airflow_vars(context, in_env_var_format=False):\n    \"\"\"", "suffix": "    externally reconstruct relations between dags, dag_runs, tasks and task_instances.\n    Default to abc.def.ghi format and can be made to ABC_DEF_GHI format if\n    in_env_var_format is set to True.\n\n    :param context: The context for the task_instance of interest.\n    :type context: dict\n    :param in_env_var_format: If returned vars should be in ABC_DEF_GHI format.\n    :type in_env_var_format: bool\n    :return: task_instance context as dict.\n    \"\"\"\n    params = dict()\n    if in_env_var_format:\n        name_format = 'env_var_format'\n    else:\n        name_format = 'default'\n    task_instance = context.get('task_instance')\n    if task_instance and task_instance.dag_id:\n        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_DAG_ID'][\n            name_format]] = task_instance.dag_id\n    if task_instance and task_instance.task_id:\n        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_TASK_ID'][\n            name_format]] = task_instance.task_id\n    if task_instance and task_instance.execution_date:\n        params[\n            AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_EXECUTION_DATE'][\n                name_format]] = task_instance.execution_date.isoformat()\n    dag_run = context.get('dag_run')\n    if dag_run and dag_run.run_id:\n        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_DAG_RUN_ID'][\n            name_format]] = dag_run.run_id\n    return params", "gt": "    Given a context, this function provides a dictionary of values that can be used to"}
{"prefix": "def on_pre_execution(**kwargs):", "suffix": "    Calls callbacks before execution.\n    Note that any exception from callback will be logged but won't be propagated.\n    :param kwargs:\n    :return: None\n    \"\"\"\n    logging.debug(\"Calling callbacks: %s\", __pre_exec_callbacks)\n    for cb in __pre_exec_callbacks:\n        try:\n            cb(**kwargs)\n        except Exception:\n            logging.exception('Failed on pre-execution callback using %s', cb)", "gt": "    \"\"\""}
{"prefix": "def on_post_execution(**kwargs):\n    \"\"\"", "suffix": "    As it's being called after execution, it can capture status of execution,\n    duration, etc. Note that any exception from callback will be logged but\n    won't be propagated.\n    :param kwargs:\n    :return: None\n    \"\"\"\n    logging.debug(\"Calling callbacks: %s\", __post_exec_callbacks)\n    for cb in __post_exec_callbacks:\n        try:\n            cb(**kwargs)\n        except Exception:\n            logging.exception('Failed on post-execution callback using %s', cb)", "gt": "    Calls callbacks after execution."}
{"prefix": "def conditionally_trigger(context, dag_run_obj):\n    \"\"\"This function decides whether or not to Trigger the remote DAG\"\"\"\n    c_p = context['params']['condition_param']\n    print(\"Controller DAG : conditionally_trigger = {}\".format(c_p))\n    if context['params']['condition_param']:\n        dag_run_obj.payload = {'message': context['params']['message']}\n        pp.pprint(dag_run_obj.payload)", "suffix": "", "gt": "        return dag_run_obj"}
{"prefix": "def send_metric(self, metric_name, datapoint, tags=None, type_=None, interval=None):\n        \"\"\"\n        Sends a single datapoint metric to DataDog\n\n        :param metric_name: The name of the metric\n        :type metric_name: str\n        :param datapoint: A single integer or float related to the metric\n        :type datapoint: int or float\n        :param tags: A list of tags associated with the metric\n        :type tags: list\n        :param type_: Type of your metric: gauge, rate, or count\n        :type type_: str\n        :param interval: If the type of the metric is rate or count, define the corresponding interval\n        :type interval: int\n        \"\"\"\n        response = api.Metric.send(\n            metric=metric_name,\n            points=datapoint,\n            host=self.host,\n            tags=tags,", "suffix": "            interval=interval)\n\n        self.validate_response(response)\n        return response", "gt": "            type=type_,"}
{"prefix": "def query_metric(self,\n                     query,\n                     from_seconds_ago,\n                     to_seconds_ago):\n        \"\"\"\n        Queries datadog for a specific metric, potentially with some\n        function applied to it and returns the results.\n\n        :param query: The datadog query to execute (see datadog docs)\n        :type query: str\n        :param from_seconds_ago: How many seconds ago to start querying for.", "suffix": "        :param to_seconds_ago: Up to how many seconds ago to query for.\n        :type to_seconds_ago: int\n        \"\"\"\n        now = int(time.time())\n\n        response = api.Metric.query(\n            start=now - from_seconds_ago,\n            end=now - to_seconds_ago,\n            query=query)\n\n        self.validate_response(response)\n        return response", "gt": "        :type from_seconds_ago: int"}
{"prefix": "def post_event(self, title, text, aggregation_key=None, alert_type=None, date_happened=None,\n                   handle=None, priority=None, related_event_id=None, tags=None, device_name=None):\n        \"\"\"\n        Posts an event to datadog (processing finished, potentially alerts, other issues)\n        Think about this as a means to maintain persistence of alerts, rather than\n        alerting itself.\n\n        :param title: The title of the event\n        :type title: str\n        :param text: The body of the event (more information)\n        :type text: str\n        :param aggregation_key: Key that can be used to aggregate this event in a stream\n        :type aggregation_key: str\n        :param alert_type: The alert type for the event, one of\n            [\"error\", \"warning\", \"info\", \"success\"]\n        :type alert_type: str\n        :param date_happened: POSIX timestamp of the event; defaults to now\n        :type date_happened: int\n        :handle: User to post the event as; defaults to owner of the application key used\n            to submit.\n        :param handle: str\n        :param priority: Priority to post the event as. (\"normal\" or \"low\", defaults to \"normal\")\n        :type priority: str\n        :param related_event_id: Post event as a child of the given event\n        :type related_event_id: id", "suffix": "        :type tags: list[str]\n        :param device_name: device_name to post the event with\n        :type device_name: list\n        \"\"\"\n        response = api.Event.create(\n            title=title,\n            text=text,\n            aggregation_key=aggregation_key,\n            alert_type=alert_type,\n            date_happened=date_happened,\n            handle=handle,\n            priority=priority,\n            related_event_id=related_event_id,\n            tags=tags,\n            host=self.host,\n            device_name=device_name,\n            source_type_name=self.source_type_name)\n\n        self.validate_response(response)\n        return response", "gt": "        :param tags: List of tags to apply to the event"}
{"prefix": "def _get_token(self, token, http_conn_id):\n        \"\"\"\n        Given either a manually set token or a conn_id, return the webhook_token to use", "suffix": "        :type token: str\n        :param http_conn_id: The conn_id provided\n        :type http_conn_id: str\n        :return: webhook_token (str) to use\n        \"\"\"\n        if token:\n            return token\n        elif http_conn_id:\n            conn = self.get_connection(http_conn_id)\n            extra = conn.extra_dejson\n            return extra.get('webhook_token', '')\n        else:\n            raise AirflowException('Cannot get token: No valid Slack '\n                                   'webhook token nor conn_id supplied')", "gt": "        :param token: The manually provided token"}
{"prefix": "def _build_slack_message(self):\n        \"\"\"\n        Construct the Slack message. All relevant parameters are combined here to a valid", "suffix": "        :return: Slack message (str) to send\n        \"\"\"\n        cmd = {}\n\n        if self.channel:\n            cmd['channel'] = self.channel\n        if self.username:\n            cmd['username'] = self.username\n        if self.icon_emoji:\n            cmd['icon_emoji'] = self.icon_emoji\n        if self.link_names:\n            cmd['link_names'] = 1\n        if self.attachments:\n            cmd['attachments'] = self.attachments\n\n        cmd['text'] = self.message\n        return json.dumps(cmd)", "gt": "        Slack json message"}
{"prefix": "def execute(self):\n        \"\"\"\n        Remote Popen (actually execute the slack webhook call)\n        \"\"\"\n        proxies = {}", "suffix": "            # we only need https proxy for Slack, as the endpoint is https\n            proxies = {'https': self.proxy}\n\n        slack_message = self._build_slack_message()\n        self.run(endpoint=self.webhook_token,\n                 data=slack_message,\n                 headers={'Content-type': 'application/json'},\n                 extra_options={'proxies': proxies})", "gt": "        if self.proxy:"}
{"prefix": "def get_dag(self, dag_id):\n        \"\"\"\n        Gets the DAG out of the dictionary, and refreshes it if expired\n        \"\"\"\n        from airflow.models.dag import DagModel  # Avoid circular import\n\n        # If asking for a known subdag, we want to refresh the parent\n        root_dag_id = dag_id\n        if dag_id in self.dags:\n            dag = self.dags[dag_id]", "suffix": "                root_dag_id = dag.parent_dag.dag_id\n\n        # If the dag corresponding to root_dag_id is absent or expired\n        orm_dag = DagModel.get_current(root_dag_id)\n        if orm_dag and (\n                root_dag_id not in self.dags or\n                (\n                    orm_dag.last_expired and\n                    dag.last_loaded < orm_dag.last_expired\n                )\n        ):\n            # Reprocess source file\n            found_dags = self.process_file(\n                filepath=orm_dag.fileloc, only_if_updated=False)\n\n            # If the source file no longer exports `dag_id`, delete it from self.dags\n            if found_dags and dag_id in [found_dag.dag_id for found_dag in found_dags]:\n                return self.dags[dag_id]\n            elif dag_id in self.dags:\n                del self.dags[dag_id]\n        return self.dags.get(dag_id)", "gt": "            if dag.is_subdag:"}
{"prefix": "def process_file(self, filepath, only_if_updated=True, safe_mode=True):\n        \"\"\"\n        Given a path to a python module or zip file, this method imports\n        the module and look for dag objects within it.\n        \"\"\"\n        from airflow.models.dag import DAG  # Avoid circular import\n\n        found_dags = []\n\n        # if the source file no longer exists in the DB or in the filesystem,\n        # return an empty list\n        # todo: raise exception?\n        if filepath is None or not os.path.isfile(filepath):\n            return found_dags\n\n        try:\n            # This failed before in what may have been a git sync\n            # race condition\n            file_last_changed_on_disk = datetime.fromtimestamp(os.path.getmtime(filepath))\n            if only_if_updated \\\n                    and filepath in self.file_last_changed \\\n                    and file_last_changed_on_disk == self.file_last_changed[filepath]:\n                return found_dags\n\n        except Exception as e:\n            self.log.exception(e)\n            return found_dags\n\n        mods = []\n        is_zipfile = zipfile.is_zipfile(filepath)\n        if not is_zipfile:\n            if safe_mode:\n                with open(filepath, 'rb') as f:\n                    content = f.read()\n                    if not all([s in content for s in (b'DAG', b'airflow')]):\n                        self.file_last_changed[filepath] = file_last_changed_on_disk\n                        # Don't want to spam user with skip messages\n                        if not self.has_logged:\n                            self.has_logged = True\n                            self.log.info(\n                                \"File %s assumed to contain no DAGs. Skipping.\",\n                                filepath)\n                        return found_dags\n\n            self.log.debug(\"Importing %s\", filepath)\n            org_mod_name, _ = os.path.splitext(os.path.split(filepath)[-1])\n            mod_name = ('unusual_prefix_' +\n                        hashlib.sha1(filepath.encode('utf-8')).hexdigest() +\n                        '_' + org_mod_name)\n\n            if mod_name in sys.modules:\n                del sys.modules[mod_name]\n\n            with timeout(configuration.conf.getint('core', \"DAGBAG_IMPORT_TIMEOUT\")):\n                try:\n                    m = imp.load_source(mod_name, filepath)\n                    mods.append(m)\n                except Exception as e:", "suffix": "                    self.import_errors[filepath] = str(e)\n                    self.file_last_changed[filepath] = file_last_changed_on_disk\n\n        else:\n            zip_file = zipfile.ZipFile(filepath)\n            for mod in zip_file.infolist():\n                head, _ = os.path.split(mod.filename)\n                mod_name, ext = os.path.splitext(mod.filename)\n                if not head and (ext == '.py' or ext == '.pyc'):\n                    if mod_name == '__init__':\n                        self.log.warning(\"Found __init__.%s at root of %s\", ext, filepath)\n                    if safe_mode:\n                        with zip_file.open(mod.filename) as zf:\n                            self.log.debug(\"Reading %s from %s\", mod.filename, filepath)\n                            content = zf.read()\n                            if not all([s in content for s in (b'DAG', b'airflow')]):\n                                self.file_last_changed[filepath] = (\n                                    file_last_changed_on_disk)\n                                # todo: create ignore list\n                                # Don't want to spam user with skip messages\n                                if not self.has_logged:\n                                    self.has_logged = True\n                                    self.log.info(\n                                        \"File %s assumed to contain no DAGs. Skipping.\",\n                                        filepath)\n\n                    if mod_name in sys.modules:\n                        del sys.modules[mod_name]\n\n                    try:\n                        sys.path.insert(0, filepath)\n                        m = importlib.import_module(mod_name)\n                        mods.append(m)\n                    except Exception as e:\n                        self.log.exception(\"Failed to import: %s\", filepath)\n                        self.import_errors[filepath] = str(e)\n                        self.file_last_changed[filepath] = file_last_changed_on_disk\n\n        for m in mods:\n            for dag in list(m.__dict__.values()):\n                if isinstance(dag, DAG):\n                    if not dag.full_filepath:\n                        dag.full_filepath = filepath\n                        if dag.fileloc != filepath and not is_zipfile:\n                            dag.fileloc = filepath\n                    try:\n                        dag.is_subdag = False\n                        self.bag_dag(dag, parent_dag=dag, root_dag=dag)\n                        if isinstance(dag._schedule_interval, six.string_types):\n                            croniter(dag._schedule_interval)\n                        found_dags.append(dag)\n                        found_dags += dag.subdags\n                    except (CroniterBadCronError,\n                            CroniterBadDateError,\n                            CroniterNotAlphaError) as cron_e:\n                        self.log.exception(\"Failed to bag_dag: %s\", dag.full_filepath)\n                        self.import_errors[dag.full_filepath] = \\\n                            \"Invalid Cron expression: \" + str(cron_e)\n                        self.file_last_changed[dag.full_filepath] = \\\n                            file_last_changed_on_disk\n                    except AirflowDagCycleException as cycle_exception:\n                        self.log.exception(\"Failed to bag_dag: %s\", dag.full_filepath)\n                        self.import_errors[dag.full_filepath] = str(cycle_exception)\n                        self.file_last_changed[dag.full_filepath] = \\\n                            file_last_changed_on_disk\n\n        self.file_last_changed[filepath] = file_last_changed_on_disk\n        return found_dags", "gt": "                    self.log.exception(\"Failed to import: %s\", filepath)"}
{"prefix": "def kill_zombies(self, zombies, session=None):\n        \"\"\"\n        Fail given zombie tasks, which are tasks that haven't\n        had a heartbeat for too long, in the current DagBag.\n\n        :param zombies: zombie task instances to kill.\n        :type zombies: airflow.utils.dag_processing.SimpleTaskInstance\n        :param session: DB session.\n        :type session: sqlalchemy.orm.session.Session\n        \"\"\"\n        from airflow.models.taskinstance import TaskInstance  # Avoid circular import\n\n        for zombie in zombies:\n            if zombie.dag_id in self.dags:\n                dag = self.dags[zombie.dag_id]\n                if zombie.task_id in dag.task_ids:\n                    task = dag.get_task(zombie.task_id)\n                    ti = TaskInstance(task, zombie.execution_date)\n                    # Get properties needed for failure handling from SimpleTaskInstance.\n                    ti.start_date = zombie.start_date\n                    ti.end_date = zombie.end_date\n                    ti.try_number = zombie.try_number", "suffix": "                    ti.test_mode = configuration.getboolean('core', 'unit_test_mode')\n                    ti.handle_failure(\"{} detected as zombie\".format(ti),\n                                      ti.test_mode, ti.get_template_context())\n                    self.log.info(\n                        'Marked zombie job %s as %s', ti, ti.state)\n                    Stats.incr('zombies_killed')\n        session.commit()", "gt": "                    ti.state = zombie.state"}
{"prefix": "def bag_dag(self, dag, parent_dag, root_dag):\n        \"\"\"\n        Adds the DAG into the bag, recurses into sub dags.\n        Throws AirflowDagCycleException if a cycle is detected in this dag or its subdags\n        \"\"\"\n\n        dag.test_cycle()  # throws if a task cycle is found\n\n        dag.resolve_template_files()\n        dag.last_loaded = timezone.utcnow()\n\n        for task in dag.tasks:\n            settings.policy(task)\n\n        subdags = dag.subdags\n\n        try:", "suffix": "                subdag.full_filepath = dag.full_filepath\n                subdag.parent_dag = dag\n                subdag.is_subdag = True\n                self.bag_dag(subdag, parent_dag=dag, root_dag=root_dag)\n\n            self.dags[dag.dag_id] = dag\n            self.log.debug('Loaded DAG %s', dag)\n        except AirflowDagCycleException as cycle_exception:\n            # There was an error in bagging the dag. Remove it from the list of dags\n            self.log.exception('Exception bagging dag: %s', dag.dag_id)\n            # Only necessary at the root level since DAG.subdags automatically\n            # performs DFS to search through all subdags\n            if dag == root_dag:\n                for subdag in subdags:\n                    if subdag.dag_id in self.dags:\n                        del self.dags[subdag.dag_id]\n            raise cycle_exception", "gt": "            for subdag in subdags:"}
{"prefix": "def collect_dags(\n            self,\n            dag_folder=None,\n            only_if_updated=True,\n            include_examples=configuration.conf.getboolean('core', 'LOAD_EXAMPLES'),\n            safe_mode=configuration.conf.getboolean('core', 'DAG_DISCOVERY_SAFE_MODE')):\n        \"\"\"\n        Given a file path or a folder, this method looks for python modules,\n        imports them and adds them to the dagbag collection.\n\n        Note that if a ``.airflowignore`` file is found while processing\n        the directory, it will behave much like a ``.gitignore``,\n        ignoring files that match any of the regex patterns specified\n        in the file.\n\n        **Note**: The patterns in .airflowignore are treated as\n        un-anchored regexes, not shell-like glob patterns.\n        \"\"\"\n        start_dttm = timezone.utcnow()\n        dag_folder = dag_folder or self.dag_folder\n\n        # Used to store stats around DagBag processing\n        stats = []\n        FileLoadStat = namedtuple(\n            'FileLoadStat', \"file duration dag_num task_num dags\")\n\n        dag_folder = correct_maybe_zipped(dag_folder)\n\n        for filepath in list_py_file_paths(dag_folder, safe_mode=safe_mode,\n                                           include_examples=include_examples):\n            try:\n                ts = timezone.utcnow()\n                found_dags = self.process_file(\n                    filepath, only_if_updated=only_if_updated,", "suffix": "\n                td = timezone.utcnow() - ts\n                td = td.total_seconds() + (\n                    float(td.microseconds) / 1000000)\n                stats.append(FileLoadStat(\n                    filepath.replace(dag_folder, ''),\n                    td,\n                    len(found_dags),\n                    sum([len(dag.tasks) for dag in found_dags]),\n                    str([dag.dag_id for dag in found_dags]),\n                ))\n            except Exception as e:\n                self.log.exception(e)\n        Stats.gauge(\n            'collect_dags', (timezone.utcnow() - start_dttm).total_seconds(), 1)\n        Stats.gauge(\n            'dagbag_size', len(self.dags), 1)\n        Stats.gauge(\n            'dagbag_import_errors', len(self.import_errors), 1)\n        self.dagbag_stats = sorted(\n            stats, key=lambda x: x.duration, reverse=True)", "gt": "                    safe_mode=safe_mode)"}
{"prefix": "def dagbag_report(self):\n        \"\"\"Prints a report around DagBag loading stats\"\"\"\n        report = textwrap.dedent(\"\"\"\\n\n        -------------------------------------------------------------------\n        DagBag loading stats for {dag_folder}", "suffix": "        Number of DAGs: {dag_num}\n        Total task number: {task_num}\n        DagBag parsing time: {duration}\n        {table}\n        \"\"\")\n        stats = self.dagbag_stats\n        return report.format(\n            dag_folder=self.dag_folder,\n            duration=sum([o.duration for o in stats]),\n            dag_num=sum([o.dag_num for o in stats]),\n            task_num=sum([o.task_num for o in stats]),\n            table=pprinttable(stats),\n        )", "gt": "        -------------------------------------------------------------------"}
{"prefix": "def execute(self, context):\n        \"\"\"\n        Call the SparkSubmitHook to run the provided spark job\n        \"\"\"", "suffix": "            spark_app_name=self._spark_app_name,\n            spark_conn_id=self._spark_conn_id,\n            spark_conf=self._spark_conf,\n            spark_py_files=self._spark_py_files,\n            spark_files=self._spark_files,\n            spark_jars=self._spark_jars,\n            num_executors=self._num_executors,\n            executor_cores=self._executor_cores,\n            executor_memory=self._executor_memory,\n            driver_memory=self._driver_memory,\n            verbose=self._verbose,\n            keytab=self._keytab,\n            principal=self._principal,\n            cmd_type=self._cmd_type,\n            jdbc_table=self._jdbc_table,\n            jdbc_conn_id=self._jdbc_conn_id,\n            jdbc_driver=self._jdbc_driver,\n            metastore_table=self._metastore_table,\n            jdbc_truncate=self._jdbc_truncate,\n            save_mode=self._save_mode,\n            save_format=self._save_format,\n            batch_size=self._batch_size,\n            fetch_size=self._fetch_size,\n            num_partitions=self._num_partitions,\n            partition_column=self._partition_column,\n            lower_bound=self._lower_bound,\n            upper_bound=self._upper_bound,\n            create_table_column_types=self._create_table_column_types\n        )\n        self._hook.submit_jdbc_job()", "gt": "        self._hook = SparkJDBCHook("}
{"prefix": "def ds_add(ds, days):\n    \"\"\"\n    Add or subtract days from a YYYY-MM-DD\n\n    :param ds: anchor date in ``YYYY-MM-DD`` format to add to\n    :type ds: str\n    :param days: number of days to add to the ds, you can use negative values", "suffix": "\n    >>> ds_add('2015-01-01', 5)\n    '2015-01-06'\n    >>> ds_add('2015-01-06', -5)\n    '2015-01-01'\n    \"\"\"\n\n    ds = datetime.strptime(ds, '%Y-%m-%d')\n    if days:\n        ds = ds + timedelta(days)\n    return ds.isoformat()[:10]", "gt": "    :type days: int"}
{"prefix": "def ds_format(ds, input_format, output_format):\n    \"\"\"\n    Takes an input string and outputs another string\n    as specified in the output format\n\n    :param ds: input string which contains a date\n    :type ds: str", "suffix": "    :type input_format: str\n    :param output_format: output string format  E.g. %Y-%m-%d\n    :type output_format: str\n\n    >>> ds_format('2015-01-01', \"%Y-%m-%d\", \"%m-%d-%y\")\n    '01-01-15'\n    >>> ds_format('1/5/2015', \"%m/%d/%Y\",  \"%Y-%m-%d\")\n    '2015-01-05'\n    \"\"\"\n    return datetime.strptime(ds, input_format).strftime(output_format)", "gt": "    :param input_format: input string format. E.g. %Y-%m-%d"}
{"prefix": "def _integrate_plugins():\n    \"\"\"Integrate plugins to the context\"\"\"", "suffix": "    from airflow.plugins_manager import macros_modules\n    for macros_module in macros_modules:\n        sys.modules[macros_module.__name__] = macros_module\n        globals()[macros_module._name] = macros_module", "gt": "    import sys"}
{"prefix": "def poke(self, context):\n        \"\"\"\n        poke matching files in a directory with self.regex\n\n        :return: Bool depending on the search criteria\n        \"\"\"\n        sb = self.hook(self.hdfs_conn_id).get_conn()", "suffix": "            'Poking for %s to be a directory with files matching %s', self.filepath, self.regex.pattern\n        )\n        result = [f for f in sb.ls([self.filepath], include_toplevel=False) if\n                  f['file_type'] == 'f' and\n                  self.regex.match(f['path'].replace('%s/' % self.filepath, ''))]\n        result = self.filter_for_ignored_ext(result, self.ignored_ext,\n                                             self.ignore_copying)\n        result = self.filter_for_filesize(result, self.file_size)\n        return bool(result)", "gt": "        self.log.info("}
{"prefix": "def poke(self, context):\n        \"\"\"\n        poke for a non empty directory\n\n        :return: Bool depending on the search criteria\n        \"\"\"\n        sb = self.hook(self.hdfs_conn_id).get_conn()\n        result = [f for f in sb.ls([self.filepath], include_toplevel=True)]\n        result = self.filter_for_ignored_ext(result, self.ignored_ext,\n                                             self.ignore_copying)\n        result = self.filter_for_filesize(result, self.file_size)\n        if self.be_empty:\n            self.log.info('Poking for filepath %s to a empty directory', self.filepath)\n            return len(result) == 1 and result[0]['path'] == self.filepath\n        else:\n            self.log.info('Poking for filepath %s to a non empty directory', self.filepath)", "suffix": "            return bool(result) and result[0]['file_type'] == 'f'", "gt": "            result.pop(0)"}
{"prefix": "def clear_task_instances(tis,\n                         session,\n                         activate_dag_runs=True,\n                         dag=None,", "suffix": "    \"\"\"\n    Clears a set of task instances, but makes sure the running ones\n    get killed.\n\n    :param tis: a list of task instances\n    :param session: current session\n    :param activate_dag_runs: flag to check for active dag run\n    :param dag: DAG object\n    \"\"\"\n    job_ids = []\n    for ti in tis:\n        if ti.state == State.RUNNING:\n            if ti.job_id:\n                ti.state = State.SHUTDOWN\n                job_ids.append(ti.job_id)\n        else:\n            task_id = ti.task_id\n            if dag and dag.has_task(task_id):\n                task = dag.get_task(task_id)\n                task_retries = task.retries\n                ti.max_tries = ti.try_number + task_retries - 1\n            else:\n                # Ignore errors when updating max_tries if dag is None or\n                # task not found in dag since database records could be\n                # outdated. We make max_tries the maximum value of its\n                # original max_tries or the current task try number.\n                ti.max_tries = max(ti.max_tries, ti.try_number - 1)\n            ti.state = State.NONE\n            session.merge(ti)\n\n    if job_ids:\n        from airflow.jobs import BaseJob as BJ\n        for job in session.query(BJ).filter(BJ.id.in_(job_ids)).all():\n            job.state = State.SHUTDOWN\n\n    if activate_dag_runs and tis:\n        from airflow.models.dagrun import DagRun  # Avoid circular import\n        drs = session.query(DagRun).filter(\n            DagRun.dag_id.in_({ti.dag_id for ti in tis}),\n            DagRun.execution_date.in_({ti.execution_date for ti in tis}),\n        ).all()\n        for dr in drs:\n            dr.state = State.RUNNING\n            dr.start_date = timezone.utcnow()", "gt": "                         ):"}
{"prefix": "def try_number(self):\n        \"\"\"\n        Return the try number that this task number will be when it is actually\n        run.\n\n        If the TI is currently running, this will match the column in the\n        databse, in all othercases this will be incremenetd\n        \"\"\"", "suffix": "        if self.state == State.RUNNING:\n            return self._try_number\n        return self._try_number + 1", "gt": "        # This is designed so that task logs end up in the right file."}
{"prefix": "def command(\n            self,\n            mark_success=False,\n            ignore_all_deps=False,\n            ignore_depends_on_past=False,\n            ignore_task_deps=False,\n            ignore_ti_state=False,\n            local=False,\n            pickle_id=None,\n            raw=False,\n            job_id=None,\n            pool=None,\n            cfg_path=None):\n        \"\"\"\n        Returns a command that can be executed anywhere where airflow is\n        installed. This command is part of the message sent to executors by\n        the orchestrator.\n        \"\"\"\n        return \" \".join(self.command_as_list(\n            mark_success=mark_success,\n            ignore_all_deps=ignore_all_deps,\n            ignore_depends_on_past=ignore_depends_on_past,\n            ignore_task_deps=ignore_task_deps,\n            ignore_ti_state=ignore_ti_state,\n            local=local,\n            pickle_id=pickle_id,\n            raw=raw,\n            job_id=job_id,", "suffix": "            cfg_path=cfg_path))", "gt": "            pool=pool,"}
{"prefix": "def command_as_list(\n            self,\n            mark_success=False,\n            ignore_all_deps=False,\n            ignore_task_deps=False,\n            ignore_depends_on_past=False,\n            ignore_ti_state=False,\n            local=False,\n            pickle_id=None,\n            raw=False,\n            job_id=None,\n            pool=None,\n            cfg_path=None):\n        \"\"\"\n        Returns a command that can be executed anywhere where airflow is\n        installed. This command is part of the message sent to executors by\n        the orchestrator.\n        \"\"\"\n        dag = self.task.dag\n\n        should_pass_filepath = not pickle_id and dag\n        if should_pass_filepath and dag.full_filepath != dag.filepath:\n            path = \"DAGS_FOLDER/{}\".format(dag.filepath)\n        elif should_pass_filepath and dag.full_filepath:\n            path = dag.full_filepath\n        else:\n            path = None\n\n        return TaskInstance.generate_command(\n            self.dag_id,\n            self.task_id,\n            self.execution_date,\n            mark_success=mark_success,", "suffix": "            ignore_task_deps=ignore_task_deps,\n            ignore_depends_on_past=ignore_depends_on_past,\n            ignore_ti_state=ignore_ti_state,\n            local=local,\n            pickle_id=pickle_id,\n            file_path=path,\n            raw=raw,\n            job_id=job_id,\n            pool=pool,\n            cfg_path=cfg_path)", "gt": "            ignore_all_deps=ignore_all_deps,"}
{"prefix": "def generate_command(dag_id,\n                         task_id,\n                         execution_date,\n                         mark_success=False,\n                         ignore_all_deps=False,\n                         ignore_depends_on_past=False,\n                         ignore_task_deps=False,", "suffix": "                         local=False,\n                         pickle_id=None,\n                         file_path=None,\n                         raw=False,\n                         job_id=None,\n                         pool=None,\n                         cfg_path=None\n                         ):\n        \"\"\"\n        Generates the shell command required to execute this task instance.\n\n        :param dag_id: DAG ID\n        :type dag_id: unicode\n        :param task_id: Task ID\n        :type task_id: unicode\n        :param execution_date: Execution date for the task\n        :type execution_date: datetime\n        :param mark_success: Whether to mark the task as successful\n        :type mark_success: bool\n        :param ignore_all_deps: Ignore all ignorable dependencies.\n            Overrides the other ignore_* parameters.\n        :type ignore_all_deps: bool\n        :param ignore_depends_on_past: Ignore depends_on_past parameter of DAGs\n            (e.g. for Backfills)\n        :type ignore_depends_on_past: bool\n        :param ignore_task_deps: Ignore task-specific dependencies such as depends_on_past\n            and trigger rule\n        :type ignore_task_deps: bool\n        :param ignore_ti_state: Ignore the task instance's previous failure/success\n        :type ignore_ti_state: bool\n        :param local: Whether to run the task locally\n        :type local: bool\n        :param pickle_id: If the DAG was serialized to the DB, the ID\n            associated with the pickled DAG\n        :type pickle_id: unicode\n        :param file_path: path to the file containing the DAG definition\n        :param raw: raw mode (needs more details)\n        :param job_id: job ID (needs more details)\n        :param pool: the Airflow pool that the task should run in\n        :type pool: unicode\n        :param cfg_path: the Path to the configuration file\n        :type cfg_path: basestring\n        :return: shell command that can be used to run the task instance\n        \"\"\"\n        iso = execution_date.isoformat()\n        cmd = [\"airflow\", \"run\", str(dag_id), str(task_id), str(iso)]\n        cmd.extend([\"--mark_success\"]) if mark_success else None\n        cmd.extend([\"--pickle\", str(pickle_id)]) if pickle_id else None\n        cmd.extend([\"--job_id\", str(job_id)]) if job_id else None\n        cmd.extend([\"-A\"]) if ignore_all_deps else None\n        cmd.extend([\"-i\"]) if ignore_task_deps else None\n        cmd.extend([\"-I\"]) if ignore_depends_on_past else None\n        cmd.extend([\"--force\"]) if ignore_ti_state else None\n        cmd.extend([\"--local\"]) if local else None\n        cmd.extend([\"--pool\", pool]) if pool else None\n        cmd.extend([\"--raw\"]) if raw else None\n        cmd.extend([\"-sd\", file_path]) if file_path else None\n        cmd.extend([\"--cfg_path\", cfg_path]) if cfg_path else None\n        return cmd", "gt": "                         ignore_ti_state=False,"}
{"prefix": "def current_state(self, session=None):\n        \"\"\"\n        Get the very latest state from the database, if a session is passed,\n        we use and looking up the state becomes part of the session, otherwise\n        a new session is used.\n        \"\"\"\n        TI = TaskInstance\n        ti = session.query(TI).filter(\n            TI.dag_id == self.dag_id,\n            TI.task_id == self.task_id,", "suffix": "        ).all()\n        if ti:\n            state = ti[0].state\n        else:\n            state = None\n        return state", "gt": "            TI.execution_date == self.execution_date,"}
{"prefix": "def error(self, session=None):", "suffix": "        Forces the task instance's state to FAILED in the database.\n        \"\"\"\n        self.log.error(\"Recording the task instance as FAILED\")\n        self.state = State.FAILED\n        session.merge(self)\n        session.commit()", "gt": "        \"\"\""}
{"prefix": "def refresh_from_db(self, session=None, lock_for_update=False):\n        \"\"\"\n        Refreshes the task instance from the database based on the primary key\n\n        :param lock_for_update: if True, indicates that the database should\n            lock the TaskInstance (issuing a FOR UPDATE clause) until the\n            session is committed.\n        \"\"\"\n        TI = TaskInstance\n\n        qry = session.query(TI).filter(\n            TI.dag_id == self.dag_id,\n            TI.task_id == self.task_id,\n            TI.execution_date == self.execution_date)\n\n        if lock_for_update:\n            ti = qry.with_for_update().first()\n        else:\n            ti = qry.first()\n        if ti:", "suffix": "            self.start_date = ti.start_date\n            self.end_date = ti.end_date\n            # Get the raw value of try_number column, don't read through the\n            # accessor here otherwise it will be incremeneted by one already.\n            self.try_number = ti._try_number\n            self.max_tries = ti.max_tries\n            self.hostname = ti.hostname\n            self.pid = ti.pid\n            self.executor_config = ti.executor_config\n        else:\n            self.state = None", "gt": "            self.state = ti.state"}
{"prefix": "", "suffix": "        \"\"\"\n        Clears all XCom data from the database for the task instance\n        \"\"\"\n        session.query(XCom).filter(\n            XCom.dag_id == self.dag_id,\n            XCom.task_id == self.task_id,\n            XCom.execution_date == self.execution_date\n        ).delete()\n        session.commit()", "gt": "def clear_xcom_data(self, session=None):"}
{"prefix": "def key(self):\n        \"\"\"", "suffix": "        \"\"\"\n        return self.dag_id, self.task_id, self.execution_date, self.try_number", "gt": "        Returns a tuple that identifies the task instance uniquely"}
{"prefix": "def are_dependents_done(self, session=None):\n        \"\"\"\n        Checks whether the dependents of this task instance have all succeeded.\n        This is meant to be used by wait_for_downstream.\n\n        This is useful when you do not want to start processing the next\n        schedule of a task until the dependents are done. For instance,\n        if the task DROPs and recreates a table.\n        \"\"\"\n        task = self.task\n\n        if not task.downstream_task_ids:\n            return True\n\n        ti = session.query(func.count(TaskInstance.task_id)).filter(\n            TaskInstance.dag_id == self.dag_id,\n            TaskInstance.task_id.in_(task.downstream_task_ids),", "suffix": "            TaskInstance.state == State.SUCCESS,\n        )\n        count = ti[0][0]\n        return count == len(task.downstream_task_ids)", "gt": "            TaskInstance.execution_date == self.execution_date,"}
{"prefix": "def are_dependencies_met(\n            self,\n            dep_context=None,\n            session=None,\n            verbose=False):\n        \"\"\"\n        Returns whether or not all the conditions are met for this task instance to be run\n        given the context for the dependencies (e.g. a task instance being force run from\n        the UI will ignore some dependencies).\n\n        :param dep_context: The execution context that determines the dependencies that\n            should be evaluated.\n        :type dep_context: DepContext\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        :param verbose: whether log details on failed dependencies on\n            info or debug log level\n        :type verbose: bool\n        \"\"\"\n        dep_context = dep_context or DepContext()\n        failed = False\n        verbose_aware_logger = self.log.info if verbose else self.log.debug\n        for dep_status in self.get_failed_dep_statuses(\n                dep_context=dep_context,\n                session=session):\n            failed = True\n\n            verbose_aware_logger(\n                \"Dependencies not met for %s, dependency '%s' FAILED: %s\",\n                self, dep_status.dep_name, dep_status.reason\n            )\n\n        if failed:\n            return False", "suffix": "        verbose_aware_logger(\"Dependencies all met for %s\", self)\n        return True", "gt": ""}
{"prefix": "def next_retry_datetime(self):\n        \"\"\"\n        Get datetime of the next retry if the task instance fails. For exponential\n        backoff, retry_delay is used as base and will be converted to seconds.\n        \"\"\"\n        delay = self.task.retry_delay\n        if self.task.retry_exponential_backoff:\n            min_backoff = int(delay.total_seconds() * (2 ** (self.try_number - 2)))\n            # deterministic per task instance\n            hash = int(hashlib.sha1(\"{}#{}#{}#{}\".format(self.dag_id,\n                                                         self.task_id,\n                                                         self.execution_date,\n                                                         self.try_number)\n                                    .encode('utf-8')).hexdigest(), 16)\n            # between 0.5 * delay * (2^retry_number) and 1.0 * delay * (2^retry_number)\n            modded_hash = min_backoff + hash % min_backoff\n            # timedelta has a maximum representable value. The exponentiation\n            # here means this value can be exceeded after a certain number\n            # of tries (around 50 if the initial delay is 1s, even fewer if\n            # the delay is larger). Cap the value here before creating a\n            # timedelta object so the operation doesn't fail.\n            delay_backoff_in_seconds = min(", "suffix": "                timedelta.max.total_seconds() - 1\n            )\n            delay = timedelta(seconds=delay_backoff_in_seconds)\n            if self.task.max_retry_delay:\n                delay = min(self.task.max_retry_delay, delay)\n        return self.end_date + delay", "gt": "                modded_hash,"}
{"prefix": "def ready_for_retry(self):\n        \"\"\"\n        Checks on whether the task instance is in the right state and timeframe\n        to be retried.", "suffix": "        return (self.state == State.UP_FOR_RETRY and\n                self.next_retry_datetime() < timezone.utcnow())", "gt": "        \"\"\""}
{"prefix": "def pool_full(self, session):", "suffix": "        Returns a boolean as to whether the slot pool has room for this\n        task to run\n        \"\"\"\n        if not self.task.pool:\n            return False\n\n        pool = (\n            session\n            .query(Pool)\n            .filter(Pool.pool == self.task.pool)\n            .first()\n        )\n        if not pool:\n            return False\n        open_slots = pool.open_slots(session=session)\n\n        return open_slots <= 0", "gt": "        \"\"\""}
{"prefix": "def get_dagrun(self, session):\n        \"\"\"\n        Returns the DagRun for this TaskInstance\n\n        :param session:\n        :return: DagRun\n        \"\"\"\n        from airflow.models.dagrun import DagRun  # Avoid circular import\n        dr = session.query(DagRun).filter(\n            DagRun.dag_id == self.dag_id,\n            DagRun.execution_date == self.execution_date\n        ).first()", "suffix": "        return dr", "gt": ""}
{"prefix": "def _check_and_change_state_before_execution(\n            self,\n            verbose=True,\n            ignore_all_deps=False,\n            ignore_depends_on_past=False,\n            ignore_task_deps=False,\n            ignore_ti_state=False,\n            mark_success=False,\n            test_mode=False,\n            job_id=None,\n            pool=None,\n            session=None):\n        \"\"\"\n        Checks dependencies and then sets state to RUNNING if they are met. Returns\n        True if and only if state is set to RUNNING, which implies that task should be\n        executed, in preparation for _run_raw_task\n\n        :param verbose: whether to turn on more verbose logging\n        :type verbose: bool\n        :param ignore_all_deps: Ignore all of the non-critical dependencies, just runs\n        :type ignore_all_deps: bool\n        :param ignore_depends_on_past: Ignore depends_on_past DAG attribute\n        :type ignore_depends_on_past: bool\n        :param ignore_task_deps: Don't check the dependencies of this TI's task\n        :type ignore_task_deps: bool\n        :param ignore_ti_state: Disregards previous task instance state\n        :type ignore_ti_state: bool\n        :param mark_success: Don't run the task, mark its state as success\n        :type mark_success: bool\n        :param test_mode: Doesn't record success or failure in the DB\n        :type test_mode: bool\n        :param pool: specifies the pool to use to run the task instance\n        :type pool: str\n        :return: whether the state was changed to running or not\n        :rtype: bool\n        \"\"\"\n        task = self.task\n        self.pool = pool or task.pool\n        self.test_mode = test_mode\n        self.refresh_from_db(session=session, lock_for_update=True)\n        self.job_id = job_id\n        self.hostname = get_hostname()\n        self.operator = task.__class__.__name__\n\n        if not ignore_all_deps and not ignore_ti_state and self.state == State.SUCCESS:\n            Stats.incr('previously_succeeded', 1, 1)\n\n        queue_dep_context = DepContext(\n            deps=QUEUE_DEPS,\n            ignore_all_deps=ignore_all_deps,\n            ignore_ti_state=ignore_ti_state,\n            ignore_depends_on_past=ignore_depends_on_past,\n            ignore_task_deps=ignore_task_deps)\n        if not self.are_dependencies_met(\n                dep_context=queue_dep_context,\n                session=session,\n                verbose=True):\n            session.commit()\n            return False\n\n        # TODO: Logging needs cleanup, not clear what is being printed\n        hr = \"\\n\" + (\"-\" * 80)  # Line break\n\n        # For reporting purposes, we report based on 1-indexed,\n        # not 0-indexed lists (i.e. Attempt 1 instead of\n        # Attempt 0 for the first attempt).\n        # Set the task start date. In case it was re-scheduled use the initial\n        # start date that is recorded in task_reschedule table\n        self.start_date = timezone.utcnow()\n        task_reschedules = TaskReschedule.find_for_task_instance(self, session)\n        if task_reschedules:\n            self.start_date = task_reschedules[0].start_date\n\n        dep_context = DepContext(\n            deps=RUN_DEPS - QUEUE_DEPS,\n            ignore_all_deps=ignore_all_deps,\n            ignore_depends_on_past=ignore_depends_on_past,\n            ignore_task_deps=ignore_task_deps,\n            ignore_ti_state=ignore_ti_state)\n        runnable = self.are_dependencies_met(\n            dep_context=dep_context,\n            session=session,\n            verbose=True)\n\n        if not runnable and not mark_success:", "suffix": "            # have been running prematurely. This should be handled in the\n            # scheduling mechanism.\n            self.state = State.NONE\n            self.log.warning(hr)\n            self.log.warning(\n                \"FIXME: Rescheduling due to concurrency limits reached at task runtime. Attempt %s of \"\n                \"%s. State set to NONE.\", self.try_number, self.max_tries + 1\n            )\n            self.log.warning(hr)\n\n            self.queued_dttm = timezone.utcnow()\n            self.log.info(\"Queuing into pool %s\", self.pool)\n            session.merge(self)\n            session.commit()\n            return False\n\n        # Another worker might have started running this task instance while\n        # the current worker process was blocked on refresh_from_db\n        if self.state == State.RUNNING:\n            self.log.warning(\"Task Instance already running %s\", self)\n            session.commit()\n            return False\n\n        # print status message\n        self.log.info(hr)\n        self.log.info(\"Starting attempt %s of %s\", self.try_number, self.max_tries + 1)\n        self.log.info(hr)\n        self._try_number += 1\n\n        if not test_mode:\n            session.add(Log(State.RUNNING, self))\n        self.state = State.RUNNING\n        self.pid = os.getpid()\n        self.end_date = None\n        if not test_mode:\n            session.merge(self)\n        session.commit()\n\n        # Closing all pooled connections to prevent\n        # \"max number of connections reached\"\n        settings.engine.dispose()\n        if verbose:\n            if mark_success:\n                self.log.info(\"Marking success for %s on %s\", self.task, self.execution_date)\n            else:\n                self.log.info(\"Executing %s on %s\", self.task, self.execution_date)\n        return True", "gt": "            # FIXME: we might have hit concurrency limits, which means we probably"}
{"prefix": "def _run_raw_task(\n            self,\n            mark_success=False,\n            test_mode=False,\n            job_id=None,\n            pool=None,\n            session=None):\n        \"\"\"\n        Immediately runs the task (without checking or changing db state\n        before execution) and then sets the appropriate final state after\n        completion and runs any post-execute callbacks. Meant to be called\n        only after another function changes the state to running.\n\n        :param mark_success: Don't run the task, mark its state as success\n        :type mark_success: bool\n        :param test_mode: Doesn't record success or failure in the DB\n        :type test_mode: bool\n        :param pool: specifies the pool to use to run the task instance\n        :type pool: str\n        \"\"\"\n        task = self.task\n        self.pool = pool or task.pool\n        self.test_mode = test_mode\n        self.refresh_from_db(session=session)\n        self.job_id = job_id\n        self.hostname = get_hostname()\n        self.operator = task.__class__.__name__\n\n        context = {}\n        actual_start_date = timezone.utcnow()\n        try:\n            if not mark_success:\n                context = self.get_template_context()\n\n                task_copy = copy.copy(task)\n                self.task = task_copy\n\n                def signal_handler(signum, frame):\n                    self.log.error(\"Received SIGTERM. Terminating subprocesses.\")\n                    task_copy.on_kill()\n                    raise AirflowException(\"Task received SIGTERM signal\")\n                signal.signal(signal.SIGTERM, signal_handler)\n\n                # Don't clear Xcom until the task is certain to execute\n                self.clear_xcom_data()\n\n                start_time = time.time()\n\n                self.render_templates()\n                task_copy.pre_execute(context=context)\n\n                # If a timeout is specified for the task, make it fail\n                # if it goes beyond\n                result = None\n                if task_copy.execution_timeout:\n                    try:\n                        with timeout(int(\n                                task_copy.execution_timeout.total_seconds())):\n                            result = task_copy.execute(context=context)\n                    except AirflowTaskTimeout:\n                        task_copy.on_kill()\n                        raise\n                else:\n                    result = task_copy.execute(context=context)\n\n                # If the task returns a result, push an XCom containing it\n                if task_copy.do_xcom_push and result is not None:\n                    self.xcom_push(key=XCOM_RETURN_KEY, value=result)", "suffix": "                task_copy.post_execute(context=context, result=result)\n\n                end_time = time.time()\n                duration = end_time - start_time\n                Stats.timing(\n                    'dag.{dag_id}.{task_id}.duration'.format(\n                        dag_id=task_copy.dag_id,\n                        task_id=task_copy.task_id),\n                    duration)\n\n                Stats.incr('operator_successes_{}'.format(\n                    self.task.__class__.__name__), 1, 1)\n                Stats.incr('ti_successes')\n            self.refresh_from_db(lock_for_update=True)\n            self.state = State.SUCCESS\n        except AirflowSkipException:\n            self.refresh_from_db(lock_for_update=True)\n            self.state = State.SKIPPED\n        except AirflowRescheduleException as reschedule_exception:\n            self.refresh_from_db()\n            self._handle_reschedule(actual_start_date, reschedule_exception, test_mode, context)\n            return\n        except AirflowException as e:\n            self.refresh_from_db()\n            # for case when task is marked as success/failed externally\n            # current behavior doesn't hit the success callback\n            if self.state in {State.SUCCESS, State.FAILED}:\n                return\n            else:\n                self.handle_failure(e, test_mode, context)\n                raise\n        except (Exception, KeyboardInterrupt) as e:\n            self.handle_failure(e, test_mode, context)\n            raise\n\n        # Success callback\n        try:\n            if task.on_success_callback:\n                task.on_success_callback(context)\n        except Exception as e3:\n            self.log.error(\"Failed when executing success callback\")\n            self.log.exception(e3)\n\n        # Recording SUCCESS\n        self.end_date = timezone.utcnow()\n        self.set_duration()\n        if not test_mode:\n            session.add(Log(self.state, self))\n            session.merge(self)\n        session.commit()", "gt": ""}
{"prefix": "def xcom_push(\n            self,\n            key,\n            value,\n            execution_date=None):\n        \"\"\"\n        Make an XCom available for tasks to pull.\n\n        :param key: A key for the XCom\n        :type key: str", "suffix": "            in the database.\n        :type value: any pickleable object\n        :param execution_date: if provided, the XCom will not be visible until\n            this date. This can be used, for example, to send a message to a\n            task on a future date without it being immediately visible.\n        :type execution_date: datetime\n        \"\"\"\n\n        if execution_date and execution_date < self.execution_date:\n            raise ValueError(\n                'execution_date can not be in the past (current '\n                'execution_date is {}; received {})'.format(\n                    self.execution_date, execution_date))\n\n        XCom.set(\n            key=key,\n            value=value,\n            task_id=self.task_id,\n            dag_id=self.dag_id,\n            execution_date=execution_date or self.execution_date)", "gt": "        :param value: A value for the XCom. The value is pickled and stored"}
{"prefix": "def xcom_pull(\n            self,\n            task_ids=None,\n            dag_id=None,\n            key=XCOM_RETURN_KEY,\n            include_prior_dates=False):\n        \"\"\"\n        Pull XComs that optionally meet certain criteria.\n\n        The default value for `key` limits the search to XComs\n        that were returned by other tasks (as opposed to those that were pushed\n        manually). To remove this filter, pass key=None (or any desired value).\n\n        If a single task_id string is provided, the result is the value of the\n        most recent matching XCom from that task_id. If multiple task_ids are\n        provided, a tuple of matching values is returned. None is returned\n        whenever no matches are found.\n\n        :param key: A key for the XCom. If provided, only XComs with matching\n            keys will be returned. The default key is 'return_value', also\n            available as a constant XCOM_RETURN_KEY. This key is automatically\n            given to XComs returned by tasks (as opposed to being pushed\n            manually). To remove the filter, pass key=None.", "suffix": "        :param task_ids: Only XComs from tasks with matching ids will be\n            pulled. Can pass None to remove the filter.\n        :type task_ids: str or iterable of strings (representing task_ids)\n        :param dag_id: If provided, only pulls XComs from this DAG.\n            If None (default), the DAG of the calling task is used.\n        :type dag_id: str\n        :param include_prior_dates: If False, only XComs from the current\n            execution_date are returned. If True, XComs from previous dates\n            are returned as well.\n        :type include_prior_dates: bool\n        \"\"\"\n\n        if dag_id is None:\n            dag_id = self.dag_id\n\n        pull_fn = functools.partial(\n            XCom.get_one,\n            execution_date=self.execution_date,\n            key=key,\n            dag_id=dag_id,\n            include_prior_dates=include_prior_dates)\n\n        if is_container(task_ids):\n            return tuple(pull_fn(task_id=t) for t in task_ids)\n        else:\n            return pull_fn(task_id=task_ids)", "gt": "        :type key: str"}
{"prefix": "", "suffix": "        \"\"\"\n        Sets the log context.\n        \"\"\"\n        self.raw = raw\n        self._set_context(self)", "gt": "def init_run_context(self, raw=False):"}
{"prefix": "def close(self):\n        \"\"\"\n        Close and upload local log file to remote storage Wasb.\n        \"\"\"\n        # When application exit, system shuts down all handlers by\n        # calling close method. Here we check if logger is already\n        # closed to prevent uploading the log to remote storage multiple\n        # times when `logging.shutdown` is called.\n        if self.closed:\n            return\n\n        super().close()\n\n        if not self.upload_on_close:\n            return\n\n        local_loc = os.path.join(self.local_base, self.log_relative_path)\n        remote_loc = os.path.join(self.remote_base, self.log_relative_path)\n        if os.path.exists(local_loc):\n            # read log and remove old logs to get just the latest additions\n            with open(local_loc, 'r') as logfile:", "suffix": "            self.wasb_write(log, remote_loc, append=True)\n\n            if self.delete_local_copy:\n                shutil.rmtree(os.path.dirname(local_loc))\n        # Mark closed so we don't double write if close is called twice\n        self.closed = True", "gt": "                log = logfile.read()"}
{"prefix": "def _read(self, ti, try_number, metadata=None):", "suffix": "        Read logs of given task instance and try_number from Wasb remote storage.\n        If failed, read the log from task instance host machine.\n        :param ti: task instance object\n        :param try_number: task instance try_number to read logs from\n        :param metadata: log metadata,\n                         can be used for steaming log reading and auto-tailing.\n        \"\"\"\n        # Explicitly getting log relative path is necessary as the given\n        # task instance might be different than task instance passed in\n        # in set_context method.\n        log_relative_path = self._render_filename(ti, try_number)\n        remote_loc = os.path.join(self.remote_base, log_relative_path)\n\n        if self.wasb_log_exists(remote_loc):\n            # If Wasb remote file exists, we do not fetch logs from task instance\n            # local machine even if there are errors reading remote logs, as\n            # returned remote_log will contain error messages.\n            remote_log = self.wasb_read(remote_loc, return_error=True)\n            log = '*** Reading remote log from {}.\\n{}\\n'.format(\n                remote_loc, remote_log)\n            return log, {'end_of_log': True}\n        else:\n            return super()._read(ti, try_number)", "gt": "        \"\"\""}
{"prefix": "def wasb_log_exists(self, remote_log_location):\n        \"\"\"\n        Check if remote_log_location exists in remote storage\n        :param remote_log_location: log's location in remote storage\n        :return: True if location exists else False\n        \"\"\"", "suffix": "            return self.hook.check_for_blob(self.wasb_container, remote_log_location)\n        except Exception:\n            pass\n        return False", "gt": "        try:"}
{"prefix": "def wasb_read(self, remote_log_location, return_error=False):\n        \"\"\"\n        Returns the log found at the remote_log_location. Returns '' if no\n        logs are found or there is an error.\n        :param remote_log_location: the log's location in remote storage\n        :type remote_log_location: str (path)", "suffix": "            error occurs. Otherwise returns '' when an error occurs.\n        :type return_error: bool\n        \"\"\"\n        try:\n            return self.hook.read_file(self.wasb_container, remote_log_location)\n        except AzureHttpError:\n            msg = 'Could not read logs from {}'.format(remote_log_location)\n            self.log.exception(msg)\n            # return error if needed\n            if return_error:\n                return msg", "gt": "        :param return_error: if True, returns a string error message if an"}
{"prefix": "def wasb_write(self, log, remote_log_location, append=True):\n        \"\"\"\n        Writes the log to the remote_log_location. Fails silently if no hook\n        was created.\n        :param log: the log to write to the remote_log_location\n        :type log: str\n        :param remote_log_location: the log's location in remote storage\n        :type remote_log_location: str (path)\n        :param append: if False, any existing log file is overwritten. If True,\n            the new log is appended to any existing logs.\n        :type append: bool", "suffix": "        if append and self.wasb_log_exists(remote_log_location):\n            old_log = self.wasb_read(remote_log_location)\n            log = '\\n'.join([old_log, log]) if old_log else log\n\n        try:\n            self.hook.load_string(\n                log,\n                self.wasb_container,\n                remote_log_location,\n            )\n        except AzureHttpError:\n            self.log.exception('Could not write logs to %s',\n                               remote_log_location)", "gt": "        \"\"\""}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Retrieves connection to Google Compute Engine.\n\n        :return: Google Compute Engine services object\n        :rtype: dict\n        \"\"\"", "suffix": "            http_authorized = self._authorize()\n            self._conn = build('compute', self.api_version,\n                               http=http_authorized, cache_discovery=False)\n        return self._conn", "gt": "        if not self._conn:"}
{"prefix": "def start_instance(self, zone, resource_id, project_id=None):\n        \"\"\"\n        Starts an existing instance defined by project_id, zone and resource_id.\n        Must be called with keyword arguments rather than positional.\n\n        :param zone: Google Cloud Platform zone where the instance exists\n        :type zone: str\n        :param resource_id: Name of the Compute Engine instance resource\n        :type resource_id: str", "suffix": "            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().start(\n            project=project_id,\n            zone=zone,\n            instance=resource_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name,\n                                             zone=zone)", "gt": "        :param project_id: Optional, Google Cloud Platform project ID where the"}
{"prefix": "def set_machine_type(self, zone, resource_id, body, project_id=None):\n        \"\"\"\n        Sets machine type of an instance defined by project_id, zone and resource_id.\n        Must be called with keyword arguments rather than positional.\n\n        :param zone: Google Cloud Platform zone where the instance exists.\n        :type zone: str\n        :param resource_id: Name of the Compute Engine instance resource\n        :type resource_id: str\n        :param body: Body required by the Compute Engine setMachineType API,\n            as described in\n            https://cloud.google.com/compute/docs/reference/rest/v1/instances/setMachineType\n        :type body: dict\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self._execute_set_machine_type(zone, resource_id, body, project_id)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))", "suffix": "                                             operation_name=operation_name,\n                                             zone=zone)", "gt": "        self._wait_for_operation_to_complete(project_id=project_id,"}
{"prefix": "def get_instance_template(self, resource_id, project_id=None):\n        \"\"\"\n        Retrieves instance template by project_id and resource_id.\n        Must be called with keyword arguments rather than positional.\n\n        :param resource_id: Name of the instance template\n        :type resource_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: Instance template representation as object according to\n            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates\n        :rtype: dict\n        \"\"\"", "suffix": "            project=project_id,\n            instanceTemplate=resource_id\n        ).execute(num_retries=self.num_retries)\n        return response", "gt": "        response = self.get_conn().instanceTemplates().get("}
{"prefix": "def insert_instance_template(self, body, request_id=None, project_id=None):\n        \"\"\"\n        Inserts instance template using body specified\n        Must be called with keyword arguments rather than positional.\n\n        :param body: Instance template representation as object according to\n            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates\n        :type body: dict\n        :param request_id: Optional, unique request_id that you might add to achieve\n            full idempotence (for example when client call times out repeating the request\n            with the same request id will not create a new instance template again)\n            It should be in UUID format as defined in RFC 4122\n        :type request_id: str", "suffix": "            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instanceTemplates().insert(\n            project=project_id,\n            body=body,\n            requestId=request_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)", "gt": "        :param project_id: Optional, Google Cloud Platform project ID where the"}
{"prefix": "def get_instance_group_manager(self, zone, resource_id, project_id=None):", "suffix": "        Retrieves Instance Group Manager by project_id, zone and resource_id.\n        Must be called with keyword arguments rather than positional.\n\n        :param zone: Google Cloud Platform zone where the Instance Group Manager exists\n        :type zone: str\n        :param resource_id: Name of the Instance Group Manager\n        :type resource_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: Instance group manager representation as object according to\n            https://cloud.google.com/compute/docs/reference/rest/beta/instanceGroupManagers\n        :rtype: dict\n        \"\"\"\n        response = self.get_conn().instanceGroupManagers().get(\n            project=project_id,\n            zone=zone,\n            instanceGroupManager=resource_id\n        ).execute(num_retries=self.num_retries)\n        return response", "gt": "        \"\"\""}
{"prefix": "", "suffix": "                                     body, request_id=None, project_id=None):\n        \"\"\"\n        Patches Instance Group Manager with the specified body.\n        Must be called with keyword arguments rather than positional.\n\n        :param zone: Google Cloud Platform zone where the Instance Group Manager exists\n        :type zone: str\n        :param resource_id: Name of the Instance Group Manager\n        :type resource_id: str\n        :param body: Instance Group Manager representation as json-merge-patch object\n            according to\n            https://cloud.google.com/compute/docs/reference/rest/beta/instanceTemplates/patch\n        :type body: dict\n        :param request_id: Optional, unique request_id that you might add to achieve\n            full idempotence (for example when client call times out repeating the request\n            with the same request id will not create a new instance template again).\n            It should be in UUID format as defined in RFC 4122\n        :type request_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instanceGroupManagers().patch(\n            project=project_id,\n            zone=zone,\n            instanceGroupManager=resource_id,\n            body=body,\n            requestId=request_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name,\n                                             zone=zone)", "gt": "def patch_instance_group_manager(self, zone, resource_id,"}
{"prefix": "def _wait_for_operation_to_complete(self, project_id, operation_name, zone=None):\n        \"\"\"", "suffix": "\n        :param operation_name: name of the operation\n        :type operation_name: str\n        :param zone: optional region of the request (might be None for global operations)\n        :type zone: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            if zone is None:\n                # noinspection PyTypeChecker\n                operation_response = self._check_global_operation_status(\n                    service, operation_name, project_id)\n            else:\n                # noinspection PyTypeChecker\n                operation_response = self._check_zone_operation_status(\n                    service, operation_name, project_id, zone, self.num_retries)\n            if operation_response.get(\"status\") == GceOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    code = operation_response.get(\"httpErrorStatusCode\")\n                    msg = operation_response.get(\"httpErrorMessage\")\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(\"{} {}: \".format(code, msg) + error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)", "gt": "        Waits for the named operation to complete - checks status of the async call."}
{"prefix": "def check_for_bucket(self, bucket_name):\n        \"\"\"\n        Check if bucket_name exists.\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        \"\"\"\n        try:", "suffix": "            return True\n        except ClientError as e:\n            self.log.info(e.response[\"Error\"][\"Message\"])\n            return False", "gt": "            self.get_conn().head_bucket(Bucket=bucket_name)"}
{"prefix": "def create_bucket(self, bucket_name, region_name=None):\n        \"\"\"\n        Creates an Amazon S3 bucket.\n\n        :param bucket_name: The name of the bucket\n        :type bucket_name: str", "suffix": "        :type region_name: str\n        \"\"\"\n        s3_conn = self.get_conn()\n        if not region_name:\n            region_name = s3_conn.meta.region_name\n        if region_name == 'us-east-1':\n            self.get_conn().create_bucket(Bucket=bucket_name)\n        else:\n            self.get_conn().create_bucket(Bucket=bucket_name,\n                                          CreateBucketConfiguration={\n                                              'LocationConstraint': region_name\n                                          })", "gt": "        :param region_name: The name of the aws region in which to create the bucket."}
{"prefix": "def check_for_prefix(self, bucket_name, prefix, delimiter):\n        \"\"\"\n        Checks that a prefix exists in a bucket\n\n        :param bucket_name: the name of the bucket", "suffix": "        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        \"\"\"\n        prefix = prefix + delimiter if prefix[-1] != delimiter else prefix\n        prefix_split = re.split(r'(\\w+[{d}])$'.format(d=delimiter), prefix, 1)\n        previous_level = prefix_split[0]\n        plist = self.list_prefixes(bucket_name, previous_level, delimiter)\n        return False if plist is None else prefix in plist", "gt": "        :type bucket_name: str"}
{"prefix": "def list_prefixes(self, bucket_name, prefix='', delimiter='',\n                      page_size=None, max_items=None):\n        \"\"\"\n        Lists prefixes in a bucket under prefix\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int\n        \"\"\"\n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('list_objects_v2')\n        response = paginator.paginate(Bucket=bucket_name,\n                                      Prefix=prefix,\n                                      Delimiter=delimiter,\n                                      PaginationConfig=config)\n\n        has_results = False", "suffix": "        for page in response:\n            if 'CommonPrefixes' in page:\n                has_results = True\n                for p in page['CommonPrefixes']:\n                    prefixes.append(p['Prefix'])\n\n        if has_results:\n            return prefixes", "gt": "        prefixes = []"}
{"prefix": "def list_keys(self, bucket_name, prefix='', delimiter='',\n                  page_size=None, max_items=None):\n        \"\"\"\n        Lists keys in a bucket under prefix and not containing delimiter\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str", "suffix": "        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int\n        \"\"\"\n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('list_objects_v2')\n        response = paginator.paginate(Bucket=bucket_name,\n                                      Prefix=prefix,\n                                      Delimiter=delimiter,\n                                      PaginationConfig=config)\n\n        has_results = False\n        keys = []\n        for page in response:\n            if 'Contents' in page:\n                has_results = True\n                for k in page['Contents']:\n                    keys.append(k['Key'])\n\n        if has_results:\n            return keys", "gt": "        :param page_size: pagination size"}
{"prefix": "def check_for_key(self, key, bucket_name=None):\n        \"\"\"\n        Checks if a key exists in a bucket\n\n        :param key: S3 key that will point to the file\n        :type key: str", "suffix": "        :type bucket_name: str\n        \"\"\"\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        try:\n            self.get_conn().head_object(Bucket=bucket_name, Key=key)\n            return True\n        except ClientError as e:\n            self.log.info(e.response[\"Error\"][\"Message\"])\n            return False", "gt": "        :param bucket_name: Name of the bucket in which the file is stored"}
{"prefix": "def get_key(self, key, bucket_name=None):\n        \"\"\"\n        Returns a boto3.s3.Object", "suffix": "        :param key: the path to the key\n        :type key: str\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        \"\"\"\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        obj = self.get_resource_type('s3').Object(bucket_name, key)\n        obj.load()\n        return obj", "gt": ""}
{"prefix": "def read_key(self, key, bucket_name=None):\n        \"\"\"", "suffix": "\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which the file is stored\n        :type bucket_name: str\n        \"\"\"\n\n        obj = self.get_key(key, bucket_name)\n        return obj.get()['Body'].read().decode('utf-8')", "gt": "        Reads a key from S3"}
{"prefix": "def select_key(self, key, bucket_name=None,\n                   expression='SELECT * FROM S3Object',\n                   expression_type='SQL',\n                   input_serialization=None,", "suffix": "        \"\"\"\n        Reads a key with S3 Select.\n\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which the file is stored\n        :type bucket_name: str\n        :param expression: S3 Select expression\n        :type expression: str\n        :param expression_type: S3 Select expression type\n        :type expression_type: str\n        :param input_serialization: S3 Select input data serialization format\n        :type input_serialization: dict\n        :param output_serialization: S3 Select output data serialization format\n        :type output_serialization: dict\n        :return: retrieved subset of original data by S3 Select\n        :rtype: str\n\n        .. seealso::\n            For more details about S3 Select parameters:\n            http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.select_object_content\n        \"\"\"\n        if input_serialization is None:\n            input_serialization = {'CSV': {}}\n        if output_serialization is None:\n            output_serialization = {'CSV': {}}\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        response = self.get_conn().select_object_content(\n            Bucket=bucket_name,\n            Key=key,\n            Expression=expression,\n            ExpressionType=expression_type,\n            InputSerialization=input_serialization,\n            OutputSerialization=output_serialization)\n\n        return ''.join(event['Records']['Payload'].decode('utf-8')\n                       for event in response['Payload']\n                       if 'Records' in event)", "gt": "                   output_serialization=None):"}
{"prefix": "def check_for_wildcard_key(self,\n                               wildcard_key, bucket_name=None, delimiter=''):\n        \"\"\"\n        Checks that a key matching a wildcard expression exists in a bucket\n\n        :param wildcard_key: the path to the key\n        :type wildcard_key: str", "suffix": "        :type bucket_name: str\n        :param delimiter: the delimiter marks key hierarchy\n        :type delimiter: str\n        \"\"\"\n        return self.get_wildcard_key(wildcard_key=wildcard_key,\n                                     bucket_name=bucket_name,\n                                     delimiter=delimiter) is not None", "gt": "        :param bucket_name: the name of the bucket"}
{"prefix": "def get_wildcard_key(self, wildcard_key, bucket_name=None, delimiter=''):\n        \"\"\"\n        Returns a boto3.s3.Object object matching the wildcard expression\n\n        :param wildcard_key: the path to the key\n        :type wildcard_key: str\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param delimiter: the delimiter marks key hierarchy\n        :type delimiter: str\n        \"\"\"\n        if not bucket_name:\n            (bucket_name, wildcard_key) = self.parse_s3_url(wildcard_key)\n\n        prefix = re.split(r'[*]', wildcard_key, 1)[0]\n        klist = self.list_keys(bucket_name, prefix=prefix, delimiter=delimiter)\n        if klist:", "suffix": "            if key_matches:\n                return self.get_key(key_matches[0], bucket_name)", "gt": "            key_matches = [k for k in klist if fnmatch.fnmatch(k, wildcard_key)]"}
{"prefix": "def load_file(self,\n                  filename,\n                  key,\n                  bucket_name=None,\n                  replace=False,\n                  encrypt=False):\n        \"\"\"\n        Loads a local file to S3\n\n        :param filename: name of the file to load.\n        :type filename: str\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which to store the file\n        :type bucket_name: str\n        :param replace: A flag to decide whether or not to overwrite the key\n            if it already exists. If replace is False and the key exists, an\n            error will be raised.\n        :type replace: bool\n        :param encrypt: If True, the file will be encrypted on the server-side\n            by S3 and will be stored in an encrypted form while at rest in S3.\n        :type encrypt: bool\n        \"\"\"\n        if not bucket_name:", "suffix": "\n        if not replace and self.check_for_key(key, bucket_name):\n            raise ValueError(\"The key {key} already exists.\".format(key=key))\n\n        extra_args = {}\n        if encrypt:\n            extra_args['ServerSideEncryption'] = \"AES256\"\n\n        client = self.get_conn()\n        client.upload_file(filename, bucket_name, key, ExtraArgs=extra_args)", "gt": "            (bucket_name, key) = self.parse_s3_url(key)"}
{"prefix": "def load_string(self,\n                    string_data,\n                    key,\n                    bucket_name=None,\n                    replace=False,\n                    encrypt=False,\n                    encoding='utf-8'):\n        \"\"\"\n        Loads a string to S3\n\n        This is provided as a convenience to drop a string in S3. It uses the", "suffix": "\n        :param string_data: str to set as content for the key.\n        :type string_data: str\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which to store the file\n        :type bucket_name: str\n        :param replace: A flag to decide whether or not to overwrite the key\n            if it already exists\n        :type replace: bool\n        :param encrypt: If True, the file will be encrypted on the server-side\n            by S3 and will be stored in an encrypted form while at rest in S3.\n        :type encrypt: bool\n        \"\"\"\n        self.load_bytes(string_data.encode(encoding),\n                        key=key,\n                        bucket_name=bucket_name,\n                        replace=replace,\n                        encrypt=encrypt)", "gt": "        boto infrastructure to ship a file to s3."}
{"prefix": "def load_bytes(self,\n                   bytes_data,\n                   key,\n                   bucket_name=None,\n                   replace=False,\n                   encrypt=False):\n        \"\"\"\n        Loads bytes to S3\n\n        This is provided as a convenience to drop a string in S3. It uses the\n        boto infrastructure to ship a file to s3.\n\n        :param bytes_data: bytes to set as content for the key.\n        :type bytes_data: bytes\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which to store the file\n        :type bucket_name: str\n        :param replace: A flag to decide whether or not to overwrite the key\n            if it already exists\n        :type replace: bool\n        :param encrypt: If True, the file will be encrypted on the server-side\n            by S3 and will be stored in an encrypted form while at rest in S3.\n        :type encrypt: bool", "suffix": "        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        if not replace and self.check_for_key(key, bucket_name):\n            raise ValueError(\"The key {key} already exists.\".format(key=key))\n\n        extra_args = {}\n        if encrypt:\n            extra_args['ServerSideEncryption'] = \"AES256\"\n\n        filelike_buffer = BytesIO(bytes_data)\n\n        client = self.get_conn()\n        client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)", "gt": "        \"\"\""}
{"prefix": "def load_file_obj(self,\n                      file_obj,\n                      key,\n                      bucket_name=None,\n                      replace=False,\n                      encrypt=False):\n        \"\"\"\n        Loads a file object to S3\n\n        :param file_obj: The file-like object to set as the content for the S3 key.\n        :type file_obj: file-like object\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which to store the file\n        :type bucket_name: str\n        :param replace: A flag that indicates whether to overwrite the key\n            if it already exists.\n        :type replace: bool\n        :param encrypt: If True, S3 encrypts the file on the server,\n            and the file is stored in encrypted form at rest in S3.\n        :type encrypt: bool\n        \"\"\"\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)", "suffix": "        if not replace and self.check_for_key(key, bucket_name):\n            raise ValueError(\"The key {key} already exists.\".format(key=key))\n\n        extra_args = {}\n        if encrypt:\n            extra_args['ServerSideEncryption'] = \"AES256\"\n\n        client = self.get_conn()\n        client.upload_fileobj(file_obj, bucket_name, key, ExtraArgs=extra_args)", "gt": ""}
{"prefix": "def copy_object(self,\n                    source_bucket_key,\n                    dest_bucket_key,\n                    source_bucket_name=None,\n                    dest_bucket_name=None,\n                    source_version_id=None):\n        \"\"\"\n        Creates a copy of an object that is already stored in S3.\n\n        Note: the S3 connection used here needs to have access to both\n        source and destination bucket/key.\n\n        :param source_bucket_key: The key of the source object.\n\n            It can be either full s3:// style url or relative path from root level.\n\n            When it's specified as a full s3:// url, please omit source_bucket_name.\n        :type source_bucket_key: str\n        :param dest_bucket_key: The key of the object to copy to.\n\n            The convention to specify `dest_bucket_key` is the same\n            as `source_bucket_key`.\n        :type dest_bucket_key: str\n        :param source_bucket_name: Name of the S3 bucket where the source object is in.\n\n            It should be omitted when `source_bucket_key` is provided as a full s3:// url.\n        :type source_bucket_name: str\n        :param dest_bucket_name: Name of the S3 bucket to where the object is copied.\n\n            It should be omitted when `dest_bucket_key` is provided as a full s3:// url.\n        :type dest_bucket_name: str\n        :param source_version_id: Version ID of the source object (OPTIONAL)\n        :type source_version_id: str\n        \"\"\"\n\n        if dest_bucket_name is None:\n            dest_bucket_name, dest_bucket_key = self.parse_s3_url(dest_bucket_key)\n        else:\n            parsed_url = urlparse(dest_bucket_key)\n            if parsed_url.scheme != '' or parsed_url.netloc != '':\n                raise AirflowException('If dest_bucket_name is provided, ' +", "suffix": "                                       'from root level, rather than a full s3:// url')\n\n        if source_bucket_name is None:\n            source_bucket_name, source_bucket_key = self.parse_s3_url(source_bucket_key)\n        else:\n            parsed_url = urlparse(source_bucket_key)\n            if parsed_url.scheme != '' or parsed_url.netloc != '':\n                raise AirflowException('If source_bucket_name is provided, ' +\n                                       'source_bucket_key should be relative path ' +\n                                       'from root level, rather than a full s3:// url')\n\n        CopySource = {'Bucket': source_bucket_name,\n                      'Key': source_bucket_key,\n                      'VersionId': source_version_id}\n        response = self.get_conn().copy_object(Bucket=dest_bucket_name,\n                                               Key=dest_bucket_key,\n                                               CopySource=CopySource)\n        return response", "gt": "                                       'dest_bucket_key should be relative path ' +"}
{"prefix": "def delete_objects(self,\n                       bucket,\n                       keys):\n        \"\"\"\n        :param bucket: Name of the bucket in which you are going to delete object(s)\n        :type bucket: str\n        :param keys: The key(s) to delete from S3 bucket.\n\n            When ``keys`` is a string, it's supposed to be the key name of\n            the single object to delete.\n\n            When ``keys`` is a list, it's supposed to be the list of the\n            keys to delete.\n        :type keys: str or list\n        \"\"\"\n        if isinstance(keys, list):", "suffix": "        else:\n            keys = [keys]\n\n        delete_dict = {\"Objects\": [{\"Key\": k} for k in keys]}\n        response = self.get_conn().delete_objects(Bucket=bucket,\n                                                  Delete=delete_dict)\n        return response", "gt": "            keys = keys"}
{"prefix": "def _query_cassandra(self):\n        \"\"\"\n        Queries cassandra and returns a cursor to the results.", "suffix": "        self.hook = CassandraHook(cassandra_conn_id=self.cassandra_conn_id)\n        session = self.hook.get_conn()\n        cursor = session.execute(self.cql)\n        return cursor", "gt": "        \"\"\""}
{"prefix": "def _write_local_data_files(self, cursor):\n        \"\"\"\n        Takes a cursor, and writes results to a local file.\n\n        :return: A dictionary where keys are filenames to be used as object\n            names in GCS, and values are file handles to local files that\n            contain the data for the GCS objects.\n        \"\"\"\n        file_no = 0\n        tmp_file_handle = NamedTemporaryFile(delete=True)\n        tmp_file_handles = {self.filename.format(file_no): tmp_file_handle}\n        for row in cursor:\n            row_dict = self.generate_data_dict(row._fields, row)", "suffix": "            tmp_file_handle.write(s)\n\n            # Append newline to make dumps BigQuery compatible.\n            tmp_file_handle.write(b'\\n')\n\n            if tmp_file_handle.tell() >= self.approx_max_file_size_bytes:\n                file_no += 1\n                tmp_file_handle = NamedTemporaryFile(delete=True)\n                tmp_file_handles[self.filename.format(file_no)] = tmp_file_handle\n\n        return tmp_file_handles", "gt": "            s = json.dumps(row_dict).encode('utf-8')"}
{"prefix": "def _write_local_schema_file(self, cursor):\n        \"\"\"\n        Takes a cursor, and writes the BigQuery schema for the results to a\n        local file system.\n\n        :return: A dictionary where key is a filename to be used as an object\n            name in GCS, and values are file handles to local files that\n            contains the BigQuery schema fields in .json format.\n        \"\"\"\n        schema = []\n        tmp_schema_file_handle = NamedTemporaryFile(delete=True)\n\n        for name, type in zip(cursor.column_names, cursor.column_types):\n            schema.append(self.generate_schema_dict(name, type))\n        json_serialized_schema = json.dumps(schema).encode('utf-8')\n", "suffix": "        return {self.schema_filename: tmp_schema_file_handle}", "gt": "        tmp_schema_file_handle.write(json_serialized_schema)"}
{"prefix": "def convert_user_type(cls, name, value):\n        \"\"\"\n        Converts a user type to RECORD that contains n fields, where n is the\n        number of attributes. Each element in the user type class will be converted to its\n        corresponding data type in BQ.\n        \"\"\"\n        names = value._fields\n        values = [cls.convert_value(name, getattr(value, name)) for name in names]", "suffix": "", "gt": "        return cls.generate_data_dict(names, values)"}
{"prefix": "def convert_tuple_type(cls, name, value):\n        \"\"\"\n        Converts a tuple to RECORD that contains n fields, each will be converted\n        to its corresponding data type in bq and will be named 'field_<index>', where\n        index is determined by the order of the tuple elements defined in cassandra.\n        \"\"\"\n        names = ['field_' + str(i) for i in range(len(value))]", "suffix": "        return cls.generate_data_dict(names, values)", "gt": "        values = [cls.convert_value(name, value) for name, value in zip(names, value)]"}
{"prefix": "def convert_map_type(cls, name, value):\n        \"\"\"\n        Converts a map to a repeated RECORD that contains two fields: 'key' and 'value',\n        each will be converted to its corresponding data type in BQ.", "suffix": "        converted_map = []\n        for k, v in zip(value.keys(), value.values()):\n            converted_map.append({\n                'key': cls.convert_value('key', k),\n                'value': cls.convert_value('value', v)\n            })\n        return converted_map", "gt": "        \"\"\""}
{"prefix": "def send_email(to, subject, html_content, files=None, dryrun=False, cc=None,\n               bcc=None, mime_subtype='mixed', sandbox_mode=False, **kwargs):\n    \"\"\"\n    Send an email with html content using sendgrid.\n\n    To use this plugin:\n    0. include sendgrid subpackage as part of your Airflow installation, e.g.,\n    pip install 'apache-airflow[sendgrid]'\n    1. update [email] backend in airflow.cfg, i.e.,\n    [email]\n    email_backend = airflow.contrib.utils.sendgrid.send_email\n    2. configure Sendgrid specific environment variables at all Airflow instances:\n    SENDGRID_MAIL_FROM={your-mail-from}\n    SENDGRID_API_KEY={your-sendgrid-api-key}.\n    \"\"\"\n    if files is None:\n        files = []\n\n    mail = Mail()\n    from_email = kwargs.get('from_email') or os.environ.get('SENDGRID_MAIL_FROM')\n    from_name = kwargs.get('from_name') or os.environ.get('SENDGRID_MAIL_SENDER')\n    mail.from_email = Email(from_email, from_name)\n    mail.subject = subject\n    mail.mail_settings = MailSettings()\n\n    if sandbox_mode:\n        mail.mail_settings.sandbox_mode = SandBoxMode(enable=True)\n\n    # Add the recipient list of to emails.\n    personalization = Personalization()", "suffix": "    for to_address in to:\n        personalization.add_to(Email(to_address))\n    if cc:\n        cc = get_email_address_list(cc)\n        for cc_address in cc:\n            personalization.add_cc(Email(cc_address))\n    if bcc:\n        bcc = get_email_address_list(bcc)\n        for bcc_address in bcc:\n            personalization.add_bcc(Email(bcc_address))\n\n    # Add custom_args to personalization if present\n    pers_custom_args = kwargs.get('personalization_custom_args', None)\n    if isinstance(pers_custom_args, dict):\n        for key in pers_custom_args.keys():\n            personalization.add_custom_arg(CustomArg(key, pers_custom_args[key]))\n\n    mail.add_personalization(personalization)\n    mail.add_content(Content('text/html', html_content))\n\n    categories = kwargs.get('categories', [])\n    for cat in categories:\n        mail.add_category(Category(cat))\n\n    # Add email attachment.\n    for fname in files:\n        basename = os.path.basename(fname)\n\n        attachment = Attachment()\n        attachment.type = mimetypes.guess_type(basename)[0]\n        attachment.filename = basename\n        attachment.disposition = \"attachment\"\n        attachment.content_id = '<{0}>'.format(basename)\n\n        with open(fname, \"rb\") as f:\n            attachment.content = base64.b64encode(f.read()).decode('utf-8')\n\n        mail.add_attachment(attachment)\n    _post_sendgrid_mail(mail.get())", "gt": "    to = get_email_address_list(to)"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Speech.\n\n        :return: Google Cloud Speech client object.\n        :rtype: google.cloud.speech_v1.SpeechClient", "suffix": "        if not self._client:\n            self._client = SpeechClient(credentials=self._get_credentials())\n        return self._client", "gt": "        \"\"\""}
{"prefix": "def recognize_speech(self, config, audio, retry=None, timeout=None):\n        \"\"\"\n        Recognizes audio input\n\n        :param config: information to the recognizer that specifies how to process the request.\n            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionConfig\n        :type config: dict or google.cloud.speech_v1.types.RecognitionConfig\n        :param audio: audio data to be recognized\n            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionAudio\n        :type audio: dict or google.cloud.speech_v1.types.RecognitionAudio\n        :param retry: (Optional) A retry object used to retry requests. If None is specified,\n            requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.\n            Note that if retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        \"\"\"", "suffix": "        response = client.recognize(config=config, audio=audio, retry=retry, timeout=timeout)\n        self.log.info(\"Recognised speech: %s\" % response)\n        return response", "gt": "        client = self.get_conn()"}
{"prefix": "def execute(self, context):\n        \"\"\"\n        Call the SparkSqlHook to run the provided sql query\n        \"\"\"\n        self._hook = SparkSqlHook(sql=self._sql,\n                                  conf=self._conf,\n                                  conn_id=self._conn_id,\n                                  total_executor_cores=self._total_executor_cores,\n                                  executor_cores=self._executor_cores,\n                                  executor_memory=self._executor_memory,\n                                  keytab=self._keytab,\n                                  principal=self._principal,\n                                  name=self._name,\n                                  num_executors=self._num_executors,\n                                  master=self._master,\n                                  yarn_queue=self._yarn_queue\n                                  )", "suffix": "", "gt": "        self._hook.run_query()"}
{"prefix": "def set_context(self, ti):\n        \"\"\"\n        Provide task_instance context to airflow task handler.\n        :param ti: task instance object\n        \"\"\"", "suffix": "        self.handler = logging.FileHandler(local_loc)\n        self.handler.setFormatter(self.formatter)\n        self.handler.setLevel(self.level)", "gt": "        local_loc = self._init_file(ti)"}
{"prefix": "", "suffix": "        \"\"\"\n        Template method that contains custom logic of reading\n        logs given the try_number.\n        :param ti: task instance record\n        :param try_number: current try_number to read log from\n        :param metadata: log metadata,\n                         can be used for steaming log reading and auto-tailing.\n        :return: log message as a string and metadata.\n        \"\"\"\n        # Task instance here might be different from task instance when\n        # initializing the handler. Thus explicitly getting log location\n        # is needed to get correct log path.\n        log_relative_path = self._render_filename(ti, try_number)\n        location = os.path.join(self.local_base, log_relative_path)\n\n        log = \"\"\n\n        if os.path.exists(location):\n            try:\n                with open(location) as f:\n                    log += \"*** Reading local file: {}\\n\".format(location)\n                    log += \"\".join(f.readlines())\n            except Exception as e:\n                log = \"*** Failed to load local log file: {}\\n\".format(location)\n                log += \"*** {}\\n\".format(str(e))\n        else:\n            url = os.path.join(\n                \"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path\n            ).format(\n                ti=ti,\n                worker_log_server_port=conf.get('celery', 'WORKER_LOG_SERVER_PORT')\n            )\n            log += \"*** Log file does not exist: {}\\n\".format(location)\n            log += \"*** Fetching from: {}\\n\".format(url)\n            try:\n                timeout = None  # No timeout\n                try:\n                    timeout = conf.getint('webserver', 'log_fetch_timeout_sec')\n                except (AirflowConfigException, ValueError):\n                    pass\n\n                response = requests.get(url, timeout=timeout)\n\n                # Check if the resource was properly fetched\n                response.raise_for_status()\n\n                log += '\\n' + response.text\n            except Exception as e:\n                log += \"*** Failed to fetch log file from worker. {}\\n\".format(str(e))\n\n        return log, {'end_of_log': True}", "gt": "def _read(self, ti, try_number, metadata=None):"}
{"prefix": "def read(self, task_instance, try_number=None, metadata=None):\n        \"\"\"\n        Read logs of given task instance from local machine.\n        :param task_instance: task instance object\n        :param try_number: task instance try_number to read logs from. If None\n                           it returns all logs separated by try_number\n        :param metadata: log metadata,\n                         can be used for steaming log reading and auto-tailing.\n        :return: a list of logs\n        \"\"\"\n        # Task instance increments its try number when it starts to run.\n        # So the log for a particular task try will only show up when\n        # try number gets incremented in DB, i.e logs produced the time\n        # after cli run and before try_number + 1 in DB will not be displayed.\n\n        if try_number is None:\n            next_try = task_instance.next_try_number\n            try_numbers = list(range(1, next_try))\n        elif try_number < 1:\n            logs = [\n                'Error fetching the logs. Try number {} is invalid.'.format(try_number),\n            ]\n            return logs\n        else:\n            try_numbers = [try_number]\n\n        logs = [''] * len(try_numbers)", "suffix": "        for i, try_number in enumerate(try_numbers):\n            log, metadata = self._read(task_instance, try_number, metadata)\n            logs[i] += log\n            metadatas[i] = metadata\n\n        return logs, metadatas", "gt": "        metadatas = [{}] * len(try_numbers)"}
{"prefix": "def _init_file(self, ti):\n        \"\"\"\n        Create log directory and give it correct permissions.\n        :param ti: task instance object\n        :return: relative log path of the given task instance\n        \"\"\"\n        # To handle log writing when tasks are impersonated, the log files need to\n        # be writable by the user that runs the Airflow command and the user\n        # that is impersonated. This is mainly to handle corner cases with the\n        # SubDagOperator. When the SubDagOperator is run, all of the operators\n        # run under the impersonated user and create appropriate log files\n        # as the impersonated user. However, if the user manually runs tasks\n        # of the SubDagOperator through the UI, then the log files are created\n        # by the user that runs the Airflow command. For example, the Airflow\n        # run command may be run by the `airflow_sudoable` user, but the Airflow\n        # tasks may be run by the `airflow` user. If the log files are not\n        # writable by both users, then it's possible that re-running a task\n        # via the UI (or vice versa) results in a permission error as the task", "suffix": "        relative_path = self._render_filename(ti, ti.try_number)\n        full_path = os.path.join(self.local_base, relative_path)\n        directory = os.path.dirname(full_path)\n        # Create the log file and give it group writable permissions\n        # TODO(aoen): Make log dirs and logs globally readable for now since the SubDag\n        # operator is not compatible with impersonation (e.g. if a Celery executor is used\n        # for a SubDag operator and the SubDag operator has a different owner than the\n        # parent DAG)\n        if not os.path.exists(directory):\n            # Create the directory as globally writable using custom mkdirs\n            # as os.makedirs doesn't set mode properly.\n            mkdirs(directory, 0o777)\n\n        if not os.path.exists(full_path):\n            open(full_path, \"a\").close()\n            # TODO: Investigate using 444 instead of 666.\n            os.chmod(full_path, 0o666)\n\n        return full_path", "gt": "        # tries to write to a log file created by the other user."}
{"prefix": "def load_entrypoint_plugins(entry_points, airflow_plugins):\n    \"\"\"\n    Load AirflowPlugin subclasses from the entrypoints\n    provided. The entry_point group should be 'airflow.plugins'.\n\n    :param entry_points: A collection of entrypoints to search for plugins\n    :type entry_points: Generator[setuptools.EntryPoint, None, None]\n    :param airflow_plugins: A collection of existing airflow plugins to\n        ensure we don't load duplicates\n    :type airflow_plugins: list[type[airflow.plugins_manager.AirflowPlugin]]\n    :rtype: list[airflow.plugins_manager.AirflowPlugin]\n    \"\"\"\n    for entry_point in entry_points:\n        log.debug('Importing entry_point plugin %s', entry_point.name)\n        plugin_obj = entry_point.load()\n        if is_valid_plugin(plugin_obj, airflow_plugins):\n            if callable(getattr(plugin_obj, 'on_load', None)):", "suffix": "                airflow_plugins.append(plugin_obj)\n    return airflow_plugins", "gt": "                plugin_obj.on_load()"}
{"prefix": "def is_valid_plugin(plugin_obj, existing_plugins):\n    \"\"\"\n    Check whether a potential object is a subclass of\n    the AirflowPlugin class.\n\n    :param plugin_obj: potential subclass of AirflowPlugin", "suffix": "    :return: Whether or not the obj is a valid subclass of\n        AirflowPlugin\n    \"\"\"\n    if (\n        inspect.isclass(plugin_obj) and\n        issubclass(plugin_obj, AirflowPlugin) and\n        (plugin_obj is not AirflowPlugin)\n    ):\n        plugin_obj.validate()\n        return plugin_obj not in existing_plugins\n    return False", "gt": "    :param existing_plugins: Existing list of AirflowPlugin subclasses"}
{"prefix": "def skip(self, dag_run, execution_date, tasks, session=None):\n        \"\"\"\n        Sets tasks instances to skipped from the same dag run.\n\n        :param dag_run: the DagRun for which to set the tasks to skipped\n        :param execution_date: execution_date\n        :param tasks: tasks to skip (not task_ids)\n        :param session: db session to use\n        \"\"\"\n        if not tasks:\n            return\n\n        task_ids = [d.task_id for d in tasks]\n        now = timezone.utcnow()\n\n        if dag_run:\n            session.query(TaskInstance).filter(", "suffix": "                TaskInstance.execution_date == dag_run.execution_date,\n                TaskInstance.task_id.in_(task_ids)\n            ).update({TaskInstance.state: State.SKIPPED,\n                      TaskInstance.start_date: now,\n                      TaskInstance.end_date: now},\n                     synchronize_session=False)\n            session.commit()\n        else:\n            assert execution_date is not None, \"Execution date is None and no dag run\"\n\n            self.log.warning(\"No DAG RUN present this should not happen\")\n            # this is defensive against dag runs that are not complete\n            for task in tasks:\n                ti = TaskInstance(task, execution_date=execution_date)\n                ti.state = State.SKIPPED\n                ti.start_date = now\n                ti.end_date = now\n                session.merge(ti)\n\n            session.commit()", "gt": "                TaskInstance.dag_id == dag_run.dag_id,"}
{"prefix": "", "suffix": "        \"\"\"Return a AzureDLFileSystem object.\"\"\"\n        conn = self.get_connection(self.conn_id)\n        service_options = conn.extra_dejson\n        self.account_name = service_options.get('account_name')\n\n        adlCreds = lib.auth(tenant_id=service_options.get('tenant'),\n                            client_secret=conn.password,\n                            client_id=conn.login)\n        adlsFileSystemClient = core.AzureDLFileSystem(adlCreds,\n                                                      store_name=self.account_name)\n        adlsFileSystemClient.connect()\n        return adlsFileSystemClient", "gt": "def get_conn(self):"}
{"prefix": "def check_for_file(self, file_path):\n        \"\"\"\n        Check if a file exists on Azure Data Lake.\n\n        :param file_path: Path and name of the file.\n        :type file_path: str\n        :return: True if the file exists, False otherwise.\n        :rtype: bool\n        \"\"\"\n        try:\n            files = self.connection.glob(file_path, details=False, invalidate_cache=True)", "suffix": "        except FileNotFoundError:\n            return False", "gt": "            return len(files) == 1"}
{"prefix": "def upload_file(self, local_path, remote_path, nthreads=64, overwrite=True,\n                    buffersize=4194304, blocksize=4194304):\n        \"\"\"\n        Upload a file to Azure Data Lake.\n\n        :param local_path: local path. Can be single file, directory (in which case,\n            upload recursively) or glob pattern. Recursive glob patterns using `**`\n            are not supported.\n        :type local_path: str\n        :param remote_path: Remote path to upload to; if multiple files, this is the\n            directory root to write within.\n        :type remote_path: str\n        :param nthreads: Number of threads to use. If None, uses the number of cores.\n        :type nthreads: int\n        :param overwrite: Whether to forcibly overwrite existing files/directories.\n            If False and remote path is a directory, will quit regardless if any files\n            would be overwritten or not. If True, only matching filenames are actually\n            overwritten.\n        :type overwrite: bool", "suffix": "            Number of bytes for internal buffer. This block cannot be bigger than\n            a chunk and cannot be smaller than a block.\n        :type buffersize: int\n        :param blocksize: int [2**22]\n            Number of bytes for a block. Within each chunk, we write a smaller\n            block for each API call. This block cannot be bigger than a chunk.\n        :type blocksize: int\n        \"\"\"\n        multithread.ADLUploader(self.connection,\n                                lpath=local_path,\n                                rpath=remote_path,\n                                nthreads=nthreads,\n                                overwrite=overwrite,\n                                buffersize=buffersize,\n                                blocksize=blocksize)", "gt": "        :param buffersize: int [2**22]"}
{"prefix": "def download_file(self, local_path, remote_path, nthreads=64, overwrite=True,\n                      buffersize=4194304, blocksize=4194304):\n        \"\"\"\n        Download a file from Azure Blob Storage.\n\n        :param local_path: local path. If downloading a single file, will write to this\n            specific file, unless it is an existing directory, in which case a file is", "suffix": "            directory to write within. Will create directories as required.\n        :type local_path: str\n        :param remote_path: remote path/globstring to use to find remote files.\n            Recursive glob patterns using `**` are not supported.\n        :type remote_path: str\n        :param nthreads: Number of threads to use. If None, uses the number of cores.\n        :type nthreads: int\n        :param overwrite: Whether to forcibly overwrite existing files/directories.\n            If False and remote path is a directory, will quit regardless if any files\n            would be overwritten or not. If True, only matching filenames are actually\n            overwritten.\n        :type overwrite: bool\n        :param buffersize: int [2**22]\n            Number of bytes for internal buffer. This block cannot be bigger than\n            a chunk and cannot be smaller than a block.\n        :type buffersize: int\n        :param blocksize: int [2**22]\n            Number of bytes for a block. Within each chunk, we write a smaller\n            block for each API call. This block cannot be bigger than a chunk.\n        :type blocksize: int\n        \"\"\"\n        multithread.ADLDownloader(self.connection,\n                                  lpath=local_path,\n                                  rpath=remote_path,\n                                  nthreads=nthreads,\n                                  overwrite=overwrite,\n                                  buffersize=buffersize,\n                                  blocksize=blocksize)", "gt": "            created within it. If downloading multiple files, this is the root"}
{"prefix": "def list(self, path):\n        \"\"\"\n        List files in Azure Data Lake Storage", "suffix": "        :param path: full path/globstring to use to list files in ADLS\n        :type path: str\n        \"\"\"\n        if \"*\" in path:\n            return self.connection.glob(path)\n        else:\n            return self.connection.walk(path)", "gt": ""}
{"prefix": "def execute(self, context):\n        \"\"\"\n        Run Presto Query on Athena\n        \"\"\"\n        self.hook = self.get_hook()\n        self.hook.get_conn()\n\n        self.query_execution_context['Database'] = self.database\n        self.result_configuration['OutputLocation'] = self.output_location\n        self.query_execution_id = self.hook.run_query(self.query, self.query_execution_context,\n                                                      self.result_configuration, self.client_request_token)\n        query_status = self.hook.poll_query_status(self.query_execution_id, self.max_tries)\n\n        if query_status in AWSAthenaHook.FAILURE_STATES:\n            raise Exception(\n                'Final state of Athena job is {}, query_execution_id is {}.'", "suffix": "        elif not query_status or query_status in AWSAthenaHook.INTERMEDIATE_STATES:\n            raise Exception(\n                'Final state of Athena job is {}. '\n                'Max tries of poll status exceeded, query_execution_id is {}.'\n                .format(query_status, self.query_execution_id))", "gt": "                .format(query_status, self.query_execution_id))"}
{"prefix": "def on_kill(self):\n        \"\"\"\n        Cancel the submitted athena query\n        \"\"\"\n        if self.query_execution_id:\n            self.log.info('\u26b0\ufe0f\u26b0\ufe0f\u26b0\ufe0f Received a kill Signal. Time to Die')\n            self.log.info(", "suffix": "            )\n            response = self.hook.stop_query(self.query_execution_id)\n            http_status_code = None\n            try:\n                http_status_code = response['ResponseMetadata']['HTTPStatusCode']\n            except Exception as ex:\n                self.log.error('Exception while cancelling query', ex)\n            finally:\n                if http_status_code is None or http_status_code != 200:\n                    self.log.error('Unable to request query cancel on athena. Exiting')\n                else:\n                    self.log.info(\n                        'Polling Athena for query with id %s to reach final state', self.query_execution_id\n                    )\n                    self.hook.poll_query_status(self.query_execution_id)", "gt": "                'Stopping Query with executionId - %s', self.query_execution_id"}
{"prefix": "", "suffix": "    \"\"\"\n    Uncompress gz and bz2 files\n    \"\"\"\n    if file_extension.lower() not in ('.gz', '.bz2'):\n        raise NotImplementedError(\"Received {} format. Only gz and bz2 \"\n                                  \"files can currently be uncompressed.\"\n                                  .format(file_extension))\n    if file_extension.lower() == '.gz':\n        fmodule = gzip.GzipFile\n    elif file_extension.lower() == '.bz2':\n        fmodule = bz2.BZ2File\n    with fmodule(input_file_name, mode='rb') as f_compressed,\\\n        NamedTemporaryFile(dir=dest_dir,\n                           mode='wb',\n                           delete=False) as f_uncompressed:\n        shutil.copyfileobj(f_compressed, f_uncompressed)\n    return f_uncompressed.name", "gt": "def uncompress_file(input_file_name, file_extension, dest_dir):"}
{"prefix": "def _query_mssql(self):\n        \"\"\"\n        Queries MSSQL and returns a cursor of results.\n\n        :return: mssql cursor\n        \"\"\"\n        mssql = MsSqlHook(mssql_conn_id=self.mssql_conn_id)\n        conn = mssql.get_conn()", "suffix": "        cursor.execute(self.sql)\n        return cursor", "gt": "        cursor = conn.cursor()"}
{"prefix": "def _write_local_data_files(self, cursor):\n        \"\"\"\n        Takes a cursor, and writes results to a local file.\n\n        :return: A dictionary where keys are filenames to be used as object\n            names in GCS, and values are file handles to local files that\n            contain the data for the GCS objects.\n        \"\"\"\n        schema = list(map(lambda schema_tuple: schema_tuple[0].replace(' ', '_'), cursor.description))\n        file_no = 0\n        tmp_file_handle = NamedTemporaryFile(delete=True)", "suffix": "\n        for row in cursor:\n            # Convert if needed\n            row = map(self.convert_types, row)\n            row_dict = dict(zip(schema, row))\n\n            s = json.dumps(row_dict, sort_keys=True)\n            s = s.encode('utf-8')\n            tmp_file_handle.write(s)\n\n            # Append newline to make dumps BQ compatible\n            tmp_file_handle.write(b'\\n')\n\n            # Stop if the file exceeds the file size limit\n            if tmp_file_handle.tell() >= self.approx_max_file_size_bytes:\n                file_no += 1\n                tmp_file_handle = NamedTemporaryFile(delete=True)\n                tmp_file_handles[self.filename.format(file_no)] = tmp_file_handle\n\n        return tmp_file_handles", "gt": "        tmp_file_handles = {self.filename.format(file_no): tmp_file_handle}"}
{"prefix": "def _upload_to_gcs(self, files_to_upload):\n        \"\"\"\n        Upload all of the file splits (and optionally the schema .json file) to\n        Google cloud storage.\n        \"\"\"", "suffix": "            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,\n            delegate_to=self.delegate_to)\n        for object_name, tmp_file_handle in files_to_upload.items():\n            hook.upload(self.bucket, object_name, tmp_file_handle.name, 'application/json',\n                        (self.gzip if object_name != self.schema_filename else False))", "gt": "        hook = GoogleCloudStorageHook("}
{"prefix": "def convert_types(cls, value):\n        \"\"\"\n        Takes a value from MSSQL, and converts it to a value that's safe for\n        JSON/Google Cloud Storage/BigQuery.", "suffix": "        if isinstance(value, decimal.Decimal):\n            return float(value)\n        else:\n            return value", "gt": "        \"\"\""}
{"prefix": "def action_logging(f):\n    \"\"\"", "suffix": "    but in CLI context. It will call action logger callbacks twice,\n    one for pre-execution and the other one for post-execution.\n\n    Action logger will be called with below keyword parameters:\n        sub_command : name of sub-command\n        start_datetime : start datetime instance by utc\n        end_datetime : end datetime instance by utc\n        full_command : full command line arguments\n        user : current user\n        log : airflow.models.log.Log ORM instance\n        dag_id : dag id (optional)\n        task_id : task_id (optional)\n        execution_date : execution date (optional)\n        error : exception instance if there's an exception\n\n    :param f: function instance\n    :return: wrapped function\n    \"\"\"\n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n        \"\"\"\n        An wrapper for cli functions. It assumes to have Namespace instance\n        at 1st positional argument\n        :param args: Positional argument. It assumes to have Namespace instance\n        at 1st positional argument\n        :param kwargs: A passthrough keyword argument\n        \"\"\"\n        assert args\n        assert isinstance(args[0], Namespace), \\\n            \"1st positional argument should be argparse.Namespace instance, \" \\\n            \"but {}\".format(args[0])\n        metrics = _build_metrics(f.__name__, args[0])\n        cli_action_loggers.on_pre_execution(**metrics)\n        try:\n            return f(*args, **kwargs)\n        except Exception as e:\n            metrics['error'] = e\n            raise\n        finally:\n            metrics['end_datetime'] = datetime.utcnow()\n            cli_action_loggers.on_post_execution(**metrics)\n\n    return wrapper", "gt": "    Decorates function to execute function at the same time submitting action_logging"}
{"prefix": "def _build_metrics(func_name, namespace):\n    \"\"\"\n    Builds metrics dict from function args\n    It assumes that function arguments is from airflow.bin.cli module's function\n    and has Namespace instance where it optionally contains \"dag_id\", \"task_id\",\n    and \"execution_date\".\n\n    :param func_name: name of function\n    :param namespace: Namespace instance from argparse\n    :return: dict with metrics\n    \"\"\"\n\n    metrics = {'sub_command': func_name, 'start_datetime': datetime.utcnow(),\n               'full_command': '{}'.format(list(sys.argv)), 'user': getpass.getuser()}\n\n    assert isinstance(namespace, Namespace)\n    tmp_dic = vars(namespace)\n    metrics['dag_id'] = tmp_dic.get('dag_id')\n    metrics['task_id'] = tmp_dic.get('task_id')\n    metrics['execution_date'] = tmp_dic.get('execution_date')\n    metrics['host_name'] = socket.gethostname()\n", "suffix": "    log = Log(\n        event='cli_{}'.format(func_name),\n        task_instance=None,\n        owner=metrics['user'],\n        extra=extra,\n        task_id=metrics.get('task_id'),\n        dag_id=metrics.get('dag_id'),\n        execution_date=metrics.get('execution_date'))\n    metrics['log'] = log\n    return metrics", "gt": "    extra = json.dumps(dict((k, metrics[k]) for k in ('host_name', 'full_command')))"}
{"prefix": "def _evaluate_trigger_rule(\n            self,\n            ti,\n            successes,\n            skipped,\n            failed,\n            upstream_failed,\n            done,\n            flag_upstream_failed,\n            session):\n        \"\"\"\n        Yields a dependency status that indicate whether the given task instance's trigger\n        rule was met.", "suffix": "        :param ti: the task instance to evaluate the trigger rule of\n        :type ti: airflow.models.TaskInstance\n        :param successes: Number of successful upstream tasks\n        :type successes: bool\n        :param skipped: Number of skipped upstream tasks\n        :type skipped: bool\n        :param failed: Number of failed upstream tasks\n        :type failed: bool\n        :param upstream_failed: Number of upstream_failed upstream tasks\n        :type upstream_failed: bool\n        :param done: Number of completed upstream tasks\n        :type done: bool\n        :param flag_upstream_failed: This is a hack to generate\n            the upstream_failed state creation while checking to see\n            whether the task instance is runnable. It was the shortest\n            path to add the feature\n        :type flag_upstream_failed: bool\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        \"\"\"\n\n        TR = airflow.utils.trigger_rule.TriggerRule\n\n        task = ti.task\n        upstream = len(task.upstream_task_ids)\n        tr = task.trigger_rule\n        upstream_done = done >= upstream\n        upstream_tasks_state = {\n            \"total\": upstream, \"successes\": successes, \"skipped\": skipped,\n            \"failed\": failed, \"upstream_failed\": upstream_failed, \"done\": done\n        }\n        # TODO(aoen): Ideally each individual trigger rules would be its own class, but\n        # this isn't very feasible at the moment since the database queries need to be\n        # bundled together for efficiency.\n        # handling instant state assignment based on trigger rules\n        if flag_upstream_failed:\n            if tr == TR.ALL_SUCCESS:\n                if upstream_failed or failed:\n                    ti.set_state(State.UPSTREAM_FAILED, session)\n                elif skipped:\n                    ti.set_state(State.SKIPPED, session)\n            elif tr == TR.ALL_FAILED:\n                if successes or skipped:\n                    ti.set_state(State.SKIPPED, session)\n            elif tr == TR.ONE_SUCCESS:\n                if upstream_done and not successes:\n                    ti.set_state(State.SKIPPED, session)\n            elif tr == TR.ONE_FAILED:\n                if upstream_done and not (failed or upstream_failed):\n                    ti.set_state(State.SKIPPED, session)\n            elif tr == TR.NONE_FAILED:\n                if upstream_failed or failed:\n                    ti.set_state(State.UPSTREAM_FAILED, session)\n                elif skipped == upstream:\n                    ti.set_state(State.SKIPPED, session)\n            elif tr == TR.NONE_SKIPPED:\n                if skipped:\n                    ti.set_state(State.SKIPPED, session)\n\n        if tr == TR.ONE_SUCCESS:\n            if successes <= 0:\n                yield self._failing_status(\n                    reason=\"Task's trigger rule '{0}' requires one upstream \"\n                    \"task success, but none were found. \"\n                    \"upstream_tasks_state={1}, upstream_task_ids={2}\"\n                    .format(tr, upstream_tasks_state, task.upstream_task_ids))\n        elif tr == TR.ONE_FAILED:\n            if not failed and not upstream_failed:\n                yield self._failing_status(\n                    reason=\"Task's trigger rule '{0}' requires one upstream \"\n                    \"task failure, but none were found. \"\n                    \"upstream_tasks_state={1}, upstream_task_ids={2}\"\n                    .format(tr, upstream_tasks_state, task.upstream_task_ids))\n        elif tr == TR.ALL_SUCCESS:\n            num_failures = upstream - successes\n            if num_failures > 0:\n                yield self._failing_status(\n                    reason=\"Task's trigger rule '{0}' requires all upstream \"\n                    \"tasks to have succeeded, but found {1} non-success(es). \"\n                    \"upstream_tasks_state={2}, upstream_task_ids={3}\"\n                    .format(tr, num_failures, upstream_tasks_state,\n                            task.upstream_task_ids))\n        elif tr == TR.ALL_FAILED:\n            num_successes = upstream - failed - upstream_failed\n            if num_successes > 0:\n                yield self._failing_status(\n                    reason=\"Task's trigger rule '{0}' requires all upstream \"\n                    \"tasks to have failed, but found {1} non-failure(s). \"\n                    \"upstream_tasks_state={2}, upstream_task_ids={3}\"\n                    .format(tr, num_successes, upstream_tasks_state,\n                            task.upstream_task_ids))\n        elif tr == TR.ALL_DONE:\n            if not upstream_done:\n                yield self._failing_status(\n                    reason=\"Task's trigger rule '{0}' requires all upstream \"\n                    \"tasks to have completed, but found {1} task(s) that \"\n                    \"weren't done. upstream_tasks_state={2}, \"\n                    \"upstream_task_ids={3}\"\n                    .format(tr, upstream_done, upstream_tasks_state,\n                            task.upstream_task_ids))\n        elif tr == TR.NONE_FAILED:\n            num_failures = upstream - successes - skipped\n            if num_failures > 0:\n                yield self._failing_status(\n                    reason=\"Task's trigger rule '{0}' requires all upstream \"\n                    \"tasks to have succeeded or been skipped, but found {1} non-success(es). \"\n                    \"upstream_tasks_state={2}, upstream_task_ids={3}\"\n                    .format(tr, num_failures, upstream_tasks_state,\n                            task.upstream_task_ids))\n        elif tr == TR.NONE_SKIPPED:\n            if skipped > 0:\n                yield self._failing_status(\n                    reason=\"Task's trigger rule '{0}' requires all upstream \"\n                    \"tasks to not have been skipped, but found {1} task(s) skipped. \"\n                    \"upstream_tasks_state={2}, upstream_task_ids={3}\"\n                    .format(tr, skipped, upstream_tasks_state,\n                            task.upstream_task_ids))\n        else:\n            yield self._failing_status(\n                reason=\"No strategy to evaluate trigger rule '{0}'.\".format(tr))", "gt": ""}
{"prefix": "def _create_cgroup(self, path):\n        \"\"\"\n        Create the specified cgroup.\n\n        :param path: The path of the cgroup to create.\n        E.g. cpu/mygroup/mysubgroup\n        :return: the Node associated with the created cgroup.\n        :rtype: cgroupspy.nodes.Node\n        \"\"\"\n        node = trees.Tree().root\n        path_split = path.split(os.sep)\n        for path_element in path_split:", "suffix": "            if path_element not in name_to_node:\n                self.log.debug(\"Creating cgroup %s in %s\", path_element, node.path)\n                node = node.create_cgroup(path_element)\n            else:\n                self.log.debug(\n                    \"Not creating cgroup %s in %s since it already exists\",\n                    path_element, node.path\n                )\n                node = name_to_node[path_element]\n        return node", "gt": "            name_to_node = {x.name: x for x in node.children}"}
{"prefix": "def _delete_cgroup(self, path):\n        \"\"\"\n        Delete the specified cgroup.", "suffix": "        :param path: The path of the cgroup to delete.\n        E.g. cpu/mygroup/mysubgroup\n        \"\"\"\n        node = trees.Tree().root\n        path_split = path.split(\"/\")\n        for path_element in path_split:\n            name_to_node = {x.name: x for x in node.children}\n            if path_element not in name_to_node:\n                self.log.warning(\"Cgroup does not exist: %s\", path)\n                return\n            else:\n                node = name_to_node[path_element]\n        # node is now the leaf node\n        parent = node.parent\n        self.log.debug(\"Deleting cgroup %s/%s\", parent, node.name)\n        parent.delete_cgroup(node.name)", "gt": ""}
{"prefix": "def _get_cgroup_names():\n        \"\"\"\n        :return: a mapping between the subsystem name to the cgroup name", "suffix": "        \"\"\"\n        with open(\"/proc/self/cgroup\") as f:\n            lines = f.readlines()\n            d = {}\n            for line in lines:\n                line_split = line.rstrip().split(\":\")\n                subsystem = line_split[1]\n                group_name = line_split[2]\n                d[subsystem] = group_name\n            return d", "gt": "        :rtype: dict[str, str]"}
{"prefix": "def _parse_host(host):\n        \"\"\"\n        The purpose of this function is to be robust to improper connections\n        settings provided by users, specifically in the host field.\n\n        For example -- when users supply ``https://xx.cloud.databricks.com`` as the\n        host, we must strip out the protocol to get the host.::\n", "suffix": "            assert h._parse_host('https://xx.cloud.databricks.com') == \\\n                'xx.cloud.databricks.com'\n\n        In the case where users supply the correct ``xx.cloud.databricks.com`` as the\n        host, this function is a no-op.::\n\n            assert h._parse_host('xx.cloud.databricks.com') == 'xx.cloud.databricks.com'\n\n        \"\"\"\n        urlparse_host = urlparse.urlparse(host).hostname\n        if urlparse_host:\n            # In this case, host = https://xx.cloud.databricks.com\n            return urlparse_host\n        else:\n            # In this case, host = xx.cloud.databricks.com\n            return host", "gt": "            h = DatabricksHook()"}
{"prefix": "def _do_api_call(self, endpoint_info, json):\n        \"\"\"\n        Utility function to perform an API call with retries\n\n        :param endpoint_info: Tuple of method and endpoint\n        :type endpoint_info: tuple[string, string]\n        :param json: Parameters for this API call.\n        :type json: dict\n        :return: If the api call returns a OK status code,\n            this function returns the response in JSON. Otherwise,", "suffix": "        :rtype: dict\n        \"\"\"\n        method, endpoint = endpoint_info\n        url = 'https://{host}/{endpoint}'.format(\n            host=self._parse_host(self.databricks_conn.host),\n            endpoint=endpoint)\n        if 'token' in self.databricks_conn.extra_dejson:\n            self.log.info('Using token auth.')\n            auth = _TokenAuth(self.databricks_conn.extra_dejson['token'])\n        else:\n            self.log.info('Using basic auth.')\n            auth = (self.databricks_conn.login, self.databricks_conn.password)\n        if method == 'GET':\n            request_func = requests.get\n        elif method == 'POST':\n            request_func = requests.post\n        else:\n            raise AirflowException('Unexpected HTTP Method: ' + method)\n\n        attempt_num = 1\n        while True:\n            try:\n                response = request_func(\n                    url,\n                    json=json,\n                    auth=auth,\n                    headers=USER_AGENT_HEADER,\n                    timeout=self.timeout_seconds)\n                response.raise_for_status()\n                return response.json()\n            except requests_exceptions.RequestException as e:\n                if not _retryable_error(e):\n                    # In this case, the user probably made a mistake.\n                    # Don't retry.\n                    raise AirflowException('Response: {0}, Status Code: {1}'.format(\n                        e.response.content, e.response.status_code))\n\n                self._log_request_error(attempt_num, e)\n\n            if attempt_num == self.retry_limit:\n                raise AirflowException(('API requests to Databricks failed {} times. ' +\n                                        'Giving up.').format(self.retry_limit))\n\n            attempt_num += 1\n            sleep(self.retry_delay)", "gt": "            we throw an AirflowException."}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Sign into Salesforce, only if we are not already signed in.\n        \"\"\"\n        if not self.conn:\n            connection = self.get_connection(self.conn_id)\n            extras = connection.extra_dejson\n            self.conn = Salesforce(", "suffix": "                password=connection.password,\n                security_token=extras['security_token'],\n                instance_url=connection.host,\n                sandbox=extras.get('sandbox', False)\n            )\n        return self.conn", "gt": "                username=connection.login,"}
{"prefix": "def make_query(self, query):\n        \"\"\"\n        Make a query to Salesforce.\n\n        :param query: The query to make to Salesforce.", "suffix": "        :return: The query result.\n        :rtype: dict\n        \"\"\"\n        conn = self.get_conn()\n\n        self.log.info(\"Querying for all objects\")\n        query_results = conn.query_all(query)\n\n        self.log.info(\"Received results: Total size: %s; Done: %s\",\n                      query_results['totalSize'], query_results['done'])\n\n        return query_results", "gt": "        :type query: str"}
{"prefix": "def describe_object(self, obj):\n        \"\"\"\n        Get the description of an object from Salesforce.\n        This description is the object's schema and\n        some extra metadata that Salesforce stores for each object.\n", "suffix": "        :type obj: str\n        :return: the description of the Salesforce object.\n        :rtype: dict\n        \"\"\"\n        conn = self.get_conn()\n\n        return conn.__getattr__(obj).describe()", "gt": "        :param obj: The name of the Salesforce object that we are getting a description of."}
{"prefix": "def get_available_fields(self, obj):\n        \"\"\"\n        Get a list of all available fields for an object.\n\n        :param obj: The name of the Salesforce object that we are getting a description of.\n        :type obj: str", "suffix": "        :rtype: list of str\n        \"\"\"\n        self.get_conn()\n\n        obj_description = self.describe_object(obj)\n\n        return [field['name'] for field in obj_description['fields']]", "gt": "        :return: the names of the fields."}
{"prefix": "def get_object_from_salesforce(self, obj, fields):\n        \"\"\"\n        Get all instances of the `object` from Salesforce.\n        For each model, only get the fields specified in fields.\n\n        All we really do underneath the hood is run:\n            SELECT <fields> FROM <obj>;\n\n        :param obj: The object name to get from Salesforce.\n        :type obj: str\n        :param fields: The fields to get from the object.\n        :type fields: iterable\n        :return: all instances of the object from Salesforce.\n        :rtype: dict\n        \"\"\"\n        query = \"SELECT {} FROM {}\".format(\",\".join(fields), obj)\n\n        self.log.info(\"Making query to Salesforce: %s\",", "suffix": "\n        return self.make_query(query)", "gt": "                      query if len(query) < 30 else \" ... \".join([query[:15], query[-15:]]))"}
{"prefix": "def _to_timestamp(cls, column):\n        \"\"\"\n        Convert a column of a dataframe to UNIX timestamps if applicable\n\n        :param column: A Series object representing a column of a dataframe.\n        :type column: pd.Series\n        :return: a new series that maintains the same index as the original\n        :rtype: pd.Series\n        \"\"\"\n        # try and convert the column to datetimes\n        # the column MUST have a four digit year somewhere in the string\n        # there should be a better way to do this,\n        # but just letting pandas try and convert every column without a format\n        # caused it to convert floats as well\n        # For example, a column of integers\n        # between 0 and 10 are turned into timestamps\n        # if the column cannot be converted,\n        # just return the original column untouched\n        try:\n            column = pd.to_datetime(column)\n        except ValueError:\n            log = LoggingMixin().log\n            log.warning(\"Could not convert field to timestamps: %s\", column.name)", "suffix": "\n        # now convert the newly created datetimes into timestamps\n        # we have to be careful here\n        # because NaT cannot be converted to a timestamp\n        # so we have to return NaN\n        converted = []\n        for value in column:\n            try:\n                converted.append(value.timestamp())\n            except (ValueError, AttributeError):\n                converted.append(pd.np.NaN)\n\n        return pd.Series(converted, index=column.index)", "gt": "            return column"}
{"prefix": "def write_object_to_file(self,\n                             query_results,\n                             filename,\n                             fmt=\"csv\",\n                             coerce_to_timestamp=False,\n                             record_time_added=False):\n        \"\"\"\n        Write query results to file.\n\n        Acceptable formats are:\n            - csv:\n                comma-separated-values file. This is the default format.\n            - json:\n                JSON array. Each element in the array is a different row.\n            - ndjson:\n                JSON array but each element is new-line delimited instead of comma delimited like in `json`\n\n        This requires a significant amount of cleanup.\n        Pandas doesn't handle output to CSV and json in a uniform way.\n        This is especially painful for datetime types.\n        Pandas wants to write them as strings in CSV, but as millisecond Unix timestamps.\n\n        By default, this function will try and leave all values as they are represented in Salesforce.\n        You use the `coerce_to_timestamp` flag to force all datetimes to become Unix timestamps (UTC).\n        This is can be greatly beneficial as it will make all of your datetime fields look the same,\n        and makes it easier to work with in other database environments\n\n        :param query_results: the results from a SQL query\n        :type query_results: list of dict\n        :param filename: the name of the file where the data should be dumped to\n        :type filename: str\n        :param fmt: the format you want the output in. Default:  'csv'\n        :type fmt: str\n        :param coerce_to_timestamp: True if you want all datetime fields to be converted into Unix timestamps.\n            False if you want them to be left in the same format as they were in Salesforce.\n            Leaving the value as False will result in datetimes being strings. Default: False\n        :type coerce_to_timestamp: bool\n        :param record_time_added: True if you want to add a Unix timestamp field\n            to the resulting data that marks when the data was fetched from Salesforce. Default: False\n        :type record_time_added: bool\n        :return: the dataframe that gets written to the file.\n        :rtype: pd.Dataframe\n        \"\"\"\n        fmt = fmt.lower()\n        if fmt not in ['csv', 'json', 'ndjson']:\n            raise ValueError(\"Format value is not recognized: {}\".format(fmt))\n\n        # this line right here will convert all integers to floats\n        # if there are any None/np.nan values in the column\n        # that's because None/np.nan cannot exist in an integer column\n        # we should write all of our timestamps as FLOATS in our final schema\n        df = pd.DataFrame.from_records(query_results, exclude=[\"attributes\"])\n\n        df.columns = [column.lower() for column in df.columns]\n\n        # convert columns with datetime strings to datetimes\n        # not all strings will be datetimes, so we ignore any errors that occur\n        # we get the object's definition at this point and only consider\n        # features that are DATE or DATETIME\n        if coerce_to_timestamp and df.shape[0] > 0:\n            # get the object name out of the query results\n            # it's stored in the \"attributes\" dictionary\n            # for each returned record\n            object_name = query_results[0]['attributes']['type']\n\n            self.log.info(\"Coercing timestamps for: %s\", object_name)\n\n            schema = self.describe_object(object_name)\n\n            # possible columns that can be converted to timestamps\n            # are the ones that are either date or datetime types\n            # strings are too general and we risk unintentional conversion\n            possible_timestamp_cols = [\n                field['name'].lower()\n                for field in schema['fields']\n                if field['type'] in [\"date\", \"datetime\"] and field['name'].lower() in df.columns\n            ]\n            df[possible_timestamp_cols] = df[possible_timestamp_cols].apply(self._to_timestamp)\n\n        if record_time_added:\n            fetched_time = time.time()\n            df[\"time_fetched_from_salesforce\"] = fetched_time\n\n        # write the CSV or JSON file depending on the option\n        # NOTE:\n        #   datetimes here are an issue.\n        #   There is no good way to manage the difference\n        #   for to_json, the options are an epoch or a ISO string\n        #   but for to_csv, it will be a string output by datetime\n        #   For JSON we decided to output the epoch timestamp in seconds\n        #   (as is fairly standard for JavaScript)\n        #   And for csv, we do a string\n        if fmt == \"csv\":\n            # there are also a ton of newline objects that mess up our ability to write to csv\n            # we remove these newlines so that the output is a valid CSV format\n            self.log.info(\"Cleaning data and writing to CSV\")\n            possible_strings = df.columns[df.dtypes == \"object\"]\n            df[possible_strings] = df[possible_strings].apply(", "suffix": "            )\n            # write the dataframe\n            df.to_csv(filename, index=False)\n        elif fmt == \"json\":\n            df.to_json(filename, \"records\", date_unit=\"s\")\n        elif fmt == \"ndjson\":\n            df.to_json(filename, \"records\", lines=True, date_unit=\"s\")\n\n        return df", "gt": "                lambda x: x.str.replace(\"\\r\\n\", \"\").str.replace(\"\\n\", \"\")"}
{"prefix": "def _read(self, ti, try_number, metadata=None):\n        \"\"\"\n        Read logs of given task instance and try_number from GCS.\n        If failed, read the log from task instance host machine.\n        :param ti: task instance object", "suffix": "        :param metadata: log metadata,\n                         can be used for steaming log reading and auto-tailing.\n        \"\"\"\n        # Explicitly getting log relative path is necessary as the given\n        # task instance might be different than task instance passed in\n        # in set_context method.\n        log_relative_path = self._render_filename(ti, try_number)\n        remote_loc = os.path.join(self.remote_base, log_relative_path)\n\n        try:\n            remote_log = self.gcs_read(remote_loc)\n            log = '*** Reading remote log from {}.\\n{}\\n'.format(\n                remote_loc, remote_log)\n            return log, {'end_of_log': True}\n        except Exception as e:\n            log = '*** Unable to read remote log from {}\\n*** {}\\n\\n'.format(\n                remote_loc, str(e))\n            self.log.error(log)\n            local_log, metadata = super()._read(ti, try_number)\n            log += local_log\n            return log, metadata", "gt": "        :param try_number: task instance try_number to read logs from"}
{"prefix": "def gcs_read(self, remote_log_location):\n        \"\"\"\n        Returns the log found at the remote_log_location.", "suffix": "        :type remote_log_location: str (path)\n        \"\"\"\n        bkt, blob = self.parse_gcs_url(remote_log_location)\n        return self.hook.download(bkt, blob).decode('utf-8')", "gt": "        :param remote_log_location: the log's location in remote storage"}
{"prefix": "def gcs_write(self, log, remote_log_location, append=True):\n        \"\"\"\n        Writes the log to the remote_log_location. Fails silently if no hook\n        was created.\n        :param log: the log to write to the remote_log_location\n        :type log: str\n        :param remote_log_location: the log's location in remote storage\n        :type remote_log_location: str (path)\n        :param append: if False, any existing log file is overwritten. If True,\n            the new log is appended to any existing logs.\n        :type append: bool\n        \"\"\"\n        if append:\n            try:\n                old_log = self.gcs_read(remote_log_location)\n                log = '\\n'.join([old_log, log]) if old_log else log\n            except Exception as e:\n                if not hasattr(e, 'resp') or e.resp.get('status') != '404':\n                    log = '*** Previous log discarded: {}\\n\\n'.format(str(e)) + log\n\n        try:\n            bkt, blob = self.parse_gcs_url(remote_log_location)\n            from tempfile import NamedTemporaryFile", "suffix": "                tmpfile.write(log)\n                # Force the file to be flushed, since we're doing the\n                # upload from within the file context (it hasn't been\n                # closed).\n                tmpfile.flush()\n                self.hook.upload(bkt, blob, tmpfile.name)\n        except Exception as e:\n            self.log.error('Could not write logs to %s: %s', remote_log_location, e)", "gt": "            with NamedTemporaryFile(mode='w+') as tmpfile:"}
{"prefix": "def parse_gcs_url(gsurl):\n        \"\"\"\n        Given a Google Cloud Storage URL (gs://<bucket>/<blob>), returns a\n        tuple containing the corresponding bucket and blob.\n        \"\"\"\n        parsed_url = urlparse(gsurl)\n        if not parsed_url.netloc:\n            raise AirflowException('Please provide a bucket name')\n        else:\n            bucket = parsed_url.netloc\n            blob = parsed_url.path.strip('/')", "suffix": "", "gt": "            return bucket, blob"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Fetches PyMongo Client\n        \"\"\"\n        if self.client is not None:\n            return self.client\n\n        # Mongo Connection Options dict that is unpacked when passed to MongoClient\n        options = self.extras\n\n        # If we are using SSL disable requiring certs from specific hostname\n        if options.get('ssl', False):\n            options.update({'ssl_cert_reqs': CERT_NONE})", "suffix": "        self.client = MongoClient(self.uri, **options)\n\n        return self.client", "gt": ""}
{"prefix": "", "suffix": "        \"\"\"\n        Fetches a mongo collection object for querying.\n\n        Uses connection schema as DB unless specified.\n        \"\"\"\n        mongo_db = mongo_db if mongo_db is not None else self.connection.schema\n        mongo_conn = self.get_conn()\n\n        return mongo_conn.get_database(mongo_db).get_collection(mongo_collection)", "gt": "def get_collection(self, mongo_collection, mongo_db=None):"}
{"prefix": "def aggregate(self, mongo_collection, aggregate_query, mongo_db=None, **kwargs):\n        \"\"\"\n        Runs an aggregation pipeline and returns the results\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.aggregate\n        https://api.mongodb.com/python/current/examples/aggregation.html\n        \"\"\"\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)", "suffix": "        return collection.aggregate(aggregate_query, **kwargs)", "gt": ""}
{"prefix": "", "suffix": "        \"\"\"\n        Runs a mongo find query and returns the results\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.find\n        \"\"\"\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        if find_one:\n            return collection.find_one(query, **kwargs)\n        else:\n            return collection.find(query, **kwargs)", "gt": "def find(self, mongo_collection, query, find_one=False, mongo_db=None, **kwargs):"}
{"prefix": "def insert_one(self, mongo_collection, doc, mongo_db=None, **kwargs):\n        \"\"\"\n        Inserts a single document into a mongo collection", "suffix": "        \"\"\"\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.insert_one(doc, **kwargs)", "gt": "        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.insert_one"}
{"prefix": "def insert_many(self, mongo_collection, docs, mongo_db=None, **kwargs):", "suffix": "        Inserts many docs into a mongo collection.\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.insert_many\n        \"\"\"\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.insert_many(docs, **kwargs)", "gt": "        \"\"\""}
{"prefix": "def update_one(self, mongo_collection, filter_doc, update_doc,\n                   mongo_db=None, **kwargs):\n        \"\"\"\n        Updates a single document in a mongo collection.\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.update_one\n\n        :param mongo_collection: The name of the collection to update.\n        :type mongo_collection: str\n        :param filter_doc: A query that matches the documents to update.", "suffix": "        :param update_doc: The modifications to apply.\n        :type update_doc: dict\n        :param mongo_db: The name of the database to use.\n            Can be omitted; then the database from the connection string is used.\n        :type mongo_db: str\n\n        \"\"\"\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.update_one(filter_doc, update_doc, **kwargs)", "gt": "        :type filter_doc: dict"}
{"prefix": "", "suffix": "                    mongo_db=None, **kwargs):\n        \"\"\"\n        Replaces a single document in a mongo collection.\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.replace_one\n\n        .. note::\n            If no ``filter_doc`` is given, it is assumed that the replacement\n            document contain the ``_id`` field which is then used as filters.\n\n        :param mongo_collection: The name of the collection to update.\n        :type mongo_collection: str\n        :param doc: The new document.\n        :type doc: dict\n        :param filter_doc: A query that matches the documents to replace.\n            Can be omitted; then the _id field from doc will be used.\n        :type filter_doc: dict\n        :param mongo_db: The name of the database to use.\n            Can be omitted; then the database from the connection string is used.\n        :type mongo_db: str\n        \"\"\"\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        if not filter_doc:\n            filter_doc = {'_id': doc['_id']}\n\n        return collection.replace_one(filter_doc, doc, **kwargs)", "gt": "def replace_one(self, mongo_collection, doc, filter_doc=None,"}
{"prefix": "def replace_many(self, mongo_collection, docs,\n                     filter_docs=None, mongo_db=None, upsert=False, collation=None,\n                     **kwargs):\n        \"\"\"\n        Replaces many documents in a mongo collection.\n\n        Uses bulk_write with multiple ReplaceOne operations\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.bulk_write\n\n        .. note::\n            If no ``filter_docs``are given, it is assumed that all\n            replacement documents contain the ``_id`` field which are then\n            used as filters.\n\n        :param mongo_collection: The name of the collection to update.\n        :type mongo_collection: str\n        :param docs: The new documents.\n        :type docs: list[dict]\n        :param filter_docs: A list of queries that match the documents to replace.\n            Can be omitted; then the _id fields from docs will be used.\n        :type filter_docs: list[dict]\n        :param mongo_db: The name of the database to use.\n            Can be omitted; then the database from the connection string is used.\n        :type mongo_db: str\n        :param upsert: If ``True``, perform an insert if no documents\n            match the filters for the replace operation.\n        :type upsert: bool\n        :param collation: An instance of\n            :class:`~pymongo.collation.Collation`. This option is only\n            supported on MongoDB 3.4 and above.", "suffix": "\n        \"\"\"\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        if not filter_docs:\n            filter_docs = [{'_id': doc['_id']} for doc in docs]\n\n        requests = [\n            ReplaceOne(\n                filter_docs[i],\n                docs[i],\n                upsert=upsert,\n                collation=collation)\n            for i in range(len(docs))\n        ]\n\n        return collection.bulk_write(requests, **kwargs)", "gt": "        :type collation: pymongo.collation.Collation"}
{"prefix": "def delete_one(self, mongo_collection, filter_doc, mongo_db=None, **kwargs):\n        \"\"\"\n        Deletes a single document in a mongo collection.\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_one\n", "suffix": "        :type mongo_collection: str\n        :param filter_doc: A query that matches the document to delete.\n        :type filter_doc: dict\n        :param mongo_db: The name of the database to use.\n            Can be omitted; then the database from the connection string is used.\n        :type mongo_db: str\n\n        \"\"\"\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.delete_one(filter_doc, **kwargs)", "gt": "        :param mongo_collection: The name of the collection to delete from."}
{"prefix": "def delete_many(self, mongo_collection, filter_doc, mongo_db=None, **kwargs):\n        \"\"\"\n        Deletes one or more documents in a mongo collection.\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_many\n\n        :param mongo_collection: The name of the collection to delete from.\n        :type mongo_collection: str\n        :param filter_doc: A query that matches the documents to delete.\n        :type filter_doc: dict\n        :param mongo_db: The name of the database to use.", "suffix": "        :type mongo_db: str\n\n        \"\"\"\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.delete_many(filter_doc, **kwargs)", "gt": "            Can be omitted; then the database from the connection string is used."}
{"prefix": "def has_mail_attachment(self, name, mail_folder='INBOX', check_regex=False):\n        \"\"\"\n        Checks the mail folder for mails containing attachments with the given name.\n\n        :param name: The name of the attachment that will be searched for.", "suffix": "        :param mail_folder: The mail folder where to look at.\n        :type mail_folder: str\n        :param check_regex: Checks the name for a regular expression.\n        :type check_regex: bool\n        :returns: True if there is an attachment with the given name and False if not.\n        :rtype: bool\n        \"\"\"\n        mail_attachments = self._retrieve_mails_attachments_by_name(name,\n                                                                    mail_folder,\n                                                                    check_regex,\n                                                                    latest_only=True)\n        return len(mail_attachments) > 0", "gt": "        :type name: str"}
{"prefix": "def retrieve_mail_attachments(self,\n                                  name,\n                                  mail_folder='INBOX',\n                                  check_regex=False,\n                                  latest_only=False,\n                                  not_found_mode='raise'):\n        \"\"\"\n        Retrieves mail's attachments in the mail folder by its name.\n\n        :param name: The name of the attachment that will be downloaded.\n        :type name: str\n        :param mail_folder: The mail folder where to look at.\n        :type mail_folder: str\n        :param check_regex: Checks the name for a regular expression.", "suffix": "        :param latest_only: If set to True it will only retrieve\n                            the first matched attachment.\n        :type latest_only: bool\n        :param not_found_mode: Specify what should happen if no attachment has been found.\n                               Supported values are 'raise', 'warn' and 'ignore'.\n                               If it is set to 'raise' it will raise an exception,\n                               if set to 'warn' it will only print a warning and\n                               if set to 'ignore' it won't notify you at all.\n        :type not_found_mode: str\n        :returns: a list of tuple each containing the attachment filename and its payload.\n        :rtype: a list of tuple\n        \"\"\"\n        mail_attachments = self._retrieve_mails_attachments_by_name(name,\n                                                                    mail_folder,\n                                                                    check_regex,\n                                                                    latest_only)\n        if not mail_attachments:\n            self._handle_not_found_mode(not_found_mode)\n\n        return mail_attachments", "gt": "        :type check_regex: bool"}
{"prefix": "def download_mail_attachments(self,\n                                  name,\n                                  local_output_directory,\n                                  mail_folder='INBOX',\n                                  check_regex=False,\n                                  latest_only=False,\n                                  not_found_mode='raise'):\n        \"\"\"\n        Downloads mail's attachments in the mail folder by its name to the local directory.\n\n        :param name: The name of the attachment that will be downloaded.\n        :type name: str\n        :param local_output_directory: The output directory on the local machine\n                                       where the files will be downloaded to.\n        :type local_output_directory: str\n        :param mail_folder: The mail folder where to look at.\n        :type mail_folder: str\n        :param check_regex: Checks the name for a regular expression.\n        :type check_regex: bool\n        :param latest_only: If set to True it will only download\n                            the first matched attachment.\n        :type latest_only: bool\n        :param not_found_mode: Specify what should happen if no attachment has been found.\n                               Supported values are 'raise', 'warn' and 'ignore'.\n                               If it is set to 'raise' it will raise an exception,\n                               if set to 'warn' it will only print a warning and\n                               if set to 'ignore' it won't notify you at all.\n        :type not_found_mode: str\n        \"\"\"\n        mail_attachments = self._retrieve_mails_attachments_by_name(name,\n                                                                    mail_folder,\n                                                                    check_regex,\n                                                                    latest_only)\n\n        if not mail_attachments:", "suffix": "\n        self._create_files(mail_attachments, local_output_directory)", "gt": "            self._handle_not_found_mode(not_found_mode)"}
{"prefix": "def get_attachments_by_name(self, name, check_regex, find_first=False):\n        \"\"\"\n        Gets all attachments by name for the mail.\n\n        :param name: The name of the attachment to look for.", "suffix": "        :param check_regex: Checks the name for a regular expression.\n        :type check_regex: bool\n        :param find_first: If set to True it will only find the first match and then quit.\n        :type find_first: bool\n        :returns: a list of tuples each containing name and payload\n                  where the attachments name matches the given name.\n        :rtype: list of tuple\n        \"\"\"\n        attachments = []\n\n        for part in self.mail.walk():\n            mail_part = MailPart(part)\n            if mail_part.is_attachment():\n                found_attachment = mail_part.has_matching_name(name) if check_regex \\\n                    else mail_part.has_equal_name(name)\n                if found_attachment:\n                    file_name, file_payload = mail_part.get_file()\n                    self.log.info('Found attachment: {}'.format(file_name))\n                    attachments.append((file_name, file_payload))\n                    if find_first:\n                        break\n\n        return attachments", "gt": "        :type name: str"}
{"prefix": "def get_file(self):\n        \"\"\"\n        Gets the file including name and payload.", "suffix": "        :returns: the part's name and payload.\n        :rtype: tuple\n        \"\"\"\n        return self.part.get_filename(), self.part.get_payload(decode=True)", "gt": ""}
{"prefix": "def put_records(self, records):\n        \"\"\"\n        Write batch records to Kinesis Firehose", "suffix": "\n        firehose_conn = self.get_conn()\n\n        response = firehose_conn.put_record_batch(\n            DeliveryStreamName=self.delivery_stream,\n            Records=records\n        )\n\n        return response", "gt": "        \"\"\""}
{"prefix": "def _get_dep_statuses(self, ti, session, dep_context):\n        \"\"\"\n        Determines whether a task is ready to be rescheduled. Only tasks in\n        NONE state with at least one row in task_reschedule table are\n        handled by this dependency class, otherwise this dependency is\n        considered as passed. This dependency fails if the latest reschedule\n        request's reschedule date is still in future.\n        \"\"\"\n        if dep_context.ignore_in_reschedule_period:\n            yield self._passing_status(\n                reason=\"The context specified that being in a reschedule period was \"\n                       \"permitted.\")\n            return\n\n        if ti.state not in self.RESCHEDULEABLE_STATES:\n            yield self._passing_status(\n                reason=\"The task instance is not in State_UP_FOR_RESCHEDULE or NONE state.\")\n            return\n\n        task_reschedules = TaskReschedule.find_for_task_instance(task_instance=ti)\n        if not task_reschedules:\n            yield self._passing_status(\n                reason=\"There is no reschedule request for this task instance.\")\n            return\n\n        now = timezone.utcnow()\n        next_reschedule_date = task_reschedules[-1].reschedule_date", "suffix": "            yield self._passing_status(\n                reason=\"Task instance id ready for reschedule.\")\n            return\n\n        yield self._failing_status(\n            reason=\"Task is not ready for reschedule yet but will be rescheduled \"\n                   \"automatically. Current date is {0} and task will be rescheduled \"\n                   \"at {1}.\".format(now.isoformat(), next_reschedule_date.isoformat()))", "gt": "        if now >= next_reschedule_date:"}
{"prefix": "def send_email(to, subject, html_content,", "suffix": "               mime_subtype='mixed', mime_charset='utf-8', **kwargs):\n    \"\"\"\n    Send email using backend specified in EMAIL_BACKEND.\n    \"\"\"\n    path, attr = configuration.conf.get('email', 'EMAIL_BACKEND').rsplit('.', 1)\n    module = importlib.import_module(path)\n    backend = getattr(module, attr)\n    to = get_email_address_list(to)\n    to = \", \".join(to)\n\n    return backend(to, subject, html_content, files=files,\n                   dryrun=dryrun, cc=cc, bcc=bcc,\n                   mime_subtype=mime_subtype, mime_charset=mime_charset, **kwargs)", "gt": "               files=None, dryrun=False, cc=None, bcc=None,"}
{"prefix": "def send_email_smtp(to, subject, html_content, files=None,\n                    dryrun=False, cc=None, bcc=None,\n                    mime_subtype='mixed', mime_charset='utf-8',\n                    **kwargs):\n    \"\"\"\n    Send an email with html content\n\n    >>> send_email('test@example.com', 'foo', '<b>Foo</b> bar', ['/dev/null'], dryrun=True)\n    \"\"\"", "suffix": "\n    to = get_email_address_list(to)\n\n    msg = MIMEMultipart(mime_subtype)\n    msg['Subject'] = subject\n    msg['From'] = smtp_mail_from\n    msg['To'] = \", \".join(to)\n    recipients = to\n    if cc:\n        cc = get_email_address_list(cc)\n        msg['CC'] = \", \".join(cc)\n        recipients = recipients + cc\n\n    if bcc:\n        # don't add bcc in header\n        bcc = get_email_address_list(bcc)\n        recipients = recipients + bcc\n\n    msg['Date'] = formatdate(localtime=True)\n    mime_text = MIMEText(html_content, 'html', mime_charset)\n    msg.attach(mime_text)\n\n    for fname in files or []:\n        basename = os.path.basename(fname)\n        with open(fname, \"rb\") as f:\n            part = MIMEApplication(\n                f.read(),\n                Name=basename\n            )\n            part['Content-Disposition'] = 'attachment; filename=\"%s\"' % basename\n            part['Content-ID'] = '<%s>' % basename\n            msg.attach(part)\n\n    send_MIME_email(smtp_mail_from, recipients, msg, dryrun)", "gt": "    smtp_mail_from = configuration.conf.get('smtp', 'SMTP_MAIL_FROM')"}
{"prefix": "def process_result_value(self, value, dialect):", "suffix": "        Processes DateTimes from the DB making sure it is always\n        returning UTC. Not using timezone.convert_to_utc as that\n        converts to configured TIMEZONE while the DB might be\n        running with some other setting. We assume UTC datetimes\n        in the database.\n        \"\"\"\n        if value is not None:\n            if value.tzinfo is None:\n                value = value.replace(tzinfo=utc)\n            else:\n                value = value.astimezone(utc)\n\n        return value", "gt": "        \"\"\""}
{"prefix": "def check_for_blob(self, container_name, blob_name, **kwargs):\n        \"\"\"", "suffix": "\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.exists()` takes.\n        :type kwargs: object\n        :return: True if the blob exists, False otherwise.\n        :rtype: bool\n        \"\"\"\n        return self.connection.exists(container_name, blob_name, **kwargs)", "gt": "        Check if a blob exists on Azure Blob Storage."}
{"prefix": "def check_for_prefix(self, container_name, prefix, **kwargs):\n        \"\"\"\n        Check if a prefix exists on Azure Blob storage.", "suffix": "        :param container_name: Name of the container.\n        :type container_name: str\n        :param prefix: Prefix of the blob.\n        :type prefix: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.list_blobs()` takes.\n        :type kwargs: object\n        :return: True if blobs matching the prefix exist, False otherwise.\n        :rtype: bool\n        \"\"\"\n        matches = self.connection.list_blobs(container_name, prefix,\n                                             num_results=1, **kwargs)\n        return len(list(matches)) > 0", "gt": ""}
{"prefix": "def load_file(self, file_path, container_name, blob_name, **kwargs):\n        \"\"\"\n        Upload a file to Azure Blob Storage.\n\n        :param file_path: Path to the file to load.\n        :type file_path: str\n        :param container_name: Name of the container.\n        :type container_name: str", "suffix": "        :type blob_name: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.create_blob_from_path()` takes.\n        :type kwargs: object\n        \"\"\"\n        # Reorder the argument order from airflow.hooks.S3_hook.load_file.\n        self.connection.create_blob_from_path(container_name, blob_name,\n                                              file_path, **kwargs)", "gt": "        :param blob_name: Name of the blob."}
{"prefix": "def load_string(self, string_data, container_name, blob_name, **kwargs):\n        \"\"\"\n        Upload a string to Azure Blob Storage.\n\n        :param string_data: String to load.\n        :type string_data: str\n        :param container_name: Name of the container.", "suffix": "        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.create_blob_from_text()` takes.\n        :type kwargs: object\n        \"\"\"\n        # Reorder the argument order from airflow.hooks.S3_hook.load_string.\n        self.connection.create_blob_from_text(container_name, blob_name,\n                                              string_data, **kwargs)", "gt": "        :type container_name: str"}
{"prefix": "def get_file(self, file_path, container_name, blob_name, **kwargs):\n        \"\"\"\n        Download a file from Azure Blob Storage.\n\n        :param file_path: Path to the file to download.\n        :type file_path: str\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.create_blob_from_path()` takes.", "suffix": "        \"\"\"\n        return self.connection.get_blob_to_path(container_name, blob_name,\n                                                file_path, **kwargs)", "gt": "        :type kwargs: object"}
{"prefix": "", "suffix": "        \"\"\"\n        Read a file from Azure Blob Storage and return as a string.\n\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.create_blob_from_path()` takes.\n        :type kwargs: object\n        \"\"\"\n        return self.connection.get_blob_to_text(container_name,\n                                                blob_name,\n                                                **kwargs).content", "gt": "def read_file(self, container_name, blob_name, **kwargs):"}
{"prefix": "def delete_file(self, container_name, blob_name, is_prefix=False,\n                    ignore_if_missing=False, **kwargs):\n        \"\"\"\n        Delete a file from Azure Blob Storage.\n\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param is_prefix: If blob_name is a prefix, delete all matching files\n        :type is_prefix: bool\n        :param ignore_if_missing: if True, then return success even if the\n            blob does not exist.\n        :type ignore_if_missing: bool\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.create_blob_from_path()` takes.\n        :type kwargs: object\n        \"\"\"\n\n        if is_prefix:\n            blobs_to_delete = [\n                blob.name for blob in self.connection.list_blobs(\n                    container_name, prefix=blob_name, **kwargs\n                )\n            ]\n        elif self.check_for_blob(container_name, blob_name):\n            blobs_to_delete = [blob_name]\n        else:\n            blobs_to_delete = []\n\n        if not ignore_if_missing and len(blobs_to_delete) == 0:\n            raise AirflowException('Blob(s) not found: {}'.format(blob_name))\n\n        for blob_uri in blobs_to_delete:", "suffix": "            self.connection.delete_blob(container_name,\n                                        blob_uri,\n                                        delete_snapshots='include',\n                                        **kwargs)", "gt": "            self.log.info(\"Deleting blob: \" + blob_uri)"}
{"prefix": "def mlsd(conn, path=\"\", facts=None):\n    \"\"\"\n    BACKPORT FROM PYTHON3 FTPLIB.\n\n    List a directory in a standardized format by using MLSD\n    command (RFC-3659). If path is omitted the current directory\n    is assumed. \"facts\" is a list of strings representing the type\n    of information desired (e.g. [\"type\", \"size\", \"perm\"]).", "suffix": "    Return a generator object yielding a tuple of two elements\n    for every file found in path.\n    First element is the file name, the second one is a dictionary\n    including a variable number of \"facts\" depending on the server\n    and whether \"facts\" argument has been provided.\n    \"\"\"\n    facts = facts or []\n    if facts:\n        conn.sendcmd(\"OPTS MLST \" + \";\".join(facts) + \";\")\n    if path:\n        cmd = \"MLSD %s\" % path\n    else:\n        cmd = \"MLSD\"\n    lines = []\n    conn.retrlines(cmd, lines.append)\n    for line in lines:\n        facts_found, _, name = line.rstrip(ftplib.CRLF).partition(' ')\n        entry = {}\n        for fact in facts_found[:-1].split(\";\"):\n            key, _, value = fact.partition(\"=\")\n            entry[key.lower()] = value\n        yield (name, entry)", "gt": ""}
{"prefix": "def get_conn(self):\n        \"\"\"", "suffix": "        \"\"\"\n        if self.conn is None:\n            params = self.get_connection(self.ftp_conn_id)\n            pasv = params.extra_dejson.get(\"passive\", True)\n            self.conn = ftplib.FTP(params.host, params.login, params.password)\n            self.conn.set_pasv(pasv)\n\n        return self.conn", "gt": "        Returns a FTP connection object"}
{"prefix": "def describe_directory(self, path):\n        \"\"\"\n        Returns a dictionary of {filename: {attributes}} for all files\n        on the remote system (where the MLSD command is supported).\n\n        :param path: full path to the remote directory\n        :type path: str\n        \"\"\"\n        conn = self.get_conn()\n        conn.cwd(path)\n        try:\n            # only works in Python 3\n            files = dict(conn.mlsd())", "suffix": "            files = dict(mlsd(conn))\n        return files", "gt": "        except AttributeError:"}
{"prefix": "def list_directory(self, path, nlst=False):\n        \"\"\"\n        Returns a list of files on the remote system.\n\n        :param path: full path to the remote directory to list\n        :type path: str\n        \"\"\"\n        conn = self.get_conn()", "suffix": "\n        files = conn.nlst()\n        return files", "gt": "        conn.cwd(path)"}
{"prefix": "def retrieve_file(\n            self,\n            remote_full_path,\n            local_full_path_or_buffer,\n            callback=None):\n        \"\"\"\n        Transfers the remote file to a local location.\n", "suffix": "        at that location; if it is a file-like buffer, the file will\n        be written to the buffer but not closed.\n\n        :param remote_full_path: full path to the remote file\n        :type remote_full_path: str\n        :param local_full_path_or_buffer: full path to the local file or a\n            file-like buffer\n        :type local_full_path_or_buffer: str or file-like buffer\n        :param callback: callback which is called each time a block of data\n            is read. if you do not use a callback, these blocks will be written\n            to the file or buffer passed in. if you do pass in a callback, note\n            that writing to a file or buffer will need to be handled inside the\n            callback.\n            [default: output_handle.write()]\n        :type callback: callable\n\n        :Example::\n\n            hook = FTPHook(ftp_conn_id='my_conn')\n\n            remote_path = '/path/to/remote/file'\n            local_path = '/path/to/local/file'\n\n            # with a custom callback (in this case displaying progress on each read)\n            def print_progress(percent_progress):\n                self.log.info('Percent Downloaded: %s%%' % percent_progress)\n\n            total_downloaded = 0\n            total_file_size = hook.get_size(remote_path)\n            output_handle = open(local_path, 'wb')\n            def write_to_file_with_progress(data):\n                total_downloaded += len(data)\n                output_handle.write(data)\n                percent_progress = (total_downloaded / total_file_size) * 100\n                print_progress(percent_progress)\n            hook.retrieve_file(remote_path, None, callback=write_to_file_with_progress)\n\n            # without a custom callback data is written to the local_path\n            hook.retrieve_file(remote_path, local_path)\n        \"\"\"\n        conn = self.get_conn()\n\n        is_path = isinstance(local_full_path_or_buffer, basestring)\n\n        # without a callback, default to writing to a user-provided file or\n        # file-like buffer\n        if not callback:\n            if is_path:\n                output_handle = open(local_full_path_or_buffer, 'wb')\n            else:\n                output_handle = local_full_path_or_buffer\n            callback = output_handle.write\n        else:\n            output_handle = None\n\n        remote_path, remote_file_name = os.path.split(remote_full_path)\n        conn.cwd(remote_path)\n        self.log.info('Retrieving file from FTP: %s', remote_full_path)\n        conn.retrbinary('RETR %s' % remote_file_name, callback)\n        self.log.info('Finished retrieving file from FTP: %s', remote_full_path)\n\n        if is_path and output_handle:\n            output_handle.close()", "gt": "        If local_full_path_or_buffer is a string path, the file will be put"}
{"prefix": "def store_file(self, remote_full_path, local_full_path_or_buffer):\n        \"\"\"\n        Transfers a local file to the remote location.\n\n        If local_full_path_or_buffer is a string path, the file will be read\n        from that location; if it is a file-like buffer, the file will\n        be read from the buffer but not closed.\n", "suffix": "        :type remote_full_path: str\n        :param local_full_path_or_buffer: full path to the local file or a\n            file-like buffer\n        :type local_full_path_or_buffer: str or file-like buffer\n        \"\"\"\n        conn = self.get_conn()\n\n        is_path = isinstance(local_full_path_or_buffer, basestring)\n\n        if is_path:\n            input_handle = open(local_full_path_or_buffer, 'rb')\n        else:\n            input_handle = local_full_path_or_buffer\n        remote_path, remote_file_name = os.path.split(remote_full_path)\n        conn.cwd(remote_path)\n        conn.storbinary('STOR %s' % remote_file_name, input_handle)\n\n        if is_path:\n            input_handle.close()", "gt": "        :param remote_full_path: full path to the remote file"}
{"prefix": "def rename(self, from_name, to_name):\n        \"\"\"\n        Rename a file.\n\n        :param from_name: rename file from name", "suffix": "        \"\"\"\n        conn = self.get_conn()\n        return conn.rename(from_name, to_name)", "gt": "        :param to_name: rename file to name"}
{"prefix": "def get_mod_time(self, path):\n        \"\"\"\n        Returns a datetime object representing the last time the file was modified\n\n        :param path: remote file path\n        :type path: string\n        \"\"\"\n        conn = self.get_conn()\n        ftp_mdtm = conn.sendcmd('MDTM ' + path)\n        time_val = ftp_mdtm[4:]", "suffix": "        try:\n            return datetime.datetime.strptime(time_val, \"%Y%m%d%H%M%S.%f\")\n        except ValueError:\n            return datetime.datetime.strptime(time_val, '%Y%m%d%H%M%S')", "gt": "        # time_val optionally has microseconds"}
{"prefix": "def _create_dagruns(dag, execution_dates, state, run_id_template):\n    \"\"\"\n    Infers from the dates which dag runs need to be created and does so.\n    :param dag: the dag to create dag runs for\n    :param execution_dates: list of execution dates to evaluate\n    :param state: the state to set the dag run to\n    :param run_id_template:the template for run id to be with the execution date\n    :return: newly created and existing dag runs for the execution dates supplied\n    \"\"\"\n    # find out if we need to create any dag runs\n    drs = DagRun.find(dag_id=dag.dag_id, execution_date=execution_dates)\n    dates_to_create = list(set(execution_dates) - set([dr.execution_date for dr in drs]))\n\n    for date in dates_to_create:\n        dr = dag.create_dagrun(\n            run_id=run_id_template.format(date.isoformat()),\n            execution_date=date,\n            start_date=timezone.utcnow(),\n            external_trigger=False,\n            state=state,\n        )", "suffix": "\n    return drs", "gt": "        drs.append(dr)"}
{"prefix": "def set_state(task, execution_date, upstream=False, downstream=False,\n              future=False, past=False, state=State.SUCCESS, commit=False, session=None):\n    \"\"\"\n    Set the state of a task instance and if needed its relatives. Can set state\n    for future tasks (calculated from execution_date) and retroactively\n    for past tasks. Will verify integrity of past dag runs in order to create\n    tasks that did not exist. It will not create dag runs that are missing\n    on the schedule (but it will as for subdag dag runs if needed).\n    :param task: the task from which to work. task.task.dag needs to be set\n    :param execution_date: the execution date from which to start looking\n    :param upstream: Mark all parents (upstream tasks)\n    :param downstream: Mark all siblings (downstream tasks) of task_id, including SubDags\n    :param future: Mark all future tasks on the interval of the dag up until\n        last execution date.\n    :param past: Retroactively mark all tasks starting from start_date of the DAG\n    :param state: State to which the tasks need to be set\n    :param commit: Commit tasks to be altered to the database\n    :param session: database session\n    :return: list of tasks that have been created and updated\n    \"\"\"\n    assert timezone.is_localized(execution_date)\n\n    assert task.dag is not None\n    dag = task.dag\n\n    latest_execution_date = dag.latest_execution_date\n    assert latest_execution_date is not None\n\n    # determine date range of dag runs and tasks to consider\n    end_date = latest_execution_date if future else execution_date\n", "suffix": "        start_date = dag.default_args['start_date']\n    elif dag.start_date:\n        start_date = dag.start_date\n    else:\n        start_date = execution_date\n\n    start_date = execution_date if not past else start_date\n\n    if dag.schedule_interval == '@once':\n        dates = [start_date]\n    else:\n        dates = dag.date_range(start_date=start_date, end_date=end_date)\n\n    # find relatives (siblings = downstream, parents = upstream) if needed\n    task_ids = [task.task_id]\n    if downstream:\n        relatives = task.get_flat_relatives(upstream=False)\n        task_ids += [t.task_id for t in relatives]\n    if upstream:\n        relatives = task.get_flat_relatives(upstream=True)\n        task_ids += [t.task_id for t in relatives]\n\n    # verify the integrity of the dag runs in case a task was added or removed\n    # set the confirmed execution dates as they might be different\n    # from what was provided\n    confirmed_dates = []\n    drs = DagRun.find(dag_id=dag.dag_id, execution_date=dates)\n    for dr in drs:\n        dr.dag = dag\n        dr.verify_integrity()\n        confirmed_dates.append(dr.execution_date)\n\n    # go through subdagoperators and create dag runs. We will only work\n    # within the scope of the subdag. We wont propagate to the parent dag,\n    # but we will propagate from parent to subdag.\n    dags = [dag]\n    sub_dag_ids = []\n    while len(dags) > 0:\n        current_dag = dags.pop()\n        for task_id in task_ids:\n            if not current_dag.has_task(task_id):\n                continue\n\n            current_task = current_dag.get_task(task_id)\n            if isinstance(current_task, SubDagOperator):\n                # this works as a kind of integrity check\n                # it creates missing dag runs for subdagoperators,\n                # maybe this should be moved to dagrun.verify_integrity\n                drs = _create_dagruns(current_task.subdag,\n                                      execution_dates=confirmed_dates,\n                                      state=State.RUNNING,\n                                      run_id_template=BackfillJob.ID_FORMAT_PREFIX)\n\n                for dr in drs:\n                    dr.dag = current_task.subdag\n                    dr.verify_integrity()\n                    if commit:\n                        dr.state = state\n                        session.merge(dr)\n\n                dags.append(current_task.subdag)\n                sub_dag_ids.append(current_task.subdag.dag_id)\n\n    # now look for the task instances that are affected\n    TI = TaskInstance\n\n    # get all tasks of the main dag that will be affected by a state change\n    qry_dag = session.query(TI).filter(\n        TI.dag_id == dag.dag_id,\n        TI.execution_date.in_(confirmed_dates),\n        TI.task_id.in_(task_ids)).filter(\n        or_(TI.state.is_(None),\n            TI.state != state)\n    )\n\n    # get *all* tasks of the sub dags\n    if len(sub_dag_ids) > 0:\n        qry_sub_dag = session.query(TI).filter(\n            TI.dag_id.in_(sub_dag_ids),\n            TI.execution_date.in_(confirmed_dates)).filter(\n            or_(TI.state.is_(None),\n                TI.state != state)\n        )\n\n    if commit:\n        tis_altered = qry_dag.with_for_update().all()\n        if len(sub_dag_ids) > 0:\n            tis_altered += qry_sub_dag.with_for_update().all()\n        for ti in tis_altered:\n            ti.state = state\n    else:\n        tis_altered = qry_dag.all()\n        if len(sub_dag_ids) > 0:\n            tis_altered += qry_sub_dag.all()\n\n    return tis_altered", "gt": "    if 'start_date' in dag.default_args:"}
{"prefix": "def _set_dag_run_state(dag_id, execution_date, state, session=None):\n    \"\"\"\n    Helper method that set dag run state in the DB.\n    :param dag_id: dag_id of target dag run", "suffix": "    :param state: target state\n    :param session: database session\n    \"\"\"\n    DR = DagRun\n    dr = session.query(DR).filter(\n        DR.dag_id == dag_id,\n        DR.execution_date == execution_date\n    ).one()\n    dr.state = state\n    if state == State.RUNNING:\n        dr.start_date = timezone.utcnow()\n        dr.end_date = None\n    else:\n        dr.end_date = timezone.utcnow()\n    session.merge(dr)", "gt": "    :param execution_date: the execution date from which to start looking"}
{"prefix": "def set_dag_run_state_to_success(dag, execution_date, commit=False, session=None):\n    \"\"\"\n    Set the dag run for a specific execution date and its task instances\n    to success.\n    :param dag: the DAG of which to alter state\n    :param execution_date: the execution date from which to start looking\n    :param commit: commit DAG and tasks to be altered to the database\n    :param session: database session\n    :return: If commit is true, list of tasks that have been updated,\n             otherwise list of tasks that will be updated", "suffix": "    \"\"\"\n    res = []\n\n    if not dag or not execution_date:\n        return res\n\n    # Mark the dag run to success.\n    if commit:\n        _set_dag_run_state(dag.dag_id, execution_date, State.SUCCESS, session)\n\n    # Mark all task instances of the dag run to success.\n    for task in dag.tasks:\n        task.dag = dag\n        new_state = set_state(task=task, execution_date=execution_date,\n                              state=State.SUCCESS, commit=commit)\n        res.extend(new_state)\n\n    return res", "gt": "    :raises: AssertionError if dag or execution_date is invalid"}
{"prefix": "def set_dag_run_state_to_failed(dag, execution_date, commit=False, session=None):\n    \"\"\"\n    Set the dag run for a specific execution date and its running task instances\n    to failed.\n    :param dag: the DAG of which to alter state\n    :param execution_date: the execution date from which to start looking\n    :param commit: commit DAG and tasks to be altered to the database\n    :param session: database session\n    :return: If commit is true, list of tasks that have been updated,\n             otherwise list of tasks that will be updated\n    :raises: AssertionError if dag or execution_date is invalid\n    \"\"\"\n    res = []", "suffix": "    if not dag or not execution_date:\n        return res\n\n    # Mark the dag run to failed.\n    if commit:\n        _set_dag_run_state(dag.dag_id, execution_date, State.FAILED, session)\n\n    # Mark only RUNNING task instances.\n    TI = TaskInstance\n    task_ids = [task.task_id for task in dag.tasks]\n    tis = session.query(TI).filter(\n        TI.dag_id == dag.dag_id,\n        TI.execution_date == execution_date,\n        TI.task_id.in_(task_ids)).filter(TI.state == State.RUNNING)\n    task_ids_of_running_tis = [ti.task_id for ti in tis]\n    for task in dag.tasks:\n        if task.task_id not in task_ids_of_running_tis:\n            continue\n        task.dag = dag\n        new_state = set_state(task=task, execution_date=execution_date,\n                              state=State.FAILED, commit=commit)\n        res.extend(new_state)\n\n    return res", "gt": ""}
{"prefix": "def set_dag_run_state_to_running(dag, execution_date, commit=False, session=None):", "suffix": "    Set the dag run for a specific execution date to running.\n    :param dag: the DAG of which to alter state\n    :param execution_date: the execution date from which to start looking\n    :param commit: commit DAG and tasks to be altered to the database\n    :param session: database session\n    :return: If commit is true, list of tasks that have been updated,\n             otherwise list of tasks that will be updated\n    \"\"\"\n    res = []\n    if not dag or not execution_date:\n        return res\n\n    # Mark the dag run to running.\n    if commit:\n        _set_dag_run_state(dag.dag_id, execution_date, State.RUNNING, session)\n\n    # To keep the return type consistent with the other similar functions.\n    return res", "gt": "    \"\"\""}
{"prefix": "def git_version(version):\n    \"\"\"\n    Return a version to identify the state of the underlying git repo. The version will\n    indicate whether the head of the current git-backed working directory is tied to a\n    release tag or not : it will indicate the former with a 'release:{version}' prefix\n    and the latter with a 'dev0' prefix. Following the prefix will be a sha of the current\n    branch head. Finally, a \"dirty\" suffix is appended to indicate that uncommitted\n    changes are present.\n    \"\"\"\n    repo = None\n    try:\n        import git\n        repo = git.Repo('.git')\n    except ImportError:\n        logger.warning('gitpython not found: Cannot compute the git version.')\n        return ''\n    except Exception as e:", "suffix": "        return ''\n    if repo:\n        sha = repo.head.commit.hexsha\n        if repo.is_dirty():\n            return '.dev0+{sha}.dirty'.format(sha=sha)\n        # commit is clean\n        return '.release:{version}+{sha}'.format(version=version, sha=sha)\n    else:\n        return 'no_git_version'", "gt": "        logger.warning('Cannot compute the git version. {}'.format(e))"}
{"prefix": "def execute(self, context):\n        \"\"\"\n        Call the DiscordWebhookHook to post message\n        \"\"\"\n        self.hook = DiscordWebhookHook(\n            self.http_conn_id,\n            self.webhook_endpoint,\n            self.message,\n            self.username,\n            self.avatar_url,\n            self.tts,\n            self.proxy", "suffix": "        self.hook.execute()", "gt": "        )"}
{"prefix": "def _validate_field(self, validation_spec, dictionary_to_validate, parent=None,\n                        force_optional=False):\n        \"\"\"\n        Validates if field is OK.\n\n        :param validation_spec: specification of the field\n        :type validation_spec: dict\n        :param dictionary_to_validate: dictionary where the field should be present\n        :type dictionary_to_validate: dict\n        :param parent: full path of parent field\n        :type parent: str\n        :param force_optional: forces the field to be optional\n            (all union fields have force_optional set to True)\n        :type force_optional: bool\n        :return: True if the field is present\n        \"\"\"\n        field_name = validation_spec['name']\n        field_type = validation_spec.get('type')\n        optional = validation_spec.get('optional')\n        regexp = validation_spec.get('regexp')\n        allow_empty = validation_spec.get('allow_empty')\n        children_validation_specs = validation_spec.get('fields')\n        required_api_version = validation_spec.get('api_version')\n        custom_validation = validation_spec.get('custom_validation')\n\n        full_field_path = self._get_field_name_with_parent(field_name=field_name,\n                                                           parent=parent)\n        if required_api_version and required_api_version != self._api_version:\n            self.log.debug(\n                \"Skipping validation of the field '%s' for API version '%s' \"\n                \"as it is only valid for API version '%s'\",\n                field_name, self._api_version, required_api_version)\n            return False\n        value = dictionary_to_validate.get(field_name)\n\n        if (optional or force_optional) and value is None:\n            self.log.debug(\"The optional field '%s' is missing. That's perfectly OK.\", full_field_path)\n            return False\n\n        # Certainly down from here the field is present (value is not None)\n        # so we should only return True from now on\n\n        self._sanity_checks(children_validation_specs=children_validation_specs,\n                            field_type=field_type,\n                            full_field_path=full_field_path,\n                            regexp=regexp,\n                            allow_empty=allow_empty,\n                            custom_validation=custom_validation,\n                            value=value)\n\n        if allow_empty is False:\n            self._validate_is_empty(full_field_path, value)\n        if regexp:\n            self._validate_regexp(full_field_path, regexp, value)\n        elif field_type == 'dict':\n            if not isinstance(value, dict):\n                raise GcpFieldValidationException(\n                    \"The field '{}' should be of dictionary type according to the \"\n                    \"specification '{}' but it is '{}'\".\n                    format(full_field_path, validation_spec, value))\n            if children_validation_specs is None:\n                self.log.debug(\n                    \"The dict field '%s' has no nested fields defined in the \"\n                    \"specification '%s'. That's perfectly ok - it's content will \"\n                    \"not be validated.\", full_field_path, validation_spec)\n            else:\n                self._validate_dict(children_validation_specs, full_field_path, value)\n        elif field_type == 'union':\n            if not children_validation_specs:\n                raise GcpValidationSpecificationException(\n                    \"The union field '%s' has no nested fields \"\n                    \"defined in specification '%s'. Unions should have at least one \"\n                    \"nested field defined.\", full_field_path, validation_spec)\n            self._validate_union(children_validation_specs, full_field_path,\n                                 dictionary_to_validate)\n        elif field_type == 'list':\n            if not isinstance(value, list):\n                raise GcpFieldValidationException(\n                    \"The field '{}' should be of list type according to the \"\n                    \"specification '{}' but it is '{}'\".\n                    format(full_field_path, validation_spec, value))\n        elif custom_validation:\n            try:\n                custom_validation(value)\n            except Exception as e:", "suffix": "                    \"Error while validating custom field '{}' specified by '{}': '{}'\".\n                    format(full_field_path, validation_spec, e))\n        elif field_type is None:\n            self.log.debug(\"The type of field '%s' is not specified in '%s'. \"\n                           \"Not validating its content.\", full_field_path, validation_spec)\n        else:\n            raise GcpValidationSpecificationException(\n                \"The field '{}' is of type '{}' in specification '{}'.\"\n                \"This type is unknown to validation!\".format(\n                    full_field_path, field_type, validation_spec))\n        return True", "gt": "                raise GcpFieldValidationException("}
{"prefix": "def validate(self, body_to_validate):\n        \"\"\"\n        Validates if the body (dictionary) follows specification that the validator was\n        instantiated with. Raises ValidationSpecificationException or\n        ValidationFieldException in case of problems with specification or the\n        body not conforming to the specification respectively.", "suffix": "        :param body_to_validate: body that must follow the specification\n        :type body_to_validate: dict\n        :return: None\n        \"\"\"\n        try:\n            for validation_spec in self._validation_specs:\n                self._validate_field(validation_spec=validation_spec,\n                                     dictionary_to_validate=body_to_validate)\n        except GcpFieldValidationException as e:\n            raise GcpFieldValidationException(\n                \"There was an error when validating: body '{}': '{}'\".\n                format(body_to_validate, e))\n        all_field_names = [spec['name'] for spec in self._validation_specs\n                           if spec.get('type') != 'union' and\n                           spec.get('api_version') != self._api_version]\n        all_union_fields = [spec for spec in self._validation_specs\n                            if spec.get('type') == 'union']\n        for union_field in all_union_fields:\n            all_field_names.extend(\n                [nested_union_spec['name'] for nested_union_spec in union_field['fields']\n                 if nested_union_spec.get('type') != 'union' and\n                 nested_union_spec.get('api_version') != self._api_version])\n        for field_name in body_to_validate.keys():\n            if field_name not in all_field_names:\n                self.log.warning(\n                    \"The field '%s' is in the body, but is not specified in the \"\n                    \"validation specification '%s'. \"\n                    \"This might be because you are using newer API version and \"\n                    \"new field names defined for that version. Then the warning \"\n                    \"can be safely ignored, or you might want to upgrade the operator\"\n                    \"to the version that supports the new API version.\",\n                    field_name, self._validation_specs)", "gt": ""}
{"prefix": "", "suffix": "        \"\"\"Return the FileService object.\"\"\"\n        conn = self.get_connection(self.conn_id)\n        service_options = conn.extra_dejson\n        return FileService(account_name=conn.login,\n                           account_key=conn.password, **service_options)", "gt": "def get_conn(self):"}
{"prefix": "def check_for_directory(self, share_name, directory_name, **kwargs):\n        \"\"\"", "suffix": "\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.exists()` takes.\n        :type kwargs: object\n        :return: True if the file exists, False otherwise.\n        :rtype: bool\n        \"\"\"\n        return self.connection.exists(share_name, directory_name,\n                                      **kwargs)", "gt": "        Check if a directory exists on Azure File Share."}
{"prefix": "def check_for_file(self, share_name, directory_name, file_name, **kwargs):\n        \"\"\"", "suffix": "\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param file_name: Name of the file.\n        :type file_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.exists()` takes.\n        :type kwargs: object\n        :return: True if the file exists, False otherwise.\n        :rtype: bool\n        \"\"\"\n        return self.connection.exists(share_name, directory_name,\n                                      file_name, **kwargs)", "gt": "        Check if a file exists on Azure File Share."}
{"prefix": "def list_directories_and_files(self, share_name, directory_name=None, **kwargs):\n        \"\"\"\n        Return the list of directories and files stored on a Azure File Share.\n\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param kwargs: Optional keyword arguments that", "suffix": "        :type kwargs: object\n        :return: A list of files and directories\n        :rtype: list\n        \"\"\"\n        return self.connection.list_directories_and_files(share_name,\n                                                          directory_name,\n                                                          **kwargs)", "gt": "            `FileService.list_directories_and_files()` takes."}
{"prefix": "def create_directory(self, share_name, directory_name, **kwargs):\n        \"\"\"\n        Create a new directory on a Azure File Share.\n\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.create_directory()` takes.\n        :type kwargs: object\n        :return: A list of files and directories\n        :rtype: list\n        \"\"\"", "suffix": "", "gt": "        return self.connection.create_directory(share_name, directory_name, **kwargs)"}
{"prefix": "def get_file(self, file_path, share_name, directory_name, file_name, **kwargs):\n        \"\"\"\n        Download a file from Azure File Share.\n\n        :param file_path: Where to store the file.\n        :type file_path: str\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param file_name: Name of the file.\n        :type file_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.get_file_to_path()` takes.", "suffix": "        \"\"\"\n        self.connection.get_file_to_path(share_name, directory_name,\n                                         file_name, file_path, **kwargs)", "gt": "        :type kwargs: object"}
{"prefix": "def get_file_to_stream(self, stream, share_name, directory_name, file_name, **kwargs):\n        \"\"\"\n        Download a file from Azure File Share.\n\n        :param stream: A filehandle to store the file to.\n        :type stream: file-like object\n        :param share_name: Name of the share.\n        :type share_name: str", "suffix": "        :type directory_name: str\n        :param file_name: Name of the file.\n        :type file_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.get_file_to_stream()` takes.\n        :type kwargs: object\n        \"\"\"\n        self.connection.get_file_to_stream(share_name, directory_name,\n                                           file_name, stream, **kwargs)", "gt": "        :param directory_name: Name of the directory."}
{"prefix": "def load_file(self, file_path, share_name, directory_name, file_name, **kwargs):\n        \"\"\"\n        Upload a file to Azure File Share.", "suffix": "        :param file_path: Path to the file to load.\n        :type file_path: str\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param file_name: Name of the file.\n        :type file_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.create_file_from_path()` takes.\n        :type kwargs: object\n        \"\"\"\n        self.connection.create_file_from_path(share_name, directory_name,\n                                              file_name, file_path, **kwargs)", "gt": ""}
{"prefix": "def load_string(self, string_data, share_name, directory_name, file_name, **kwargs):\n        \"\"\"\n        Upload a string to Azure File Share.\n\n        :param string_data: String to load.\n        :type string_data: str\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param file_name: Name of the file.\n        :type file_name: str", "suffix": "            `FileService.create_file_from_text()` takes.\n        :type kwargs: object\n        \"\"\"\n        self.connection.create_file_from_text(share_name, directory_name,\n                                              file_name, string_data, **kwargs)", "gt": "        :param kwargs: Optional keyword arguments that"}
{"prefix": "def load_stream(self, stream, share_name, directory_name, file_name, count, **kwargs):\n        \"\"\"\n        Upload a stream to Azure File Share.\n\n        :param stream: Opened file/stream to upload as the file content.\n        :type stream: file-like\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param file_name: Name of the file.", "suffix": "        :param count: Size of the stream in bytes\n        :type count: int\n        :param kwargs: Optional keyword arguments that\n            `FileService.create_file_from_stream()` takes.\n        :type kwargs: object\n        \"\"\"\n        self.connection.create_file_from_stream(share_name, directory_name,\n                                                file_name, stream, count, **kwargs)", "gt": "        :type file_name: str"}
{"prefix": "def set_context(self, filename):\n        \"\"\"\n        Provide filename context to airflow task handler.\n        :param filename: filename in which the dag is located\n        \"\"\"\n        local_loc = self._init_file(filename)\n        self.handler = logging.FileHandler(local_loc)", "suffix": "        self.handler.setLevel(self.level)\n\n        if self._cur_date < datetime.today():\n            self._symlink_latest_log_directory()\n            self._cur_date = datetime.today()", "gt": "        self.handler.setFormatter(self.formatter)"}
{"prefix": "def _init_file(self, filename):\n        \"\"\"\n        Create log file and directory if required.\n        :param filename: task instance object\n        :return: relative log path of the given task instance\n        \"\"\"\n        relative_path = self._render_filename(filename)\n        full_path = os.path.join(self._get_log_directory(), relative_path)\n        directory = os.path.dirname(full_path)\n\n        if not os.path.exists(directory):\n            try:\n                os.makedirs(directory)\n            except OSError:\n                if not os.path.isdir(directory):\n                    raise\n\n        if not os.path.exists(full_path):\n            open(full_path, \"a\").close()\n", "suffix": "", "gt": "        return full_path"}
{"prefix": "def _parse_gcs_url(gsurl):\n    \"\"\"\n    Given a Google Cloud Storage URL (gs://<bucket>/<blob>), returns a\n    tuple containing the corresponding bucket and blob.\n    \"\"\"\n\n    parsed_url = urlparse(gsurl)", "suffix": "        raise AirflowException('Please provide a bucket name')\n    else:\n        bucket = parsed_url.netloc\n        # Remove leading '/' but NOT trailing one\n        blob = parsed_url.path.lstrip('/')\n        return bucket, blob", "gt": "    if not parsed_url.netloc:"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Returns a Google Cloud Storage service object.\n        \"\"\"\n        if not self._conn:\n            self._conn = storage.Client(credentials=self._get_credentials())", "suffix": "        return self._conn", "gt": ""}
{"prefix": "def copy(self, source_bucket, source_object, destination_bucket=None,\n             destination_object=None):\n        \"\"\"\n        Copies an object from a bucket to another, with renaming if requested.\n\n        destination_bucket or destination_object can be omitted, in which case\n        source bucket/object is used, but not both.\n\n        :param source_bucket: The bucket of the object to copy from.\n        :type source_bucket: str\n        :param source_object: The object to copy.\n        :type source_object: str\n        :param destination_bucket: The destination of the object to copied to.\n            Can be omitted; then the same bucket is used.\n        :type destination_bucket: str\n        :param destination_object: The (renamed) path of the object if given.\n            Can be omitted; then the same name is used.\n        :type destination_object: str\n        \"\"\"\n        destination_bucket = destination_bucket or source_bucket\n        destination_object = destination_object or source_object\n        if source_bucket == destination_bucket and \\\n                source_object == destination_object:\n\n            raise ValueError(\n                'Either source/destination bucket or source/destination object '\n                'must be different, not both the same: bucket=%s, object=%s' %\n                (source_bucket, source_object))\n        if not source_bucket or not source_object:\n            raise ValueError('source_bucket and source_object cannot be empty.')", "suffix": "        client = self.get_conn()\n        source_bucket = client.get_bucket(source_bucket)\n        source_object = source_bucket.blob(source_object)\n        destination_bucket = client.get_bucket(destination_bucket)\n        destination_object = source_bucket.copy_blob(\n            blob=source_object,\n            destination_bucket=destination_bucket,\n            new_name=destination_object)\n\n        self.log.info('Object %s in bucket %s copied to object %s in bucket %s',\n                      source_object.name, source_bucket.name,\n                      destination_object.name, destination_bucket.name)", "gt": ""}
{"prefix": "def rewrite(self, source_bucket, source_object, destination_bucket,\n                destination_object=None):\n        \"\"\"\n        Has the same functionality as copy, except that will work on files\n        over 5 TB, as well as when copying between locations and/or storage\n        classes.\n\n        destination_object can be omitted, in which case source_object is used.\n\n        :param source_bucket: The bucket of the object to copy from.\n        :type source_bucket: str\n        :param source_object: The object to copy.\n        :type source_object: str\n        :param destination_bucket: The destination of the object to copied to.\n        :type destination_bucket: str\n        :param destination_object: The (renamed) path of the object if given.", "suffix": "        :type destination_object: str\n        \"\"\"\n        destination_object = destination_object or source_object\n        if (source_bucket == destination_bucket and\n                source_object == destination_object):\n            raise ValueError(\n                'Either source/destination bucket or source/destination object '\n                'must be different, not both the same: bucket=%s, object=%s' %\n                (source_bucket, source_object))\n        if not source_bucket or not source_object:\n            raise ValueError('source_bucket and source_object cannot be empty.')\n\n        client = self.get_conn()\n        source_bucket = client.get_bucket(bucket_name=source_bucket)\n        source_object = source_bucket.blob(blob_name=source_object)\n        destination_bucket = client.get_bucket(bucket_name=destination_bucket)\n\n        token, bytes_rewritten, total_bytes = destination_bucket.blob(\n            blob_name=destination_object).rewrite(\n            source=source_object\n        )\n\n        self.log.info('Total Bytes: %s | Bytes Written: %s',\n                      total_bytes, bytes_rewritten)\n\n        while token is not None:\n            token, bytes_rewritten, total_bytes = destination_bucket.blob(\n                blob_name=destination_object).rewrite(\n                source=source_object, token=token\n            )\n\n            self.log.info('Total Bytes: %s | Bytes Written: %s',\n                          total_bytes, bytes_rewritten)\n        self.log.info('Object %s in bucket %s copied to object %s in bucket %s',\n                      source_object.name, source_bucket.name,\n                      destination_object, destination_bucket.name)", "gt": "            Can be omitted; then the same name is used."}
{"prefix": "def download(self, bucket_name, object_name, filename=None):\n        \"\"\"\n        Get a file from Google Cloud Storage.\n\n        :param bucket_name: The bucket to fetch from.\n        :type bucket_name: str\n        :param object_name: The object to fetch.\n        :type object_name: str", "suffix": "        :type filename: str\n        \"\"\"\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n\n        if filename:\n            blob.download_to_filename(filename)\n            self.log.info('File downloaded to %s', filename)\n\n        return blob.download_as_string()", "gt": "        :param filename: If set, a local file path where the file should be written to."}
{"prefix": "def upload(self, bucket_name, object_name, filename,\n               mime_type='application/octet-stream', gzip=False):\n        \"\"\"\n        Uploads a local file to Google Cloud Storage.\n\n        :param bucket_name: The bucket to upload to.\n        :type bucket_name: str\n        :param object_name: The object name to set when uploading the local file.\n        :type object_name: str\n        :param filename: The local file path to the file to be uploaded.\n        :type filename: str\n        :param mime_type: The MIME type to set when uploading the file.\n        :type mime_type: str\n        :param gzip: Option to compress file for upload\n        :type gzip: bool\n        \"\"\"\n\n        if gzip:\n            filename_gz = filename + '.gz'\n\n            with open(filename, 'rb') as f_in:\n                with gz.open(filename_gz, 'wb') as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n                    filename = filename_gz\n\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n        blob.upload_from_filename(filename=filename,\n                                  content_type=mime_type)\n\n        if gzip:\n            os.remove(filename)", "suffix": "", "gt": "        self.log.info('File %s uploaded to %s in %s bucket', filename, object_name, bucket_name)"}
{"prefix": "def exists(self, bucket_name, object_name):\n        \"\"\"", "suffix": "\n        :param bucket_name: The Google cloud storage bucket where the object is.\n        :type bucket_name: str\n        :param object_name: The name of the blob_name to check in the Google cloud\n            storage bucket.\n        :type object_name: str\n        \"\"\"\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n        return blob.exists()", "gt": "        Checks for the existence of a file in Google Cloud Storage."}
{"prefix": "def is_updated_after(self, bucket_name, object_name, ts):\n        \"\"\"\n        Checks if an blob_name is updated in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the object is.\n        :type bucket_name: str\n        :param object_name: The name of the object to check in the Google cloud\n            storage bucket.", "suffix": "        :param ts: The timestamp to check against.\n        :type ts: datetime.datetime\n        \"\"\"\n        client = self.get_conn()\n        bucket = storage.Bucket(client=client, name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n\n        blob_update_time = blob.updated\n\n        if blob_update_time is not None:\n            import dateutil.tz\n\n            if not ts.tzinfo:\n                ts = ts.replace(tzinfo=dateutil.tz.tzutc())\n\n            self.log.info(\"Verify object date: %s > %s\", blob_update_time, ts)\n\n            if blob_update_time > ts:\n                return True\n\n        return False", "gt": "        :type object_name: str"}
{"prefix": "def delete(self, bucket_name, object_name):\n        \"\"\"\n        Deletes an object from the bucket.\n\n        :param bucket_name: name of the bucket, where the object resides\n        :type bucket_name: str\n        :param object_name: name of the object to delete\n        :type object_name: str", "suffix": "        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n        blob.delete()\n\n        self.log.info('Blob %s deleted.', object_name)", "gt": "        \"\"\""}
{"prefix": "def list(self, bucket_name, versions=None, max_results=None, prefix=None, delimiter=None):\n        \"\"\"\n        List all objects from the bucket with the give string prefix in name\n\n        :param bucket_name: bucket name\n        :type bucket_name: str\n        :param versions: if true, list all versions of the objects\n        :type versions: bool\n        :param max_results: max count of items to return in a single page of responses\n        :type max_results: int\n        :param prefix: prefix string which filters objects whose name begin with\n            this prefix\n        :type prefix: str\n        :param delimiter: filters objects based on the delimiter (for e.g '.csv')\n        :type delimiter: str\n        :return: a stream of object names matching the filtering criteria\n        \"\"\"\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n\n        ids = []\n        pageToken = None\n        while True:\n            blobs = bucket.list_blobs(\n                max_results=max_results,\n                page_token=pageToken,\n                prefix=prefix,\n                delimiter=delimiter,\n                versions=versions\n            )\n\n            blob_names = []\n            for blob in blobs:\n                blob_names.append(blob.name)\n\n            prefixes = blobs.prefixes\n            if prefixes:\n                ids += list(prefixes)", "suffix": "                ids += blob_names\n\n            pageToken = blobs.next_page_token\n            if pageToken is None:\n                # empty next page token\n                break\n        return ids", "gt": "            else:"}
{"prefix": "def get_size(self, bucket_name, object_name):\n        \"\"\"\n        Gets the size of a file in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the blob_name is.\n        :type bucket_name: str\n        :param object_name: The name of the object to check in the Google\n            cloud storage bucket_name.", "suffix": "\n        \"\"\"\n        self.log.info('Checking the file size of object: %s in bucket_name: %s',\n                      object_name,\n                      bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n        blob_size = blob.size\n        self.log.info('The file size of %s is %s bytes.', object_name, blob_size)\n        return blob_size", "gt": "        :type object_name: str"}
{"prefix": "def get_crc32c(self, bucket_name, object_name):\n        \"\"\"\n        Gets the CRC32c checksum of an object in Google Cloud Storage.\n\n        :param bucket_name: The Google cloud storage bucket where the blob_name is.\n        :type bucket_name: str\n        :param object_name: The name of the object to check in the Google cloud\n            storage bucket_name.\n        :type object_name: str\n        \"\"\"\n        self.log.info('Retrieving the crc32c checksum of '\n                      'object_name: %s in bucket_name: %s', object_name, bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)", "suffix": "        blob.reload()\n        blob_crc32c = blob.crc32c\n        self.log.info('The crc32c checksum of %s is %s', object_name, blob_crc32c)\n        return blob_crc32c", "gt": "        blob = bucket.get_blob(blob_name=object_name)"}
{"prefix": "def get_md5hash(self, bucket_name, object_name):\n        \"\"\"\n        Gets the MD5 hash of an object in Google Cloud Storage.\n", "suffix": "        :type bucket_name: str\n        :param object_name: The name of the object to check in the Google cloud\n            storage bucket_name.\n        :type object_name: str\n        \"\"\"\n        self.log.info('Retrieving the MD5 hash of '\n                      'object: %s in bucket: %s', object_name, bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name=bucket_name)\n        blob = bucket.get_blob(blob_name=object_name)\n        blob.reload()\n        blob_md5hash = blob.md5_hash\n        self.log.info('The md5Hash of %s is %s', object_name, blob_md5hash)\n        return blob_md5hash", "gt": "        :param bucket_name: The Google cloud storage bucket where the blob_name is."}
{"prefix": "def create_bucket(self,\n                      bucket_name,\n                      resource=None,\n                      storage_class='MULTI_REGIONAL',\n                      location='US',\n                      project_id=None,\n                      labels=None\n                      ):\n        \"\"\"\n        Creates a new bucket. Google Cloud Storage uses a flat namespace, so\n        you can't create a bucket with a name that is already in use.\n\n        .. seealso::\n            For more information, see Bucket Naming Guidelines:\n            https://cloud.google.com/storage/docs/bucketnaming.html#requirements\n\n        :param bucket_name: The name of the bucket.\n        :type bucket_name: str\n        :param resource: An optional dict with parameters for creating the bucket.\n            For information on available parameters, see Cloud Storage API doc:\n            https://cloud.google.com/storage/docs/json_api/v1/buckets/insert\n        :type resource: dict\n        :param storage_class: This defines how objects in the bucket are stored\n            and determines the SLA and the cost of storage. Values include\n\n            - ``MULTI_REGIONAL``\n            - ``REGIONAL``\n            - ``STANDARD``\n            - ``NEARLINE``\n            - ``COLDLINE``.\n\n            If this value is not specified when the bucket is\n            created, it will default to STANDARD.\n        :type storage_class: str\n        :param location: The location of the bucket.\n            Object data for objects in the bucket resides in physical storage\n            within this region. Defaults to US.\n\n            .. seealso::\n                https://developers.google.com/storage/docs/bucket-locations\n\n        :type location: str\n        :param project_id: The ID of the GCP Project.", "suffix": "        :param labels: User-provided labels, in key/value pairs.\n        :type labels: dict\n        :return: If successful, it returns the ``id`` of the bucket.\n        \"\"\"\n\n        self.log.info('Creating Bucket: %s; Location: %s; Storage Class: %s',\n                      bucket_name, location, storage_class)\n\n        client = self.get_conn()\n        bucket = client.bucket(bucket_name=bucket_name)\n        bucket_resource = resource or {}\n\n        for item in bucket_resource:\n            if item != \"name\":\n                bucket._patch_property(name=item, value=resource[item])\n\n        bucket.storage_class = storage_class\n        bucket.labels = labels or {}\n        bucket.create(project=project_id, location=location)\n        return bucket.id", "gt": "        :type project_id: str"}
{"prefix": "def insert_bucket_acl(self, bucket_name, entity, role, user_project=None):\n        \"\"\"\n        Creates a new ACL entry on the specified bucket_name.\n        See: https://cloud.google.com/storage/docs/json_api/v1/bucketAccessControls/insert\n\n        :param bucket_name: Name of a bucket_name.\n        :type bucket_name: str", "suffix": "            user-userId, user-email, group-groupId, group-email, domain-domain,\n            project-team-projectId, allUsers, allAuthenticatedUsers.\n            See: https://cloud.google.com/storage/docs/access-control/lists#scopes\n        :type entity: str\n        :param role: The access permission for the entity.\n            Acceptable values are: \"OWNER\", \"READER\", \"WRITER\".\n        :type role: str\n        :param user_project: (Optional) The project to be billed for this request.\n            Required for Requester Pays buckets.\n        :type user_project: str\n        \"\"\"\n        self.log.info('Creating a new ACL entry in bucket: %s', bucket_name)\n        client = self.get_conn()\n        bucket = client.bucket(bucket_name=bucket_name)\n        bucket.acl.reload()\n        bucket.acl.entity_from_dict(entity_dict={\"entity\": entity, \"role\": role})\n        if user_project:\n            bucket.acl.user_project = user_project\n        bucket.acl.save()\n\n        self.log.info('A new ACL entry created in bucket: %s', bucket_name)", "gt": "        :param entity: The entity holding the permission, in one of the following forms:"}
{"prefix": "def insert_object_acl(self, bucket_name, object_name, entity, role, user_project=None):\n        \"\"\"\n        Creates a new ACL entry on the specified object.\n        See: https://cloud.google.com/storage/docs/json_api/v1/objectAccessControls/insert\n", "suffix": "        :type bucket_name: str\n        :param object_name: Name of the object. For information about how to URL encode\n            object names to be path safe, see:\n            https://cloud.google.com/storage/docs/json_api/#encoding\n        :type object_name: str\n        :param entity: The entity holding the permission, in one of the following forms:\n            user-userId, user-email, group-groupId, group-email, domain-domain,\n            project-team-projectId, allUsers, allAuthenticatedUsers\n            See: https://cloud.google.com/storage/docs/access-control/lists#scopes\n        :type entity: str\n        :param role: The access permission for the entity.\n            Acceptable values are: \"OWNER\", \"READER\".\n        :type role: str\n        :param user_project: (Optional) The project to be billed for this request.\n            Required for Requester Pays buckets.\n        :type user_project: str\n        \"\"\"\n        self.log.info('Creating a new ACL entry for object: %s in bucket: %s',\n                      object_name, bucket_name)\n        client = self.get_conn()\n        bucket = client.bucket(bucket_name=bucket_name)\n        blob = bucket.blob(object_name)\n        # Reload fetches the current ACL from Cloud Storage.\n        blob.acl.reload()\n        blob.acl.entity_from_dict(entity_dict={\"entity\": entity, \"role\": role})\n        if user_project:\n            blob.acl.user_project = user_project\n        blob.acl.save()\n\n        self.log.info('A new ACL entry created for object: %s in bucket: %s',\n                      object_name, bucket_name)", "gt": "        :param bucket_name: Name of a bucket_name."}
{"prefix": "def compose(self, bucket_name, source_objects, destination_object):\n        \"\"\"\n        Composes a list of existing object into a new object in the same storage bucket_name\n\n        Currently it only supports up to 32 objects that can be concatenated\n        in a single operation\n\n        https://cloud.google.com/storage/docs/json_api/v1/objects/compose\n\n        :param bucket_name: The name of the bucket containing the source objects.\n            This is also the same bucket to store the composed destination object.\n        :type bucket_name: str\n        :param source_objects: The list of source objects that will be composed\n            into a single object.\n        :type source_objects: list\n        :param destination_object: The path of the object if given.\n        :type destination_object: str", "suffix": "\n        if not source_objects or not len(source_objects):\n            raise ValueError('source_objects cannot be empty.')\n\n        if not bucket_name or not destination_object:\n            raise ValueError('bucket_name and destination_object cannot be empty.')\n\n        self.log.info(\"Composing %s to %s in the bucket %s\",\n                      source_objects, destination_object, bucket_name)\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name)\n        destination_blob = bucket.blob(destination_object)\n        destination_blob.compose(\n            sources=[\n                bucket.blob(blob_name=source_object) for source_object in source_objects\n            ])\n\n        self.log.info(\"Completed successfully.\")", "gt": "        \"\"\""}
{"prefix": "", "suffix": "    \"\"\"Return the index, i, in arr that minimizes f(arr[i])\"\"\"\n    m = None\n    i = None\n    for idx, item in enumerate(arr):\n        if item is not None:\n            if m is None or f(item) < m:\n                m = f(item)\n                i = idx\n    return i", "gt": "def argmin(arr, f):"}
{"prefix": "def secondary_training_status_changed(current_job_description, prev_job_description):\n    \"\"\"\n    Returns true if training job's secondary status message has changed.\n\n    :param current_job_description: Current job description, returned from DescribeTrainingJob call.\n    :type current_job_description: dict", "suffix": "    :type prev_job_description: dict\n\n    :return: Whether the secondary status message of a training job changed or not.\n    \"\"\"\n    current_secondary_status_transitions = current_job_description.get('SecondaryStatusTransitions')\n    if current_secondary_status_transitions is None or len(current_secondary_status_transitions) == 0:\n        return False\n\n    prev_job_secondary_status_transitions = prev_job_description.get('SecondaryStatusTransitions') \\\n        if prev_job_description is not None else None\n\n    last_message = prev_job_secondary_status_transitions[-1]['StatusMessage'] \\\n        if prev_job_secondary_status_transitions is not None \\\n        and len(prev_job_secondary_status_transitions) > 0 else ''\n\n    message = current_job_description['SecondaryStatusTransitions'][-1]['StatusMessage']\n\n    return message != last_message", "gt": "    :param prev_job_description: Previous job description, returned from DescribeTrainingJob call."}
{"prefix": "def secondary_training_status_message(job_description, prev_description):\n    \"\"\"", "suffix": "\n    :param job_description: Returned response from DescribeTrainingJob call\n    :type job_description: dict\n    :param prev_description: Previous job description from DescribeTrainingJob call\n    :type prev_description: dict\n\n    :return: Job status string to be printed.\n    \"\"\"\n\n    if job_description is None or job_description.get('SecondaryStatusTransitions') is None\\\n            or len(job_description.get('SecondaryStatusTransitions')) == 0:\n        return ''\n\n    prev_description_secondary_transitions = prev_description.get('SecondaryStatusTransitions')\\\n        if prev_description is not None else None\n    prev_transitions_num = len(prev_description['SecondaryStatusTransitions'])\\\n        if prev_description_secondary_transitions is not None else 0\n    current_transitions = job_description['SecondaryStatusTransitions']\n\n    transitions_to_print = current_transitions[-1:] if len(current_transitions) == prev_transitions_num else \\\n        current_transitions[prev_transitions_num - len(current_transitions):]\n\n    status_strs = []\n    for transition in transitions_to_print:\n        message = transition['StatusMessage']\n        time_str = timezone.convert_to_utc(job_description['LastModifiedTime']).strftime('%Y-%m-%d %H:%M:%S')\n        status_strs.append('{} {} - {}'.format(time_str, transition['Status'], message))\n\n    return '\\n'.join(status_strs)", "gt": "    Returns a string contains start time and the secondary training job status message."}
{"prefix": "def tar_and_s3_upload(self, path, key, bucket):\n        \"\"\"\n        Tar the local file or directory and upload to s3\n\n        :param path: local file or directory\n        :type path: str\n        :param key: s3 key\n        :type key: str", "suffix": "        :type bucket: str\n        :return: None\n        \"\"\"\n        with tempfile.TemporaryFile() as temp_file:\n            if os.path.isdir(path):\n                files = [os.path.join(path, name) for name in os.listdir(path)]\n            else:\n                files = [path]\n            with tarfile.open(mode='w:gz', fileobj=temp_file) as tar_file:\n                for f in files:\n                    tar_file.add(f, arcname=os.path.basename(f))\n            temp_file.seek(0)\n            self.s3_hook.load_file_obj(temp_file, key, bucket, replace=True)", "gt": "        :param bucket: s3 bucket"}
{"prefix": "def configure_s3_resources(self, config):\n        \"\"\"\n        Extract the S3 operations from the configuration and execute them.\n\n        :param config: config of SageMaker operation", "suffix": "        :rtype: dict\n        \"\"\"\n        s3_operations = config.pop('S3Operations', None)\n\n        if s3_operations is not None:\n            create_bucket_ops = s3_operations.get('S3CreateBucket', [])\n            upload_ops = s3_operations.get('S3Upload', [])\n            for op in create_bucket_ops:\n                self.s3_hook.create_bucket(bucket_name=op['Bucket'])\n            for op in upload_ops:\n                if op['Tar']:\n                    self.tar_and_s3_upload(op['Path'], op['Key'],\n                                           op['Bucket'])\n                else:\n                    self.s3_hook.load_file(op['Path'], op['Key'],\n                                           op['Bucket'])", "gt": "        :type config: dict"}
{"prefix": "def check_s3_url(self, s3url):\n        \"\"\"\n        Check if an S3 URL exists\n\n        :param s3url: S3 url\n        :type s3url: str\n        :rtype: bool\n        \"\"\"\n        bucket, key = S3Hook.parse_s3_url(s3url)", "suffix": "            raise AirflowException(\n                \"The input S3 Bucket {} does not exist \".format(bucket))\n        if key and not self.s3_hook.check_for_key(key=key, bucket_name=bucket)\\\n           and not self.s3_hook.check_for_prefix(\n                prefix=key, bucket_name=bucket, delimiter='/'):\n            # check if s3 key exists in the case user provides a single file\n            # or if s3 prefix exists in the case user provides multiple files in\n            # a prefix\n            raise AirflowException(\"The input S3 Key \"\n                                   \"or Prefix {} does not exist in the Bucket {}\"\n                                   .format(s3url, bucket))\n        return True", "gt": "        if not self.s3_hook.check_for_bucket(bucket_name=bucket):"}
{"prefix": "", "suffix": "        \"\"\"\n        Establish an AWS connection for retrieving logs during training\n\n        :rtype: CloudWatchLogs.Client\n        \"\"\"\n        config = botocore.config.Config(retries={'max_attempts': 15})\n        return self.get_client_type('logs', config=config)", "gt": "def get_log_conn(self):"}
{"prefix": "def create_training_job(self, config, wait_for_completion=True, print_log=True,\n                            check_interval=30, max_ingestion_time=None):\n        \"\"\"\n        Create a training job\n\n        :param config: the config for training\n        :type config: dict\n        :param wait_for_completion: if the program should keep running until job finishes\n        :type wait_for_completion: bool\n        :param check_interval: the time interval in seconds which the operator\n            will check the status of any SageMaker job\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to", "suffix": "        :type max_ingestion_time: int\n        :return: A response to training job creation\n        \"\"\"\n\n        self.check_training_config(config)\n\n        response = self.get_conn().create_training_job(**config)\n        if print_log:\n            self.check_training_status_with_log(config['TrainingJobName'],\n                                                self.non_terminal_states,\n                                                self.failed_states,\n                                                wait_for_completion,\n                                                check_interval, max_ingestion_time\n                                                )\n        elif wait_for_completion:\n            describe_response = self.check_status(config['TrainingJobName'],\n                                                  'TrainingJobStatus',\n                                                  self.describe_training_job,\n                                                  check_interval, max_ingestion_time\n                                                  )\n\n            billable_time = \\\n                (describe_response['TrainingEndTime'] - describe_response['TrainingStartTime']) * \\\n                describe_response['ResourceConfig']['InstanceCount']\n            self.log.info('Billable seconds:{}'.format(int(billable_time.total_seconds()) + 1))\n\n        return response", "gt": "            None implies no timeout for any SageMaker job."}
{"prefix": "def create_tuning_job(self, config, wait_for_completion=True,\n                          check_interval=30, max_ingestion_time=None):\n        \"\"\"\n        Create a tuning job\n\n        :param config: the config for tuning\n        :type config: dict\n        :param wait_for_completion: if the program should keep running until job finishes\n        :type wait_for_completion: bool\n        :param check_interval: the time interval in seconds which the operator\n            will check the status of any SageMaker job\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :return: A response to tuning job creation\n        \"\"\"\n\n        self.check_tuning_config(config)\n\n        response = self.get_conn().create_hyper_parameter_tuning_job(**config)\n        if wait_for_completion:\n            self.check_status(config['HyperParameterTuningJobName'],\n                              'HyperParameterTuningJobStatus',\n                              self.describe_tuning_job,\n                              check_interval, max_ingestion_time\n                              )", "suffix": "", "gt": "        return response"}
{"prefix": "def create_transform_job(self, config, wait_for_completion=True,\n                             check_interval=30, max_ingestion_time=None):\n        \"\"\"\n        Create a transform job", "suffix": "        :param config: the config for transform job\n        :type config: dict\n        :param wait_for_completion: if the program should keep running until job finishes\n        :type wait_for_completion: bool\n        :param check_interval: the time interval in seconds which the operator\n            will check the status of any SageMaker job\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :return: A response to transform job creation\n        \"\"\"\n\n        self.check_s3_url(config['TransformInput']['DataSource']['S3DataSource']['S3Uri'])\n\n        response = self.get_conn().create_transform_job(**config)\n        if wait_for_completion:\n            self.check_status(config['TransformJobName'],\n                              'TransformJobStatus',\n                              self.describe_transform_job,\n                              check_interval, max_ingestion_time\n                              )\n        return response", "gt": ""}
{"prefix": "def create_endpoint(self, config, wait_for_completion=True,\n                        check_interval=30, max_ingestion_time=None):", "suffix": "        Create an endpoint\n\n        :param config: the config for endpoint\n        :type config: dict\n        :param wait_for_completion: if the program should keep running until job finishes\n        :type wait_for_completion: bool\n        :param check_interval: the time interval in seconds which the operator\n            will check the status of any SageMaker job\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :return: A response to endpoint creation\n        \"\"\"\n\n        response = self.get_conn().create_endpoint(**config)\n        if wait_for_completion:\n            self.check_status(config['EndpointName'],\n                              'EndpointStatus',\n                              self.describe_endpoint,\n                              check_interval, max_ingestion_time,\n                              non_terminal_states=self.endpoint_non_terminal_states\n                              )\n        return response", "gt": "        \"\"\""}
{"prefix": "def describe_training_job_with_log(self, job_name, positions, stream_names,\n                                       instance_count, state, last_description,", "suffix": "        \"\"\"\n        Return the training job info associated with job_name and print CloudWatch logs\n        \"\"\"\n        log_group = '/aws/sagemaker/TrainingJobs'\n\n        if len(stream_names) < instance_count:\n            # Log streams are created whenever a container starts writing to stdout/err, so this list\n            # may be dynamic until we have a stream for every instance.\n            logs_conn = self.get_log_conn()\n            try:\n                streams = logs_conn.describe_log_streams(\n                    logGroupName=log_group,\n                    logStreamNamePrefix=job_name + '/',\n                    orderBy='LogStreamName',\n                    limit=instance_count\n                )\n                stream_names = [s['logStreamName'] for s in streams['logStreams']]\n                positions.update([(s, Position(timestamp=0, skip=0))\n                                  for s in stream_names if s not in positions])\n            except logs_conn.exceptions.ResourceNotFoundException:\n                # On the very first training job run on an account, there's no log group until\n                # the container starts logging, so ignore any errors thrown about that\n                pass\n\n        if len(stream_names) > 0:\n            for idx, event in self.multi_stream_iter(log_group, stream_names, positions):\n                self.log.info(event['message'])\n                ts, count = positions[stream_names[idx]]\n                if event['timestamp'] == ts:\n                    positions[stream_names[idx]] = Position(timestamp=ts, skip=count + 1)\n                else:\n                    positions[stream_names[idx]] = Position(timestamp=event['timestamp'], skip=1)\n\n        if state == LogState.COMPLETE:\n            return state, last_description, last_describe_job_call\n\n        if state == LogState.JOB_COMPLETE:\n            state = LogState.COMPLETE\n        elif time.time() - last_describe_job_call >= 30:\n            description = self.describe_training_job(job_name)\n            last_describe_job_call = time.time()\n\n            if secondary_training_status_changed(description, last_description):\n                self.log.info(secondary_training_status_message(description, last_description))\n                last_description = description\n\n            status = description['TrainingJobStatus']\n\n            if status not in self.non_terminal_states:\n                state = LogState.JOB_COMPLETE\n        return state, last_description, last_describe_job_call", "gt": "                                       last_describe_job_call):"}
{"prefix": "def check_status(self, job_name, key,\n                     describe_function, check_interval,\n                     max_ingestion_time,\n                     non_terminal_states=None):\n        \"\"\"", "suffix": "\n        :param job_name: name of the job to check status\n        :type job_name: str\n        :param key: the key of the response dict\n            that points to the state\n        :type key: str\n        :param describe_function: the function used to retrieve the status\n        :type describe_function: python callable\n        :param args: the arguments for the function\n        :param check_interval: the time interval in seconds which the operator\n            will check the status of any SageMaker job\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :param non_terminal_states: the set of nonterminal states\n        :type non_terminal_states: set\n        :return: response of describe call after job is done\n        \"\"\"\n        if not non_terminal_states:\n            non_terminal_states = self.non_terminal_states\n\n        sec = 0\n        running = True\n\n        while running:\n            time.sleep(check_interval)\n            sec = sec + check_interval\n\n            try:\n                response = describe_function(job_name)\n                status = response[key]\n                self.log.info('Job still running for %s seconds... '\n                              'current status is %s' % (sec, status))\n            except KeyError:\n                raise AirflowException('Could not get status of the SageMaker job')\n            except ClientError:\n                raise AirflowException('AWS request failed, check logs for more info')\n\n            if status in non_terminal_states:\n                running = True\n            elif status in self.failed_states:\n                raise AirflowException('SageMaker job failed because %s' % response['FailureReason'])\n            else:\n                running = False\n\n            if max_ingestion_time and sec > max_ingestion_time:\n                # ensure that the job gets killed if the max ingestion time is exceeded\n                raise AirflowException('SageMaker job took more than %s seconds', max_ingestion_time)\n\n        self.log.info('SageMaker Job Compeleted')\n        response = describe_function(job_name)\n        return response", "gt": "        Check status of a SageMaker job"}
{"prefix": "def check_training_status_with_log(self, job_name, non_terminal_states, failed_states,\n                                       wait_for_completion, check_interval, max_ingestion_time):\n        \"\"\"\n        Display the logs for a given training job, optionally tailing them until the\n        job is complete.\n\n        :param job_name: name of the training job to check status and display logs for\n        :type job_name: str\n        :param non_terminal_states: the set of non_terminal states\n        :type non_terminal_states: set\n        :param failed_states: the set of failed states\n        :type failed_states: set\n        :param wait_for_completion: Whether to keep looking for new log entries\n            until the job completes\n        :type wait_for_completion: bool\n        :param check_interval: The interval in seconds between polling for new log entries and job completion\n        :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds. Any\n            SageMaker jobs that run longer than this will fail. Setting this to\n            None implies no timeout for any SageMaker job.\n        :type max_ingestion_time: int\n        :return: None\n        \"\"\"\n\n        sec = 0", "suffix": "        self.log.info(secondary_training_status_message(description, None))\n        instance_count = description['ResourceConfig']['InstanceCount']\n        status = description['TrainingJobStatus']\n\n        stream_names = []  # The list of log streams\n        positions = {}     # The current position in each stream, map of stream name -> position\n\n        job_already_completed = status not in non_terminal_states\n\n        state = LogState.TAILING if wait_for_completion and not job_already_completed else LogState.COMPLETE\n\n        # The loop below implements a state machine that alternates between checking the job status and\n        # reading whatever is available in the logs at this point. Note, that if we were called with\n        # wait_for_completion == False, we never check the job status.\n        #\n        # If wait_for_completion == TRUE and job is not completed, the initial state is TAILING\n        # If wait_for_completion == FALSE, the initial state is COMPLETE\n        # (doesn't matter if the job really is complete).\n        #\n        # The state table:\n        #\n        # STATE               ACTIONS                        CONDITION             NEW STATE\n        # ----------------    ----------------               -----------------     ----------------\n        # TAILING             Read logs, Pause, Get status   Job complete          JOB_COMPLETE\n        #                                                    Else                  TAILING\n        # JOB_COMPLETE        Read logs, Pause               Any                   COMPLETE\n        # COMPLETE            Read logs, Exit                                      N/A\n        #\n        # Notes:\n        # - The JOB_COMPLETE state forces us to do an extra pause and read any items that\n        # got to Cloudwatch after the job was marked complete.\n        last_describe_job_call = time.time()\n        last_description = description\n\n        while True:\n            time.sleep(check_interval)\n            sec = sec + check_interval\n\n            state, last_description, last_describe_job_call = \\\n                self.describe_training_job_with_log(job_name, positions, stream_names,\n                                                    instance_count, state, last_description,\n                                                    last_describe_job_call)\n            if state == LogState.COMPLETE:\n                break\n\n            if max_ingestion_time and sec > max_ingestion_time:\n                # ensure that the job gets killed if the max ingestion time is exceeded\n                raise AirflowException('SageMaker job took more than %s seconds', max_ingestion_time)\n\n        if wait_for_completion:\n            status = last_description['TrainingJobStatus']\n            if status in failed_states:\n                reason = last_description.get('FailureReason', '(No reason provided)')\n                raise AirflowException('Error training {}: {} Reason: {}'.format(job_name, status, reason))\n            billable_time = (last_description['TrainingEndTime'] - last_description['TrainingStartTime']) \\\n                * instance_count\n            self.log.info('Billable seconds:{}'.format(int(billable_time.total_seconds()) + 1))", "gt": "        description = self.describe_training_job(job_name)"}
{"prefix": "def execute(self, context):\n        \"\"\"Execute the python dataflow job.\"\"\"\n        bucket_helper = GoogleCloudBucketHelper(\n            self.gcp_conn_id, self.delegate_to)\n        self.py_file = bucket_helper.google_cloud_to_local(self.py_file)", "suffix": "                            delegate_to=self.delegate_to,\n                            poll_sleep=self.poll_sleep)\n        dataflow_options = self.dataflow_default_options.copy()\n        dataflow_options.update(self.options)\n        # Convert argument names from lowerCamelCase to snake case.\n        camel_to_snake = lambda name: re.sub(\n            r'[A-Z]', lambda x: '_' + x.group(0).lower(), name)\n        formatted_options = {camel_to_snake(key): dataflow_options[key]\n                             for key in dataflow_options}\n        hook.start_python_dataflow(\n            self.job_name, formatted_options,\n            self.py_file, self.py_options)", "gt": "        hook = DataFlowHook(gcp_conn_id=self.gcp_conn_id,"}
{"prefix": "def google_cloud_to_local(self, file_name):\n        \"\"\"\n        Checks whether the file specified by file_name is stored in Google Cloud\n        Storage (GCS), if so, downloads the file and saves it locally. The full\n        path of the saved file will be returned. Otherwise the local file_name\n        will be returned immediately.\n\n        :param file_name: The full path of input file.\n        :type file_name: str\n        :return: The full path of local file.\n        :rtype: str\n        \"\"\"\n        if not file_name.startswith('gs://'):\n            return file_name\n", "suffix": "        # then split the remaining by path delimiter '/'.\n        path_components = file_name[self.GCS_PREFIX_LENGTH:].split('/')\n        if len(path_components) < 2:\n            raise Exception(\n                'Invalid Google Cloud Storage (GCS) object path: {}'\n                .format(file_name))\n\n        bucket_id = path_components[0]\n        object_id = '/'.join(path_components[1:])\n        local_file = '/tmp/dataflow{}-{}'.format(str(uuid.uuid4())[:8],\n                                                 path_components[-1])\n        self._gcs_hook.download(bucket_id, object_id, local_file)\n\n        if os.stat(local_file).st_size > 0:\n            return local_file\n        raise Exception(\n            'Failed to download Google Cloud Storage (GCS) object: {}'\n            .format(file_name))", "gt": "        # Extracts bucket_id and object_id by first removing 'gs://' prefix and"}
{"prefix": "def run_migrations_offline():\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"", "suffix": "        url=settings.SQL_ALCHEMY_CONN, target_metadata=target_metadata,\n        literal_binds=True, compare_type=COMPARE_TYPE)\n\n    with context.begin_transaction():\n        context.run_migrations()", "gt": "    context.configure("}
{"prefix": "def run_migrations_online():\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    connectable = settings.engine\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            transaction_per_migration=True,\n            target_metadata=target_metadata,\n            compare_type=COMPARE_TYPE,\n        )", "suffix": "        with context.begin_transaction():\n            context.run_migrations()", "gt": ""}
{"prefix": "def delete_instance(self, instance_id, project_id=None):\n        \"\"\"\n        Deletes the specified Cloud Bigtable instance.\n        Raises google.api_core.exceptions.NotFound if the Cloud Bigtable instance does\n        not exist.\n\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            BigTable exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Bigtable instance.\n        :type instance_id: str\n        \"\"\"\n        instance = self.get_instance(instance_id=instance_id, project_id=project_id)\n        if instance:\n            instance.delete()", "suffix": "            self.log.info(\"The instance '%s' does not exist in project '%s'. Exiting\", instance_id,\n                          project_id)", "gt": "        else:"}
{"prefix": "def create_instance(self,\n                        instance_id,\n                        main_cluster_id,\n                        main_cluster_zone,\n                        project_id=None,\n                        replica_cluster_id=None,\n                        replica_cluster_zone=None,\n                        instance_display_name=None,\n                        instance_type=enums.Instance.Type.TYPE_UNSPECIFIED,\n                        instance_labels=None,\n                        cluster_nodes=None,\n                        cluster_storage_type=enums.StorageType.STORAGE_TYPE_UNSPECIFIED,\n                        timeout=None):\n        \"\"\"\n        Creates new instance.\n\n        :type instance_id: str\n        :param instance_id: The ID for the new instance.\n        :type main_cluster_id: str\n        :param main_cluster_id: The ID for main cluster for the new instance.\n        :type main_cluster_zone: str\n        :param main_cluster_zone: The zone for main cluster.\n            See https://cloud.google.com/bigtable/docs/locations for more details.\n        :type project_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            BigTable exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type replica_cluster_id: str\n        :param replica_cluster_id: (optional) The ID for replica cluster for the new\n            instance.\n        :type replica_cluster_zone: str\n        :param replica_cluster_zone: (optional)  The zone for replica cluster.\n        :type instance_type: enums.Instance.Type\n        :param instance_type: (optional) The type of the instance.\n        :type instance_display_name: str\n        :param instance_display_name: (optional) Human-readable name of the instance.\n                Defaults to ``instance_id``.\n        :type instance_labels: dict\n        :param instance_labels: (optional) Dictionary of labels to associate with the\n            instance.\n        :type cluster_nodes: int\n        :param cluster_nodes: (optional) Number of nodes for cluster.\n        :type cluster_storage_type: enums.StorageType\n        :param cluster_storage_type: (optional) The type of storage.\n        :type timeout: int\n        :param timeout: (optional) timeout (in seconds) for instance creation.\n                        If None is not specified, Operator will wait indefinitely.\n        \"\"\"\n        cluster_storage_type = enums.StorageType(cluster_storage_type)\n        instance_type = enums.Instance.Type(instance_type)\n\n        instance = Instance(\n            instance_id,\n            self._get_client(project_id=project_id),", "suffix": "            instance_type,\n            instance_labels,\n        )\n\n        clusters = [\n            instance.cluster(\n                main_cluster_id,\n                main_cluster_zone,\n                cluster_nodes,\n                cluster_storage_type\n            )\n        ]\n        if replica_cluster_id and replica_cluster_zone:\n            clusters.append(instance.cluster(\n                replica_cluster_id,\n                replica_cluster_zone,\n                cluster_nodes,\n                cluster_storage_type\n            ))\n        operation = instance.create(\n            clusters=clusters\n        )\n        operation.result(timeout)\n        return instance", "gt": "            instance_display_name,"}
{"prefix": "def create_table(instance,\n                     table_id,\n                     initial_split_keys=None,\n                     column_families=None):\n        \"\"\"\n        Creates the specified Cloud Bigtable table.\n        Raises ``google.api_core.exceptions.AlreadyExists`` if the table exists.\n\n        :type instance: Instance\n        :param instance: The Cloud Bigtable instance that owns the table.\n        :type table_id: str\n        :param table_id: The ID of the table to create in Cloud Bigtable.\n        :type initial_split_keys: list\n        :param initial_split_keys: (Optional) A list of row keys in bytes to use to\n            initially split the table.\n        :type column_families: dict\n        :param column_families: (Optional) A map of columns to create. The key is the\n            column_id str, and the value is a", "suffix": "        \"\"\"\n        if column_families is None:\n            column_families = {}\n        if initial_split_keys is None:\n            initial_split_keys = []\n        table = Table(table_id, instance)\n        table.create(initial_split_keys, column_families)", "gt": "            :class:`google.cloud.bigtable.column_family.GarbageCollectionRule`."}
{"prefix": "def delete_table(self, instance_id, table_id, project_id=None):\n        \"\"\"\n        Deletes the specified table in Cloud Bigtable.\n        Raises google.api_core.exceptions.NotFound if the table does not exist.\n\n        :type instance_id: str\n        :param instance_id: The ID of the Cloud Bigtable instance.\n        :type table_id: str\n        :param table_id: The ID of the table in Cloud Bigtable.", "suffix": "        :param project_id: Optional, Google Cloud Platform project ID where the\n            BigTable exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        \"\"\"\n        table = self.get_instance(instance_id=instance_id, project_id=project_id).table(table_id=table_id)\n        table.delete()", "gt": "        :type project_id: str"}
{"prefix": "def update_cluster(instance, cluster_id, nodes):\n        \"\"\"\n        Updates number of nodes in the specified Cloud Bigtable cluster.\n        Raises google.api_core.exceptions.NotFound if the cluster does not exist.\n\n        :type instance: Instance\n        :param instance: The Cloud Bigtable instance that owns the cluster.\n        :type cluster_id: str", "suffix": "        :type nodes: int\n        :param nodes: The desired number of nodes.\n        \"\"\"\n        cluster = Cluster(cluster_id, instance)\n        cluster.serve_nodes = nodes\n        cluster.update()", "gt": "        :param cluster_id: The ID of the cluster."}
{"prefix": "", "suffix": "        \"\"\"\n        This function creates the command list from available information\n        \"\"\"\n        conn = self.conn\n        hive_bin = 'hive'\n        cmd_extra = []\n\n        if self.use_beeline:\n            hive_bin = 'beeline'\n            jdbc_url = \"jdbc:hive2://{host}:{port}/{schema}\".format(\n                host=conn.host, port=conn.port, schema=conn.schema)\n            if configuration.conf.get('core', 'security') == 'kerberos':\n                template = conn.extra_dejson.get(\n                    'principal', \"hive/_HOST@EXAMPLE.COM\")\n                if \"_HOST\" in template:\n                    template = utils.replace_hostname_pattern(\n                        utils.get_components(template))\n\n                proxy_user = \"\"  # noqa\n                if conn.extra_dejson.get('proxy_user') == \"login\" and conn.login:\n                    proxy_user = \"hive.server2.proxy.user={0}\".format(conn.login)\n                elif conn.extra_dejson.get('proxy_user') == \"owner\" and self.run_as:\n                    proxy_user = \"hive.server2.proxy.user={0}\".format(self.run_as)\n\n                jdbc_url += \";principal={template};{proxy_user}\".format(\n                    template=template, proxy_user=proxy_user)\n            elif self.auth:\n                jdbc_url += \";auth=\" + self.auth\n\n            jdbc_url = '\"{}\"'.format(jdbc_url)\n\n            cmd_extra += ['-u', jdbc_url]\n            if conn.login:\n                cmd_extra += ['-n', conn.login]\n            if conn.password:\n                cmd_extra += ['-p', conn.password]\n\n        hive_params_list = self.hive_cli_params.split()\n\n        return [hive_bin] + cmd_extra + hive_params_list", "gt": "def _prepare_cli_cmd(self):"}
{"prefix": "def _prepare_hiveconf(d):\n        \"\"\"\n        This function prepares a list of hiveconf params\n        from a dictionary of key value pairs.\n\n        :param d:\n        :type d: dict\n\n        >>> hh = HiveCliHook()\n        >>> hive_conf = {\"hive.exec.dynamic.partition\": \"true\",\n        ... \"hive.exec.dynamic.partition.mode\": \"nonstrict\"}\n        >>> hh._prepare_hiveconf(hive_conf)", "suffix": " \"-hiveconf\", \"hive.exec.dynamic.partition.mode=nonstrict\"]\n        \"\"\"\n        if not d:\n            return []\n        return as_flattened_list(\n            zip([\"-hiveconf\"] * len(d),\n                [\"{}={}\".format(k, v) for k, v in d.items()])\n        )", "gt": "        [\"-hiveconf\", \"hive.exec.dynamic.partition=true\",\\"}
{"prefix": "def run_cli(self, hql, schema=None, verbose=True, hive_conf=None):\n        \"\"\"\n        Run an hql statement using the hive cli. If hive_conf is specified\n        it should be a dict and the entries will be set as key/value pairs\n        in HiveConf\n\n\n        :param hive_conf: if specified these key value pairs will be passed\n            to hive as ``-hiveconf \"key\"=\"value\"``. Note that they will be\n            passed after the ``hive_cli_params`` and thus will override\n            whatever values are specified in the database.\n        :type hive_conf: dict\n\n        >>> hh = HiveCliHook()\n        >>> result = hh.run_cli(\"USE airflow;\")\n        >>> (\"OK\" in result)\n        True\n        \"\"\"\n        conn = self.conn\n        schema = schema or conn.schema\n        if schema:\n            hql = \"USE {schema};\\n{hql}\".format(schema=schema, hql=hql)\n\n        with TemporaryDirectory(prefix='airflow_hiveop_') as tmp_dir:\n            with NamedTemporaryFile(dir=tmp_dir) as f:\n                hql = hql + '\\n'\n                f.write(hql.encode('UTF-8'))\n                f.flush()\n                hive_cmd = self._prepare_cli_cmd()\n                env_context = get_context_from_env_var()\n                # Only extend the hive_conf if it is defined.\n                if hive_conf:\n                    env_context.update(hive_conf)\n                hive_conf_params = self._prepare_hiveconf(env_context)\n                if self.mapred_queue:\n                    hive_conf_params.extend(\n                        ['-hiveconf',\n                         'mapreduce.job.queuename={}'\n                         .format(self.mapred_queue),\n                         '-hiveconf',\n                         'mapred.job.queue.name={}'\n                         .format(self.mapred_queue),\n                         '-hiveconf',\n                         'tez.job.queue.name={}'\n                         .format(self.mapred_queue)\n                         ])\n", "suffix": "                    hive_conf_params.extend(\n                        ['-hiveconf',\n                         'mapreduce.job.priority={}'\n                         .format(self.mapred_queue_priority)])\n\n                if self.mapred_job_name:\n                    hive_conf_params.extend(\n                        ['-hiveconf',\n                         'mapred.job.name={}'\n                         .format(self.mapred_job_name)])\n\n                hive_cmd.extend(hive_conf_params)\n                hive_cmd.extend(['-f', f.name])\n\n                if verbose:\n                    self.log.info(\"%s\", \" \".join(hive_cmd))\n                sp = subprocess.Popen(\n                    hive_cmd,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.STDOUT,\n                    cwd=tmp_dir,\n                    close_fds=True)\n                self.sp = sp\n                stdout = ''\n                while True:\n                    line = sp.stdout.readline()\n                    if not line:\n                        break\n                    stdout += line.decode('UTF-8')\n                    if verbose:\n                        self.log.info(line.decode('UTF-8').strip())\n                sp.wait()\n\n                if sp.returncode:\n                    raise AirflowException(stdout)\n\n                return stdout", "gt": "                if self.mapred_queue_priority:"}
{"prefix": "def load_df(\n            self,\n            df,\n            table,\n            field_dict=None,\n            delimiter=',',\n            encoding='utf8',\n            pandas_kwargs=None, **kwargs):\n        \"\"\"\n        Loads a pandas DataFrame into hive.\n\n        Hive data types will be inferred if not passed but column names will\n        not be sanitized.\n\n        :param df: DataFrame to load into a Hive table\n        :type df: pandas.DataFrame\n        :param table: target Hive table, use dot notation to target a\n            specific database\n        :type table: str\n        :param field_dict: mapping from column name to hive data type.\n            Note that it must be OrderedDict so as to keep columns' order.\n        :type field_dict: collections.OrderedDict\n        :param delimiter: field delimiter in the file\n        :type delimiter: str\n        :param encoding: str encoding to use when writing DataFrame to file\n        :type encoding: str\n        :param pandas_kwargs: passed to DataFrame.to_csv\n        :type pandas_kwargs: dict\n        :param kwargs: passed to self.load_file\n        \"\"\"\n\n        def _infer_field_types_from_df(df):\n            DTYPE_KIND_HIVE_TYPE = {\n                'b': 'BOOLEAN',    # boolean\n                'i': 'BIGINT',     # signed integer\n                'u': 'BIGINT',     # unsigned integer\n                'f': 'DOUBLE',     # floating-point\n                'c': 'STRING',     # complex floating-point\n                'M': 'TIMESTAMP',  # datetime\n                'O': 'STRING',     # object\n                'S': 'STRING',     # (byte-)string\n                'U': 'STRING',     # Unicode\n                'V': 'STRING'      # void\n            }\n\n            d = OrderedDict()\n            for col, dtype in df.dtypes.iteritems():\n                d[col] = DTYPE_KIND_HIVE_TYPE[dtype.kind]\n            return d\n\n        if pandas_kwargs is None:", "suffix": "\n        with TemporaryDirectory(prefix='airflow_hiveop_') as tmp_dir:\n            with NamedTemporaryFile(dir=tmp_dir, mode=\"w\") as f:\n\n                if field_dict is None:\n                    field_dict = _infer_field_types_from_df(df)\n\n                df.to_csv(path_or_buf=f,\n                          sep=delimiter,\n                          header=False,\n                          index=False,\n                          encoding=encoding,\n                          date_format=\"%Y-%m-%d %H:%M:%S\",\n                          **pandas_kwargs)\n                f.flush()\n\n                return self.load_file(filepath=f.name,\n                                      table=table,\n                                      delimiter=delimiter,\n                                      field_dict=field_dict,\n                                      **kwargs)", "gt": "            pandas_kwargs = {}"}
{"prefix": "def load_file(\n            self,\n            filepath,\n            table,\n            delimiter=\",\",\n            field_dict=None,\n            create=True,\n            overwrite=True,\n            partition=None,\n            recreate=False,\n            tblproperties=None):\n        \"\"\"\n        Loads a local file into Hive\n\n        Note that the table generated in Hive uses ``STORED AS textfile``\n        which isn't the most efficient serialization format. If a\n        large amount of data is loaded and/or if the tables gets\n        queried considerably, you may want to use this operator only to\n        stage the data into a temporary table before loading it into its\n        final destination using a ``HiveOperator``.\n\n        :param filepath: local filepath of the file to load", "suffix": "        :param table: target Hive table, use dot notation to target a\n            specific database\n        :type table: str\n        :param delimiter: field delimiter in the file\n        :type delimiter: str\n        :param field_dict: A dictionary of the fields name in the file\n            as keys and their Hive types as values.\n            Note that it must be OrderedDict so as to keep columns' order.\n        :type field_dict: collections.OrderedDict\n        :param create: whether to create the table if it doesn't exist\n        :type create: bool\n        :param overwrite: whether to overwrite the data in table or partition\n        :type overwrite: bool\n        :param partition: target partition as a dict of partition columns\n            and values\n        :type partition: dict\n        :param recreate: whether to drop and recreate the table at every\n            execution\n        :type recreate: bool\n        :param tblproperties: TBLPROPERTIES of the hive table being created\n        :type tblproperties: dict\n        \"\"\"\n        hql = ''\n        if recreate:\n            hql += \"DROP TABLE IF EXISTS {table};\\n\".format(table=table)\n        if create or recreate:\n            if field_dict is None:\n                raise ValueError(\"Must provide a field dict when creating a table\")\n            fields = \",\\n    \".join(\n                [k + ' ' + v for k, v in field_dict.items()])\n            hql += \"CREATE TABLE IF NOT EXISTS {table} (\\n{fields})\\n\".format(\n                table=table, fields=fields)\n            if partition:\n                pfields = \",\\n    \".join(\n                    [p + \" STRING\" for p in partition])\n                hql += \"PARTITIONED BY ({pfields})\\n\".format(pfields=pfields)\n            hql += \"ROW FORMAT DELIMITED\\n\"\n            hql += \"FIELDS TERMINATED BY '{delimiter}'\\n\".format(delimiter=delimiter)\n            hql += \"STORED AS textfile\\n\"\n            if tblproperties is not None:\n                tprops = \", \".join(\n                    [\"'{0}'='{1}'\".format(k, v) for k, v in tblproperties.items()])\n                hql += \"TBLPROPERTIES({tprops})\\n\".format(tprops=tprops)\n        hql += \";\"\n        self.log.info(hql)\n        self.run_cli(hql)\n        hql = \"LOAD DATA LOCAL INPATH '{filepath}' \".format(filepath=filepath)\n        if overwrite:\n            hql += \"OVERWRITE \"\n        hql += \"INTO TABLE {table} \".format(table=table)\n        if partition:\n            pvals = \", \".join(\n                [\"{0}='{1}'\".format(k, v) for k, v in partition.items()])\n            hql += \"PARTITION ({pvals})\".format(pvals=pvals)\n\n        # As a workaround for HIVE-10541, add a newline character\n        # at the end of hql (AIRFLOW-2412).\n        hql += ';\\n'\n\n        self.log.info(hql)\n        self.run_cli(hql)", "gt": "        :type filepath: str"}
{"prefix": "def get_metastore_client(self):\n        \"\"\"\n        Returns a Hive thrift client.\n        \"\"\"\n        import hmsclient\n        from thrift.transport import TSocket, TTransport\n        from thrift.protocol import TBinaryProtocol\n        ms = self.metastore_conn\n        auth_mechanism = ms.extra_dejson.get('authMechanism', 'NOSASL')\n        if configuration.conf.get('core', 'security') == 'kerberos':\n            auth_mechanism = ms.extra_dejson.get('authMechanism', 'GSSAPI')\n            kerberos_service_name = ms.extra_dejson.get('kerberos_service_name', 'hive')\n\n        socket = TSocket.TSocket(ms.host, ms.port)\n        if configuration.conf.get('core', 'security') == 'kerberos' \\\n                and auth_mechanism == 'GSSAPI':\n            try:\n                import saslwrapper as sasl\n            except ImportError:\n                import sasl\n\n            def sasl_factory():\n                sasl_client = sasl.Client()\n                sasl_client.setAttr(\"host\", ms.host)\n                sasl_client.setAttr(\"service\", kerberos_service_name)\n                sasl_client.init()\n                return sasl_client\n\n            from thrift_sasl import TSaslClientTransport\n            transport = TSaslClientTransport(sasl_factory, \"GSSAPI\", socket)\n        else:\n            transport = TTransport.TBufferedTransport(socket)\n\n        protocol = TBinaryProtocol.TBinaryProtocol(transport)\n", "suffix": "", "gt": "        return hmsclient.HMSClient(iprot=protocol)"}
{"prefix": "def check_for_partition(self, schema, table, partition):\n        \"\"\"\n        Checks whether a partition exists\n\n        :param schema: Name of hive schema (database) @table belongs to\n        :type schema: str\n        :param table: Name of hive table @partition belongs to\n        :type schema: str\n        :partition: Expression that matches the partitions to check for\n            (eg `a = 'b' AND c = 'd'`)\n        :type schema: str\n        :rtype: bool\n\n        >>> hh = HiveMetastoreHook()", "suffix": "        >>> hh.check_for_partition('airflow', t, \"ds='2015-01-01'\")\n        True\n        \"\"\"\n        with self.metastore as client:\n            partitions = client.get_partitions_by_filter(\n                schema, table, partition, 1)\n\n        if partitions:\n            return True\n        else:\n            return False", "gt": "        >>> t = 'static_babynames_partitioned'"}
{"prefix": "def check_for_named_partition(self, schema, table, partition_name):\n        \"\"\"\n        Checks whether a partition with a given name exists\n\n        :param schema: Name of hive schema (database) @table belongs to\n        :type schema: str\n        :param table: Name of hive table @partition belongs to\n        :type schema: str\n        :partition: Name of the partitions to check for (eg `a=b/c=d`)\n        :type schema: str\n        :rtype: bool\n\n        >>> hh = HiveMetastoreHook()", "suffix": "        >>> hh.check_for_named_partition('airflow', t, \"ds=2015-01-01\")\n        True\n        >>> hh.check_for_named_partition('airflow', t, \"ds=xxx\")\n        False\n        \"\"\"\n        with self.metastore as client:\n            return client.check_for_named_partition(schema, table, partition_name)", "gt": "        >>> t = 'static_babynames_partitioned'"}
{"prefix": "def get_table(self, table_name, db='default'):\n        \"\"\"Get a metastore table object\n\n        >>> hh = HiveMetastoreHook()\n        >>> t = hh.get_table(db='airflow', table_name='static_babynames')\n        >>> t.tableName\n        'static_babynames'\n        >>> [col.name for col in t.sd.cols]\n        ['state', 'year', 'name', 'gender', 'num']", "suffix": "        if db == 'default' and '.' in table_name:\n            db, table_name = table_name.split('.')[:2]\n        with self.metastore as client:\n            return client.get_table(dbname=db, tbl_name=table_name)", "gt": "        \"\"\""}
{"prefix": "def get_tables(self, db, pattern='*'):", "suffix": "        Get a metastore table object\n        \"\"\"\n        with self.metastore as client:\n            tables = client.get_tables(db_name=db, pattern=pattern)\n            return client.get_table_objects_by_name(db, tables)", "gt": "        \"\"\""}
{"prefix": "def get_partitions(\n            self, schema, table_name, filter=None):\n        \"\"\"\n        Returns a list of all partitions in a table. Works only\n        for tables with less than 32767 (java short max val).\n        For subpartitioned table, the number might easily exceed this.\n\n        >>> hh = HiveMetastoreHook()\n        >>> t = 'static_babynames_partitioned'\n        >>> parts = hh.get_partitions(schema='airflow', table_name=t)", "suffix": "        1\n        >>> parts\n        [{'ds': '2015-01-01'}]\n        \"\"\"\n        with self.metastore as client:\n            table = client.get_table(dbname=schema, tbl_name=table_name)\n            if len(table.partitionKeys) == 0:\n                raise AirflowException(\"The table isn't partitioned\")\n            else:\n                if filter:\n                    parts = client.get_partitions_by_filter(\n                        db_name=schema, tbl_name=table_name,\n                        filter=filter, max_parts=HiveMetastoreHook.MAX_PART_COUNT)\n                else:\n                    parts = client.get_partitions(\n                        db_name=schema, tbl_name=table_name,\n                        max_parts=HiveMetastoreHook.MAX_PART_COUNT)\n\n                pnames = [p.name for p in table.partitionKeys]\n                return [dict(zip(pnames, p.values)) for p in parts]", "gt": "        >>> len(parts)"}
{"prefix": "def _get_max_partition_from_part_specs(part_specs, partition_key, filter_map):\n        \"\"\"", "suffix": "        from part specs. key:value pair in filter_map will be used to\n        filter out partitions.\n\n        :param part_specs: list of partition specs.\n        :type part_specs: list\n        :param partition_key: partition key name.\n        :type partition_key: str\n        :param filter_map: partition_key:partition_value map used for partition filtering,\n                           e.g. {'key1': 'value1', 'key2': 'value2'}.\n                           Only partitions matching all partition_key:partition_value\n                           pairs will be considered as candidates of max partition.\n        :type filter_map: map\n        :return: Max partition or None if part_specs is empty.\n        \"\"\"\n        if not part_specs:\n            return None\n\n        # Assuming all specs have the same keys.\n        if partition_key not in part_specs[0].keys():\n            raise AirflowException(\"Provided partition_key {} \"\n                                   \"is not in part_specs.\".format(partition_key))\n        if filter_map:\n            is_subset = set(filter_map.keys()).issubset(set(part_specs[0].keys()))\n        if filter_map and not is_subset:\n            raise AirflowException(\"Keys in provided filter_map {} \"\n                                   \"are not subset of part_spec keys: {}\"\n                                   .format(', '.join(filter_map.keys()),\n                                           ', '.join(part_specs[0].keys())))\n\n        candidates = [p_dict[partition_key] for p_dict in part_specs\n                      if filter_map is None or\n                      all(item in p_dict.items() for item in filter_map.items())]\n\n        if not candidates:\n            return None\n        else:\n            return max(candidates).encode('utf-8')", "gt": "        Helper method to get max partition of partitions with partition_key"}
{"prefix": "def max_partition(self, schema, table_name, field=None, filter_map=None):\n        \"\"\"\n        Returns the maximum value for all partitions with given field in a table.\n        If only one partition key exist in the table, the key will be used as field.\n        filter_map should be a partition_key:partition_value map and will be used to\n        filter out partitions.\n\n        :param schema: schema name.\n        :type schema: str\n        :param table_name: table name.\n        :type table_name: str\n        :param field: partition key to get max partition from.\n        :type field: str\n        :param filter_map: partition_key:partition_value map used for partition filtering.\n        :type filter_map: map\n\n        >>> hh = HiveMetastoreHook()\n        >>> filter_map = {'ds': '2015-01-01', 'ds': '2014-01-01'}\n        >>> t = 'static_babynames_partitioned'\n        >>> hh.max_partition(schema='airflow',\\\n        ... table_name=t, field='ds', filter_map=filter_map)\n        '2015-01-01'\n        \"\"\"\n        with self.metastore as client:\n            table = client.get_table(dbname=schema, tbl_name=table_name)\n            key_name_set = set(key.name for key in table.partitionKeys)\n            if len(table.partitionKeys) == 1:\n                field = table.partitionKeys[0].name\n            elif not field:", "suffix": "                                       \"value for.\")\n            elif field not in key_name_set:\n                raise AirflowException(\"Provided field is not a partition key.\")\n\n            if filter_map and not set(filter_map.keys()).issubset(key_name_set):\n                raise AirflowException(\"Provided filter_map contains keys \"\n                                       \"that are not partition key.\")\n\n            part_names = \\\n                client.get_partition_names(schema,\n                                           table_name,\n                                           max_parts=HiveMetastoreHook.MAX_PART_COUNT)\n            part_specs = [client.partition_name_to_spec(part_name)\n                          for part_name in part_names]\n\n        return HiveMetastoreHook._get_max_partition_from_part_specs(part_specs,\n                                                                    field,\n                                                                    filter_map)", "gt": "                raise AirflowException(\"Please specify the field you want the max \""}
{"prefix": "def table_exists(self, table_name, db='default'):\n        \"\"\"\n        Check if table exists\n\n        >>> hh = HiveMetastoreHook()\n        >>> hh.table_exists(db='airflow', table_name='static_babynames')\n        True\n        >>> hh.table_exists(db='airflow', table_name='does_not_exist')\n        False\n        \"\"\"", "suffix": "            self.get_table(table_name, db)\n            return True\n        except Exception:\n            return False", "gt": "        try:"}
{"prefix": "def get_conn(self, schema=None):\n        \"\"\"\n        Returns a Hive connection object.\n        \"\"\"", "suffix": "        auth_mechanism = db.extra_dejson.get('authMechanism', 'NONE')\n        if auth_mechanism == 'NONE' and db.login is None:\n            # we need to give a username\n            username = 'airflow'\n        kerberos_service_name = None\n        if configuration.conf.get('core', 'security') == 'kerberos':\n            auth_mechanism = db.extra_dejson.get('authMechanism', 'KERBEROS')\n            kerberos_service_name = db.extra_dejson.get('kerberos_service_name', 'hive')\n\n        # pyhive uses GSSAPI instead of KERBEROS as a auth_mechanism identifier\n        if auth_mechanism == 'GSSAPI':\n            self.log.warning(\n                \"Detected deprecated 'GSSAPI' for authMechanism \"\n                \"for %s. Please use 'KERBEROS' instead\",\n                self.hiveserver2_conn_id\n            )\n            auth_mechanism = 'KERBEROS'\n\n        from pyhive.hive import connect\n        return connect(\n            host=db.host,\n            port=db.port,\n            auth=auth_mechanism,\n            kerberos_service_name=kerberos_service_name,\n            username=db.login or username,\n            password=db.password,\n            database=schema or db.schema or 'default')", "gt": "        db = self.get_connection(self.hiveserver2_conn_id)"}
{"prefix": "def get_results(self, hql, schema='default', fetch_size=None, hive_conf=None):\n        \"\"\"\n        Get results of the provided hql in target schema.\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :param fetch_size: max size of result to fetch.\n        :type fetch_size: int\n        :param hive_conf: hive_conf to execute alone with the hql.\n        :type hive_conf: dict\n        :return: results of hql execution, dict with data (list of results) and header\n        :rtype: dict\n        \"\"\"\n        results_iter = self._get_results(hql, schema,\n                                         fetch_size=fetch_size, hive_conf=hive_conf)", "suffix": "        results = {\n            'data': list(results_iter),\n            'header': header\n        }\n        return results", "gt": "        header = next(results_iter)"}
{"prefix": "def to_csv(\n            self,\n            hql,\n            csv_filepath,\n            schema='default',\n            delimiter=',',\n            lineterminator='\\r\\n',\n            output_header=True,\n            fetch_size=1000,\n            hive_conf=None):\n        \"\"\"\n        Execute hql in target schema and write results to a csv file.\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param csv_filepath: filepath of csv to write results into.\n        :type csv_filepath: str\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :param delimiter: delimiter of the csv file, default to ','.\n        :type delimiter: str\n        :param lineterminator: lineterminator of the csv file.\n        :type lineterminator: str\n        :param output_header: header of the csv file, default to True.\n        :type output_header: bool\n        :param fetch_size: number of result rows to write into the csv file, default to 1000.\n        :type fetch_size: int\n        :param hive_conf: hive_conf to execute alone with the hql.\n        :type hive_conf: dict\n\n        \"\"\"\n\n        results_iter = self._get_results(hql, schema,\n                                         fetch_size=fetch_size, hive_conf=hive_conf)\n        header = next(results_iter)\n        message = None\n\n        i = 0\n        with open(csv_filepath, 'wb') as f:\n            writer = csv.writer(f,\n                                delimiter=delimiter,\n                                lineterminator=lineterminator,\n                                encoding='utf-8')\n            try:\n                if output_header:\n                    self.log.debug('Cursor description is %s', header)\n                    writer.writerow([c[0] for c in header])\n\n                for i, row in enumerate(results_iter, 1):\n                    writer.writerow(row)\n                    if i % fetch_size == 0:\n                        self.log.info(\"Written %s rows so far.\", i)\n            except ValueError as exception:\n                message = str(exception)\n\n        if message:\n            # need to clean up the file first\n            os.remove(csv_filepath)\n            raise ValueError(message)", "suffix": "        self.log.info(\"Done. Loaded a total of %s rows.\", i)", "gt": ""}
{"prefix": "def get_records(self, hql, schema='default', hive_conf=None):\n        \"\"\"\n        Get a set of records from a Hive query.\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :param hive_conf: hive_conf to execute alone with the hql.\n        :type hive_conf: dict\n        :return: result of hive execution\n        :rtype: list\n\n        >>> hh = HiveServer2Hook()", "suffix": "        >>> len(hh.get_records(sql))\n        100\n        \"\"\"\n        return self.get_results(hql, schema=schema, hive_conf=hive_conf)['data']", "gt": "        >>> sql = \"SELECT * FROM airflow.static_babynames LIMIT 100\""}
{"prefix": "", "suffix": "        \"\"\"\n        Get a pandas dataframe from a Hive query\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :return: result of hql execution\n        :rtype: DataFrame\n\n        >>> hh = HiveServer2Hook()\n        >>> sql = \"SELECT * FROM airflow.static_babynames LIMIT 100\"\n        >>> df = hh.get_pandas_df(sql)\n        >>> len(df.index)\n        100\n\n        :return: pandas.DateFrame\n        \"\"\"\n        import pandas as pd\n        res = self.get_results(hql, schema=schema)\n        df = pd.DataFrame(res['data'])\n        df.columns = [c[0] for c in res['header']]\n        return df", "gt": "def get_pandas_df(self, hql, schema='default'):"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Vision.\n\n        :return: Google Cloud Vision client object.\n        :rtype: google.cloud.vision_v1.ProductSearchClient", "suffix": "        if not self._client:\n            self._client = ProductSearchClient(credentials=self._get_credentials())\n        return self._client", "gt": "        \"\"\""}
{"prefix": "def create_product_set(\n        self,\n        location,\n        product_set,\n        project_id=None,\n        product_set_id=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        For the documentation see:\n        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetCreateOperator`\n        \"\"\"\n        client = self.get_conn()\n        parent = ProductSearchClient.location_path(project_id, location)", "suffix": "        response = client.create_product_set(\n            parent=parent,\n            product_set=product_set,\n            product_set_id=product_set_id,\n            retry=retry,\n            timeout=timeout,\n            metadata=metadata,\n        )\n        self.log.info('ProductSet created: %s', response.name if response else '')\n        self.log.debug('ProductSet created:\\n%s', response)\n\n        if not product_set_id:\n            # Product set id was generated by the API\n            product_set_id = self._get_autogenerated_id(response)\n            self.log.info('Extracted autogenerated ProductSet ID from the response: %s', product_set_id)\n\n        return product_set_id", "gt": "        self.log.info('Creating a new ProductSet under the parent: %s', parent)"}
{"prefix": "def get_product_set(\n        self, location, product_set_id, project_id=None, retry=None, timeout=None, metadata=None\n    ):\n        \"\"\"\n        For the documentation see:\n        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetGetOperator`\n        \"\"\"\n        client = self.get_conn()\n        name = ProductSearchClient.product_set_path(project_id, location, product_set_id)\n        self.log.info('Retrieving ProductSet: %s', name)", "suffix": "        self.log.info('ProductSet retrieved.')\n        self.log.debug('ProductSet retrieved:\\n%s', response)\n        return MessageToDict(response)", "gt": "        response = client.get_product_set(name=name, retry=retry, timeout=timeout, metadata=metadata)"}
{"prefix": "def update_product_set(", "suffix": "        product_set,\n        location=None,\n        product_set_id=None,\n        update_mask=None,\n        project_id=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        For the documentation see:\n        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetUpdateOperator`\n        \"\"\"\n        client = self.get_conn()\n        product_set = self.product_set_name_determiner.get_entity_with_name(\n            product_set, product_set_id, location, project_id\n        )\n        self.log.info('Updating ProductSet: %s', product_set.name)\n        response = client.update_product_set(\n            product_set=product_set, update_mask=update_mask, retry=retry, timeout=timeout, metadata=metadata\n        )\n        self.log.info('ProductSet updated: %s', response.name if response else '')\n        self.log.debug('ProductSet updated:\\n%s', response)\n        return MessageToDict(response)", "gt": "        self,"}
{"prefix": "def delete_product_set(\n        self, location, product_set_id, project_id=None, retry=None, timeout=None, metadata=None\n    ):\n        \"\"\"\n        For the documentation see:", "suffix": "        \"\"\"\n        client = self.get_conn()\n        name = ProductSearchClient.product_set_path(project_id, location, product_set_id)\n        self.log.info('Deleting ProductSet: %s', name)\n        client.delete_product_set(name=name, retry=retry, timeout=timeout, metadata=metadata)\n        self.log.info('ProductSet with the name [%s] deleted.', name)", "gt": "        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetDeleteOperator`"}
{"prefix": "def create_product(\n        self, location, product, project_id=None, product_id=None, retry=None, timeout=None, metadata=None\n    ):\n        \"\"\"\n        For the documentation see:\n        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductCreateOperator`\n        \"\"\"\n        client = self.get_conn()\n        parent = ProductSearchClient.location_path(project_id, location)\n        self.log.info('Creating a new Product under the parent: %s', parent)\n        response = client.create_product(\n            parent=parent,\n            product=product,\n            product_id=product_id,\n            retry=retry,\n            timeout=timeout,\n            metadata=metadata,\n        )\n        self.log.info('Product created: %s', response.name if response else '')\n        self.log.debug('Product created:\\n%s', response)\n\n        if not product_id:\n            # Product id was generated by the API\n            product_id = self._get_autogenerated_id(response)\n            self.log.info('Extracted autogenerated Product ID from the response: %s', product_id)\n", "suffix": "", "gt": "        return product_id"}
{"prefix": "def get_product(self, location, product_id, project_id=None, retry=None, timeout=None, metadata=None):\n        \"\"\"\n        For the documentation see:\n        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductGetOperator`\n        \"\"\"\n        client = self.get_conn()\n        name = ProductSearchClient.product_path(project_id, location, product_id)", "suffix": "        response = client.get_product(name=name, retry=retry, timeout=timeout, metadata=metadata)\n        self.log.info('Product retrieved.')\n        self.log.debug('Product retrieved:\\n%s', response)\n        return MessageToDict(response)", "gt": "        self.log.info('Retrieving Product: %s', name)"}
{"prefix": "def update_product(\n        self,\n        product,\n        location=None,\n        product_id=None,\n        update_mask=None,\n        project_id=None,\n        retry=None,\n        timeout=None,", "suffix": "    ):\n        \"\"\"\n        For the documentation see:\n        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductUpdateOperator`\n        \"\"\"\n        client = self.get_conn()\n        product = self.product_name_determiner.get_entity_with_name(product, product_id, location, project_id)\n        self.log.info('Updating ProductSet: %s', product.name)\n        response = client.update_product(\n            product=product, update_mask=update_mask, retry=retry, timeout=timeout, metadata=metadata\n        )\n        self.log.info('Product updated: %s', response.name if response else '')\n        self.log.debug('Product updated:\\n%s', response)\n        return MessageToDict(response)", "gt": "        metadata=None,"}
{"prefix": "def delete_product(self, location, product_id, project_id=None, retry=None, timeout=None, metadata=None):\n        \"\"\"\n        For the documentation see:\n        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductDeleteOperator`\n        \"\"\"\n        client = self.get_conn()\n        name = ProductSearchClient.product_path(project_id, location, product_id)\n        self.log.info('Deleting ProductSet: %s', name)", "suffix": "        self.log.info('Product with the name [%s] deleted:', name)", "gt": "        client.delete_product(name=name, retry=retry, timeout=timeout, metadata=metadata)"}
{"prefix": "def create_reference_image(\n        self,\n        location,\n        product_id,\n        reference_image,\n        reference_image_id=None,\n        project_id=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        For the documentation see:\n        :py:class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionReferenceImageCreateOperator`\n        \"\"\"\n        client = self.get_conn()\n        self.log.info('Creating ReferenceImage')\n        parent = ProductSearchClient.product_path(project=project_id, location=location, product=product_id)\n\n        response = client.create_reference_image(\n            parent=parent,\n            reference_image=reference_image,\n            reference_image_id=reference_image_id,\n            retry=retry,\n            timeout=timeout,\n            metadata=metadata,\n        )\n\n        self.log.info('ReferenceImage created: %s', response.name if response else '')", "suffix": "\n        if not reference_image_id:\n            # Refernece image  id was generated by the API\n            reference_image_id = self._get_autogenerated_id(response)\n            self.log.info(\n                'Extracted autogenerated ReferenceImage ID from the response: %s', reference_image_id\n            )\n\n        return reference_image_id", "gt": "        self.log.debug('ReferenceImage created:\\n%s', response)"}
{"prefix": "def delete_reference_image(\n        self,\n        location,\n        product_id,\n        reference_image_id,\n        project_id=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        For the documentation see:\n        :py:class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionReferenceImageCreateOperator`\n        \"\"\"\n        client = self.get_conn()\n        self.log.info('Deleting ReferenceImage')\n        name = ProductSearchClient.reference_image_path(\n            project=project_id, location=location, product=product_id, reference_image=reference_image_id\n        )", "suffix": "        self.log.info('ReferenceImage with the name [%s] deleted.', name)\n\n        return MessageToDict(response)", "gt": "        response = client.delete_reference_image(name=name, retry=retry, timeout=timeout, metadata=metadata)"}
{"prefix": "def add_product_to_product_set(\n        self,\n        product_set_id,\n        product_id,\n        location=None,\n        project_id=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        For the documentation see:\n        :py:class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionAddProductToProductSetOperator`\n        \"\"\"\n        client = self.get_conn()\n\n        product_name = ProductSearchClient.product_path(project_id, location, product_id)\n        product_set_name = ProductSearchClient.product_set_path(project_id, location, product_set_id)\n\n        self.log.info('Add Product[name=%s] to Product Set[name=%s]', product_name, product_set_name)\n\n        client.add_product_to_product_set(\n            name=product_set_name, product=product_name, retry=retry, timeout=timeout, metadata=metadata\n        )", "suffix": "        self.log.info('Product added to Product Set')", "gt": ""}
{"prefix": "def annotate_image(self, request, retry=None, timeout=None):\n        \"\"\"\n        For the documentation see:\n        :py:class:`~airflow.contrib.operators.gcp_vision_image_annotator_operator.CloudVisionAnnotateImage`\n        \"\"\"\n        client = self.annotator_client\n\n        self.log.info('Annotating image')", "suffix": "        response = client.annotate_image(request=request, retry=retry, timeout=timeout)\n\n        self.log.info('Image annotated')\n\n        return MessageToDict(response)", "gt": ""}
{"prefix": "def safe_search_detection(\n        self, image, max_results=None, retry=None, timeout=None, additional_properties=None\n    ):\n        \"\"\"\n        For the documentation see:\n        :py:class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionDetectImageSafeSearchOperator`\n        \"\"\"\n        client = self.annotator_client\n\n        self.log.info(\"Detecting safe search\")\n\n        if additional_properties is None:\n            additional_properties = {}\n\n        response = client.safe_search_detection(\n            image=image, max_results=max_results, retry=retry, timeout=timeout, **additional_properties\n        )", "suffix": "        self._check_for_error(response)\n\n        self.log.info(\"Safe search detection finished\")\n        return response", "gt": "        response = MessageToDict(response)"}
{"prefix": "def _get_endpoint(self):\n        \"\"\"", "suffix": "        \"\"\"\n        conn = self.get_connection(self.http_conn_id)\n        token = conn.password\n        if not token:\n            raise AirflowException('Dingding token is requests but get nothing, '\n                                   'check you conn_id configuration.')\n        return 'robot/send?access_token={}'.format(token)", "gt": "        Get Dingding endpoint for sending message."}
{"prefix": "def _build_message(self):\n        \"\"\"\n        Build different type of Dingding message\n        As most commonly used type, text message just need post message content", "suffix": "        \"\"\"\n        if self.message_type in ['text', 'markdown']:\n            data = {\n                'msgtype': self.message_type,\n                self.message_type: {\n                    'content': self.message\n                } if self.message_type == 'text' else self.message,\n                'at': {\n                    'atMobiles': self.at_mobiles,\n                    'isAtAll': self.at_all\n                }\n            }\n        else:\n            data = {\n                'msgtype': self.message_type,\n                self.message_type: self.message\n            }\n        return json.dumps(data)", "gt": "        rather than a dict like ``{'content': 'message'}``"}
{"prefix": "def send(self):\n        \"\"\"\n        Send Dingding message\n        \"\"\"\n        support_type = ['text', 'link', 'markdown', 'actionCard', 'feedCard']\n        if self.message_type not in support_type:\n            raise ValueError('DingdingWebhookHook only support {} '\n                             'so far, but receive {}'.format(support_type, self.message_type))\n\n        data = self._build_message()\n        self.log.info('Sending Dingding type %s message %s', self.message_type, data)\n        resp = self.run(endpoint=self._get_endpoint(),\n                        data=data,\n                        headers={'Content-Type': 'application/json'})\n\n        # Dingding success send message will with errcode equal to 0\n        if int(resp.json().get('errcode')) != 0:\n            raise AirflowException('Send Dingding message failed, receive error '\n                                   'message %s', resp.text)", "suffix": "", "gt": "        self.log.info('Success Send Dingding message')"}
{"prefix": "", "suffix": "        \"\"\"\n        Endpoint for streaming log.\n        :param ti: task instance object\n        :param try_number: try_number of the task instance\n        :param metadata: log metadata,\n                         can be used for steaming log reading and auto-tailing.\n        :return: a list of log documents and metadata.\n        \"\"\"\n        if not metadata:\n            metadata = {'offset': 0}\n        if 'offset' not in metadata:\n            metadata['offset'] = 0\n\n        offset = metadata['offset']\n        log_id = self._render_log_id(ti, try_number)\n\n        logs = self.es_read(log_id, offset)\n\n        next_offset = offset if not logs else logs[-1].offset\n\n        metadata['offset'] = next_offset\n        # end_of_log_mark may contain characters like '\\n' which is needed to\n        # have the log uploaded but will not be stored in elasticsearch.\n        metadata['end_of_log'] = False if not logs \\\n            else logs[-1].message == self.end_of_log_mark.strip()\n\n        cur_ts = pendulum.now()\n        # Assume end of log after not receiving new log for 5 min,\n        # as executor heartbeat is 1 min and there might be some\n        # delay before Elasticsearch makes the log available.\n        if 'last_log_timestamp' in metadata:\n            last_log_ts = timezone.parse(metadata['last_log_timestamp'])\n            if cur_ts.diff(last_log_ts).in_minutes() >= 5:\n                metadata['end_of_log'] = True\n\n        if offset != next_offset or 'last_log_timestamp' not in metadata:\n            metadata['last_log_timestamp'] = str(cur_ts)\n\n        message = '\\n'.join([log.message for log in logs])\n\n        return message, metadata", "gt": "def _read(self, ti, try_number, metadata=None):"}
{"prefix": "def es_read(self, log_id, offset):\n        \"\"\"\n        Returns the logs matching log_id in Elasticsearch and next offset.\n        Returns '' if no log is found or there was an error.\n        :param log_id: the log_id of the log to read.\n        :type log_id: str\n        :param offset: the offset start to read log from.\n        :type offset: str\n        \"\"\"\n\n        # Offset is the unique key for sorting logs given log_id.\n        s = Search(using=self.client) \\\n            .query('match_phrase', log_id=log_id) \\\n            .sort('offset')\n\n        s = s.filter('range', offset={'gt': offset})\n", "suffix": "        if s.count() != 0:\n            try:\n\n                logs = s[self.MAX_LINE_PER_PAGE * self.PAGE:self.MAX_LINE_PER_PAGE] \\\n                    .execute()\n            except Exception as e:\n                self.log.exception('Could not read log with log_id: %s, error: %s', log_id, str(e))\n\n        return logs", "gt": "        logs = []"}
{"prefix": "def _bind_parameters(operation, parameters):\n    \"\"\" Helper method that binds parameters to a SQL query. \"\"\"\n    # inspired by MySQL Python Connector (conversion.py)\n    string_parameters = {}\n    for (name, value) in iteritems(parameters):", "suffix": "            string_parameters[name] = 'NULL'\n        elif isinstance(value, basestring):\n            string_parameters[name] = \"'\" + _escape(value) + \"'\"\n        else:\n            string_parameters[name] = str(value)\n    return operation % string_parameters", "gt": "        if value is None:"}
{"prefix": "def _escape(s):\n    \"\"\" Helper method that escapes parameters to a SQL query. \"\"\"\n    e = s\n    e = e.replace('\\\\', '\\\\\\\\')\n    e = e.replace('\\n', '\\\\n')", "suffix": "    e = e.replace(\"'\", \"\\\\'\")\n    e = e.replace('\"', '\\\\\"')\n    return e", "gt": "    e = e.replace('\\r', '\\\\r')"}
{"prefix": "def _bq_cast(string_field, bq_type):", "suffix": "    Helper method that casts a BigQuery row to the appropriate data types.\n    This is useful because BigQuery returns all fields as strings.\n    \"\"\"\n    if string_field is None:\n        return None\n    elif bq_type == 'INTEGER':\n        return int(string_field)\n    elif bq_type == 'FLOAT' or bq_type == 'TIMESTAMP':\n        return float(string_field)\n    elif bq_type == 'BOOLEAN':\n        if string_field not in ['true', 'false']:\n            raise ValueError(\"{} must have value 'true' or 'false'\".format(\n                string_field))\n        return string_field == 'true'\n    else:\n        return string_field", "gt": "    \"\"\""}
{"prefix": "def _validate_value(key, value, expected_type):\n    \"\"\" function to check expected type and raise\n    error if type is not correct \"\"\"", "suffix": "        raise TypeError(\"{} argument must have a type {} not {}\".format(\n            key, expected_type, type(value)))", "gt": "    if not isinstance(value, expected_type):"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Returns a BigQuery PEP 249 connection object.\n        \"\"\"\n        service = self.get_service()\n        project = self._get_field('project')\n        return BigQueryConnection(\n            service=service,\n            project_id=project,\n            use_legacy_sql=self.use_legacy_sql,", "suffix": "            num_retries=self.num_retries\n        )", "gt": "            location=self.location,"}
{"prefix": "def get_service(self):\n        \"\"\"\n        Returns a BigQuery service object.\n        \"\"\"\n        http_authorized = self._authorize()", "suffix": "            'bigquery', 'v2', http=http_authorized, cache_discovery=False)", "gt": "        return build("}
{"prefix": "def get_pandas_df(self, sql, parameters=None, dialect=None):\n        \"\"\"\n        Returns a Pandas DataFrame for the results produced by a BigQuery\n        query. The DbApiHook method must be overridden because Pandas\n        doesn't support PEP 249 connections, except for SQLite. See:\n\n        https://github.com/pydata/pandas/blob/master/pandas/io/sql.py#L447\n        https://github.com/pydata/pandas/issues/6900\n\n        :param sql: The BigQuery SQL to execute.\n        :type sql: str\n        :param parameters: The parameters to render the SQL query with (not\n            used, leave to override superclass method)\n        :type parameters: mapping or iterable\n        :param dialect: Dialect of BigQuery SQL \u2013 legacy SQL or standard SQL\n            defaults to use `self.use_legacy_sql` if not specified\n        :type dialect: str in {'legacy', 'standard'}\n        \"\"\"\n        private_key = self._get_field('key_path', None) or self._get_field('keyfile_dict', None)\n\n        if dialect is None:\n            dialect = 'legacy' if self.use_legacy_sql else 'standard'", "suffix": "        return read_gbq(sql,\n                        project_id=self._get_field('project'),\n                        dialect=dialect,\n                        verbose=False,\n                        private_key=private_key)", "gt": ""}
{"prefix": "def table_exists(self, project_id, dataset_id, table_id):", "suffix": "        Checks for the existence of a table in Google BigQuery.\n\n        :param project_id: The Google cloud project in which to look for the\n            table. The connection supplied to the hook must provide access to\n            the specified project.\n        :type project_id: str\n        :param dataset_id: The name of the dataset in which to look for the\n            table.\n        :type dataset_id: str\n        :param table_id: The name of the table to check the existence of.\n        :type table_id: str\n        \"\"\"\n        service = self.get_service()\n        try:\n            service.tables().get(\n                projectId=project_id, datasetId=dataset_id,\n                tableId=table_id).execute(num_retries=self.num_retries)\n            return True\n        except HttpError as e:\n            if e.resp['status'] == '404':\n                return False\n            raise", "gt": "        \"\"\""}
{"prefix": "def create_empty_table(self,\n                           project_id,\n                           dataset_id,\n                           table_id,\n                           schema_fields=None,\n                           time_partitioning=None,\n                           cluster_fields=None,\n                           labels=None,\n                           view=None,\n                           num_retries=None):\n        \"\"\"\n        Creates a new, empty table in the dataset.\n        To create a view, which is defined by a SQL query, parse a dictionary to 'view' kwarg\n\n        :param project_id: The project to create the table into.\n        :type project_id: str\n        :param dataset_id: The dataset to create the table into.\n        :type dataset_id: str\n        :param table_id: The Name of the table to be created.\n        :type table_id: str\n        :param schema_fields: If set, the schema field list as defined here:\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schema\n        :type schema_fields: list\n        :param labels: a dictionary containing labels for the table, passed to BigQuery\n        :type labels: dict\n\n        **Example**: ::\n\n            schema_fields=[{\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n                           {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"}]\n\n        :param time_partitioning: configure optional time partitioning fields i.e.\n            partition by field, type and expiration as per API specifications.\n\n            .. seealso::\n                https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#timePartitioning\n        :type time_partitioning: dict\n        :param cluster_fields: [Optional] The fields used for clustering.\n            Must be specified with time_partitioning, data in the table will be first\n            partitioned and subsequently clustered.\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#clustering.fields\n        :type cluster_fields: list\n        :param view: [Optional] A dictionary containing definition for the view.\n            If set, it will create a view instead of a table:\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#view\n        :type view: dict\n\n        **Example**: ::\n\n            view = {\n                \"query\": \"SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 1000\",\n                \"useLegacySql\": False\n            }\n\n        :return: None\n        \"\"\"\n\n        project_id = project_id if project_id is not None else self.project_id\n\n        table_resource = {\n            'tableReference': {\n                'tableId': table_id\n            }\n        }\n\n        if schema_fields:\n            table_resource['schema'] = {'fields': schema_fields}\n\n        if time_partitioning:\n            table_resource['timePartitioning'] = time_partitioning\n\n        if cluster_fields:\n            table_resource['clustering'] = {\n                'fields': cluster_fields\n            }\n\n        if labels:\n            table_resource['labels'] = labels\n\n        if view:\n            table_resource['view'] = view\n\n        num_retries = num_retries if num_retries else self.num_retries\n\n        self.log.info('Creating Table %s:%s.%s',\n                      project_id, dataset_id, table_id)\n\n        try:\n            self.service.tables().insert(\n                projectId=project_id,\n                datasetId=dataset_id,\n                body=table_resource).execute(num_retries=num_retries)\n\n            self.log.info('Table created successfully: %s:%s.%s',\n                          project_id, dataset_id, table_id)", "suffix": "        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content)\n            )", "gt": ""}
{"prefix": "def create_external_table(self,\n                              external_project_dataset_table,\n                              schema_fields,\n                              source_uris,\n                              source_format='CSV',\n                              autodetect=False,\n                              compression='NONE',\n                              ignore_unknown_values=False,\n                              max_bad_records=0,\n                              skip_leading_rows=0,\n                              field_delimiter=',',\n                              quote_character=None,\n                              allow_quoted_newlines=False,\n                              allow_jagged_rows=False,\n                              src_fmt_configs=None,\n                              labels=None\n                              ):\n        \"\"\"\n        Creates a new external table in the dataset with the data in Google\n        Cloud Storage. See here:\n\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#resource\n\n        for more details about these parameters.\n\n        :param external_project_dataset_table:\n            The dotted ``(<project>.|<project>:)<dataset>.<table>($<partition>)`` BigQuery\n            table name to create external table.\n            If ``<project>`` is not included, project will be the\n            project defined in the connection json.\n        :type external_project_dataset_table: str\n        :param schema_fields: The schema field list as defined here:\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#resource\n        :type schema_fields: list\n        :param source_uris: The source Google Cloud\n            Storage URI (e.g. gs://some-bucket/some-file.txt). A single wild\n            per-object name can be used.\n        :type source_uris: list\n        :param source_format: File format to export.\n        :type source_format: str\n        :param autodetect: Try to detect schema and format options automatically.\n            Any option specified explicitly will be honored.\n        :type autodetect: bool\n        :param compression: [Optional] The compression type of the data source.\n            Possible values include GZIP and NONE.\n            The default value is NONE.\n            This setting is ignored for Google Cloud Bigtable,\n            Google Cloud Datastore backups and Avro formats.\n        :type compression: str\n        :param ignore_unknown_values: [Optional] Indicates if BigQuery should allow\n            extra values that are not represented in the table schema.\n            If true, the extra values are ignored. If false, records with extra columns\n            are treated as bad records, and if there are too many bad records, an\n            invalid error is returned in the job result.\n        :type ignore_unknown_values: bool\n        :param max_bad_records: The maximum number of bad records that BigQuery can\n            ignore when running the job.\n        :type max_bad_records: int\n        :param skip_leading_rows: Number of rows to skip when loading from a CSV.\n        :type skip_leading_rows: int\n        :param field_delimiter: The delimiter to use when loading from a CSV.\n        :type field_delimiter: str\n        :param quote_character: The value that is used to quote data sections in a CSV\n            file.\n        :type quote_character: str\n        :param allow_quoted_newlines: Whether to allow quoted newlines (true) or not\n            (false).\n        :type allow_quoted_newlines: bool\n        :param allow_jagged_rows: Accept rows that are missing trailing optional columns.\n            The missing values are treated as nulls. If false, records with missing\n            trailing columns are treated as bad records, and if there are too many bad\n            records, an invalid error is returned in the job result. Only applicable when\n            soure_format is CSV.\n        :type allow_jagged_rows: bool\n        :param src_fmt_configs: configure optional fields specific to the source format\n        :type src_fmt_configs: dict\n        :param labels: a dictionary containing labels for the table, passed to BigQuery\n        :type labels: dict\n        \"\"\"\n\n        if src_fmt_configs is None:\n            src_fmt_configs = {}\n        project_id, dataset_id, external_table_id = \\\n            _split_tablename(table_input=external_project_dataset_table,\n                             default_project_id=self.project_id,\n                             var_name='external_project_dataset_table')\n\n        # bigquery only allows certain source formats\n        # we check to make sure the passed source format is valid\n        # if it's not, we raise a ValueError\n        # Refer to this link for more details:\n        #   https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#externalDataConfiguration.sourceFormat\n\n        source_format = source_format.upper()\n        allowed_formats = [\n            \"CSV\", \"NEWLINE_DELIMITED_JSON\", \"AVRO\", \"GOOGLE_SHEETS\",\n            \"DATASTORE_BACKUP\", \"PARQUET\"\n        ]\n        if source_format not in allowed_formats:\n            raise ValueError(\"{0} is not a valid source format. \"\n                             \"Please use one of the following types: {1}\"\n                             .format(source_format, allowed_formats))\n\n        compression = compression.upper()\n        allowed_compressions = ['NONE', 'GZIP']\n        if compression not in allowed_compressions:\n            raise ValueError(\"{0} is not a valid compression format. \"\n                             \"Please use one of the following types: {1}\"\n                             .format(compression, allowed_compressions))\n\n        table_resource = {\n            'externalDataConfiguration': {\n                'autodetect': autodetect,\n                'sourceFormat': source_format,\n                'sourceUris': source_uris,\n                'compression': compression,\n                'ignoreUnknownValues': ignore_unknown_values", "suffix": "            'tableReference': {\n                'projectId': project_id,\n                'datasetId': dataset_id,\n                'tableId': external_table_id,\n            }\n        }\n\n        if schema_fields:\n            table_resource['externalDataConfiguration'].update({\n                'schema': {\n                    'fields': schema_fields\n                }\n            })\n\n        self.log.info('Creating external table: %s', external_project_dataset_table)\n\n        if max_bad_records:\n            table_resource['externalDataConfiguration']['maxBadRecords'] = max_bad_records\n\n        # if following fields are not specified in src_fmt_configs,\n        # honor the top-level params for backward-compatibility\n        if 'skipLeadingRows' not in src_fmt_configs:\n            src_fmt_configs['skipLeadingRows'] = skip_leading_rows\n        if 'fieldDelimiter' not in src_fmt_configs:\n            src_fmt_configs['fieldDelimiter'] = field_delimiter\n        if 'quote_character' not in src_fmt_configs:\n            src_fmt_configs['quote'] = quote_character\n        if 'allowQuotedNewlines' not in src_fmt_configs:\n            src_fmt_configs['allowQuotedNewlines'] = allow_quoted_newlines\n        if 'allowJaggedRows' not in src_fmt_configs:\n            src_fmt_configs['allowJaggedRows'] = allow_jagged_rows\n\n        src_fmt_to_param_mapping = {\n            'CSV': 'csvOptions',\n            'GOOGLE_SHEETS': 'googleSheetsOptions'\n        }\n\n        src_fmt_to_configs_mapping = {\n            'csvOptions': [\n                'allowJaggedRows', 'allowQuotedNewlines',\n                'fieldDelimiter', 'skipLeadingRows',\n                'quote'\n            ],\n            'googleSheetsOptions': ['skipLeadingRows']\n        }\n\n        if source_format in src_fmt_to_param_mapping.keys():\n\n            valid_configs = src_fmt_to_configs_mapping[\n                src_fmt_to_param_mapping[source_format]\n            ]\n\n            src_fmt_configs = {\n                k: v\n                for k, v in src_fmt_configs.items() if k in valid_configs\n            }\n\n            table_resource['externalDataConfiguration'][src_fmt_to_param_mapping[\n                source_format]] = src_fmt_configs\n\n        if labels:\n            table_resource['labels'] = labels\n\n        try:\n            self.service.tables().insert(\n                projectId=project_id,\n                datasetId=dataset_id,\n                body=table_resource\n            ).execute(num_retries=self.num_retries)\n\n            self.log.info('External table created successfully: %s',\n                          external_project_dataset_table)\n\n        except HttpError as err:\n            raise Exception(\n                'BigQuery job failed. Error was: {}'.format(err.content)\n            )", "gt": "            },"}
{"prefix": "def patch_table(self,\n                    dataset_id,\n                    table_id,\n                    project_id=None,\n                    description=None,\n                    expiration_time=None,\n                    external_data_configuration=None,\n                    friendly_name=None,\n                    labels=None,\n                    schema=None,\n                    time_partitioning=None,\n                    view=None,\n                    require_partition_filter=None):\n        \"\"\"\n        Patch information in an existing table.\n        It only updates fileds that are provided in the request object.\n\n        Reference: https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/patch\n\n        :param dataset_id: The dataset containing the table to be patched.\n        :type dataset_id: str\n        :param table_id: The Name of the table to be patched.\n        :type table_id: str\n        :param project_id: The project containing the table to be patched.\n        :type project_id: str\n        :param description: [Optional] A user-friendly description of this table.\n        :type description: str\n        :param expiration_time: [Optional] The time when this table expires,\n            in milliseconds since the epoch.\n        :type expiration_time: int\n        :param external_data_configuration: [Optional] A dictionary containing\n            properties of a table stored outside of BigQuery.\n        :type external_data_configuration: dict\n        :param friendly_name: [Optional] A descriptive name for this table.\n        :type friendly_name: str\n        :param labels: [Optional] A dictionary containing labels associated with this table.\n        :type labels: dict\n        :param schema: [Optional] If set, the schema field list as defined here:\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schema\n            The supported schema modifications and unsupported schema modification are listed here:\n            https://cloud.google.com/bigquery/docs/managing-table-schemas\n            **Example**: ::\n\n                schema=[{\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},\n                               {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"}]\n\n        :type schema: list\n        :param time_partitioning: [Optional] A dictionary containing time-based partitioning\n             definition for the table.\n        :type time_partitioning: dict\n        :param view: [Optional] A dictionary containing definition for the view.\n            If set, it will patch a view instead of a table:\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#view\n            **Example**: ::\n\n                view = {\n                    \"query\": \"SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 500\",\n                    \"useLegacySql\": False\n                }\n\n        :type view: dict\n        :param require_partition_filter: [Optional] If true, queries over the this table require a\n            partition filter. If false, queries over the table\n        :type require_partition_filter: bool\n\n        \"\"\"\n\n        project_id = project_id if project_id is not None else self.project_id\n\n        table_resource = {}\n\n        if description is not None:\n            table_resource['description'] = description\n        if expiration_time is not None:\n            table_resource['expirationTime'] = expiration_time\n        if external_data_configuration:\n            table_resource['externalDataConfiguration'] = external_data_configuration\n        if friendly_name is not None:\n            table_resource['friendlyName'] = friendly_name\n        if labels:\n            table_resource['labels'] = labels\n        if schema:\n            table_resource['schema'] = {'fields': schema}\n        if time_partitioning:\n            table_resource['timePartitioning'] = time_partitioning\n        if view:\n            table_resource['view'] = view\n        if require_partition_filter is not None:\n            table_resource['requirePartitionFilter'] = require_partition_filter\n\n        self.log.info('Patching Table %s:%s.%s',\n                      project_id, dataset_id, table_id)\n\n        try:\n            self.service.tables().patch(\n                projectId=project_id,\n                datasetId=dataset_id,\n                tableId=table_id,\n                body=table_resource).execute(num_retries=self.num_retries)\n\n            self.log.info('Table patched successfully: %s:%s.%s',\n                          project_id, dataset_id, table_id)\n\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content)", "suffix": "", "gt": "            )"}
{"prefix": "def run_query(self,\n                  sql,\n                  destination_dataset_table=None,\n                  write_disposition='WRITE_EMPTY',\n                  allow_large_results=False,\n                  flatten_results=None,\n                  udf_config=None,\n                  use_legacy_sql=None,\n                  maximum_billing_tier=None,\n                  maximum_bytes_billed=None,\n                  create_disposition='CREATE_IF_NEEDED',\n                  query_params=None,\n                  labels=None,\n                  schema_update_options=(),\n                  priority='INTERACTIVE',\n                  time_partitioning=None,\n                  api_resource_configs=None,\n                  cluster_fields=None,\n                  location=None):\n        \"\"\"\n        Executes a BigQuery SQL query. Optionally persists results in a BigQuery\n        table. See here:\n\n        https://cloud.google.com/bigquery/docs/reference/v2/jobs\n\n        For more details about these parameters.\n\n        :param sql: The BigQuery SQL to execute.\n        :type sql: str\n        :param destination_dataset_table: The dotted ``<dataset>.<table>``\n            BigQuery table to save the query results.\n        :type destination_dataset_table: str\n        :param write_disposition: What to do if the table already exists in\n            BigQuery.\n        :type write_disposition: str\n        :param allow_large_results: Whether to allow large results.\n        :type allow_large_results: bool\n        :param flatten_results: If true and query uses legacy SQL dialect, flattens\n            all nested and repeated fields in the query results. ``allowLargeResults``\n            must be true if this is set to false. For standard SQL queries, this\n            flag is ignored and results are never flattened.\n        :type flatten_results: bool\n        :param udf_config: The User Defined Function configuration for the query.\n            See https://cloud.google.com/bigquery/user-defined-functions for details.\n        :type udf_config: list\n        :param use_legacy_sql: Whether to use legacy SQL (true) or standard SQL (false).\n            If `None`, defaults to `self.use_legacy_sql`.\n        :type use_legacy_sql: bool\n        :param api_resource_configs: a dictionary that contain params\n            'configuration' applied for Google BigQuery Jobs API:\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs\n            for example, {'query': {'useQueryCache': False}}. You could use it\n            if you need to provide some params that are not supported by the\n            BigQueryHook like args.\n        :type api_resource_configs: dict\n        :param maximum_billing_tier: Positive integer that serves as a\n            multiplier of the basic price.\n        :type maximum_billing_tier: int\n        :param maximum_bytes_billed: Limits the bytes billed for this job.\n            Queries that will have bytes billed beyond this limit will fail\n            (without incurring a charge). If unspecified, this will be\n            set to your project default.\n        :type maximum_bytes_billed: float\n        :param create_disposition: Specifies whether the job is allowed to\n            create new tables.\n        :type create_disposition: str\n        :param query_params: a list of dictionary containing query parameter types and\n            values, passed to BigQuery\n        :type query_params: list\n        :param labels: a dictionary containing labels for the job/query,\n            passed to BigQuery\n        :type labels: dict\n        :param schema_update_options: Allows the schema of the destination\n            table to be updated as a side effect of the query job.\n        :type schema_update_options: tuple\n        :param priority: Specifies a priority for the query.\n            Possible values include INTERACTIVE and BATCH.\n            The default value is INTERACTIVE.\n        :type priority: str\n        :param time_partitioning: configure optional time partitioning fields i.e.\n            partition by field, type and expiration as per API specifications.\n        :type time_partitioning: dict\n        :param cluster_fields: Request that the result of this query be stored sorted\n            by one or more columns. This is only available in combination with\n            time_partitioning. The order of columns given determines the sort order.\n        :type cluster_fields: list[str]\n        :param location: The geographic location of the job. Required except for\n            US and EU. See details at\n            https://cloud.google.com/bigquery/docs/locations#specifying_your_location\n        :type location: str\n        \"\"\"\n\n        if time_partitioning is None:\n            time_partitioning = {}\n\n        if location:\n            self.location = location\n\n        if not api_resource_configs:\n            api_resource_configs = self.api_resource_configs\n        else:\n            _validate_value('api_resource_configs',\n                            api_resource_configs, dict)\n        configuration = deepcopy(api_resource_configs)\n        if 'query' not in configuration:\n            configuration['query'] = {}\n\n        else:\n            _validate_value(\"api_resource_configs['query']\",\n                            configuration['query'], dict)\n\n        if sql is None and not configuration['query'].get('query', None):\n            raise TypeError('`BigQueryBaseCursor.run_query` '\n                            'missing 1 required positional argument: `sql`')\n\n        # BigQuery also allows you to define how you want a table's schema to change\n        # as a side effect of a query job\n        # for more details:\n        #   https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.query.schemaUpdateOptions\n\n        allowed_schema_update_options = [\n            'ALLOW_FIELD_ADDITION', \"ALLOW_FIELD_RELAXATION\"\n        ]\n\n        if not set(allowed_schema_update_options\n                   ).issuperset(set(schema_update_options)):\n            raise ValueError(\"{0} contains invalid schema update options. \"\n                             \"Please only use one or more of the following \"\n                             \"options: {1}\"\n                             .format(schema_update_options,\n                                     allowed_schema_update_options))\n\n        if schema_update_options:\n            if write_disposition not in [\"WRITE_APPEND\", \"WRITE_TRUNCATE\"]:\n                raise ValueError(\"schema_update_options is only \"\n                                 \"allowed if write_disposition is \"\n                                 \"'WRITE_APPEND' or 'WRITE_TRUNCATE'.\")\n\n        if destination_dataset_table:\n            destination_project, destination_dataset, destination_table = \\\n                _split_tablename(table_input=destination_dataset_table,\n                                 default_project_id=self.project_id)\n\n            destination_dataset_table = {\n                'projectId': destination_project,\n                'datasetId': destination_dataset,\n                'tableId': destination_table,\n            }\n\n        if cluster_fields:\n            cluster_fields = {'fields': cluster_fields}\n\n        query_param_list = [\n            (sql, 'query', None, six.string_types),\n            (priority, 'priority', 'INTERACTIVE', six.string_types),\n            (use_legacy_sql, 'useLegacySql', self.use_legacy_sql, bool),\n            (query_params, 'queryParameters', None, list),\n            (udf_config, 'userDefinedFunctionResources', None, list),\n            (maximum_billing_tier, 'maximumBillingTier', None, int),\n            (maximum_bytes_billed, 'maximumBytesBilled', None, float),\n            (time_partitioning, 'timePartitioning', {}, dict),\n            (schema_update_options, 'schemaUpdateOptions', None, tuple),\n            (destination_dataset_table, 'destinationTable', None, dict),\n            (cluster_fields, 'clustering', None, dict),\n        ]\n\n        for param_tuple in query_param_list:\n\n            param, param_name, param_default, param_type = param_tuple\n\n            if param_name not in configuration['query'] and param in [None, {}, ()]:\n                if param_name == 'timePartitioning':\n                    param_default = _cleanse_time_partitioning(\n                        destination_dataset_table, time_partitioning)\n                param = param_default\n\n            if param not in [None, {}, ()]:\n                _api_resource_configs_duplication_check(\n                    param_name, param, configuration['query'])\n\n                configuration['query'][param_name] = param\n\n                # check valid type of provided param,\n                # it last step because we can get param from 2 sources,\n                # and first of all need to find it\n\n                _validate_value(param_name, configuration['query'][param_name],\n                                param_type)\n\n                if param_name == 'schemaUpdateOptions' and param:\n                    self.log.info(\"Adding experimental 'schemaUpdateOptions': \"\n                                  \"%s\", schema_update_options)\n\n                if param_name == 'destinationTable':\n                    for key in ['projectId', 'datasetId', 'tableId']:\n                        if key not in configuration['query']['destinationTable']:\n                            raise ValueError(\n                                \"Not correct 'destinationTable' in \"\n                                \"api_resource_configs. 'destinationTable' \"\n                                \"must be a dict with {'projectId':'', \"\n                                \"'datasetId':'', 'tableId':''}\")\n\n                    configuration['query'].update({\n                        'allowLargeResults': allow_large_results,\n                        'flattenResults': flatten_results,\n                        'writeDisposition': write_disposition,\n                        'createDisposition': create_disposition,\n                    })\n\n        if 'useLegacySql' in configuration['query'] and configuration['query']['useLegacySql'] and\\\n                'queryParameters' in configuration['query']:\n            raise ValueError(\"Query parameters are not allowed \"\n                             \"when using legacy SQL\")\n\n        if labels:\n            _api_resource_configs_duplication_check(\n                'labels', labels, configuration)\n            configuration['labels'] = labels", "suffix": "        return self.run_with_configuration(configuration)", "gt": ""}
{"prefix": "def run_extract(  # noqa\n            self,\n            source_project_dataset_table,\n            destination_cloud_storage_uris,\n            compression='NONE',\n            export_format='CSV',\n            field_delimiter=',',\n            print_header=True,\n            labels=None):\n        \"\"\"\n        Executes a BigQuery extract command to copy data from BigQuery to\n        Google Cloud Storage. See here:\n\n        https://cloud.google.com/bigquery/docs/reference/v2/jobs\n\n        For more details about these parameters.\n\n        :param source_project_dataset_table: The dotted ``<dataset>.<table>``\n            BigQuery table to use as the source data.\n        :type source_project_dataset_table: str\n        :param destination_cloud_storage_uris: The destination Google Cloud\n            Storage URI (e.g. gs://some-bucket/some-file.txt). Follows\n            convention defined here:\n            https://cloud.google.com/bigquery/exporting-data-from-bigquery#exportingmultiple\n        :type destination_cloud_storage_uris: list\n        :param compression: Type of compression to use.\n        :type compression: str\n        :param export_format: File format to export.\n        :type export_format: str\n        :param field_delimiter: The delimiter to use when extracting to a CSV.\n        :type field_delimiter: str\n        :param print_header: Whether to print a header for a CSV file extract.\n        :type print_header: bool\n        :param labels: a dictionary containing labels for the job/query,\n            passed to BigQuery\n        :type labels: dict\n        \"\"\"\n\n        source_project, source_dataset, source_table = \\\n            _split_tablename(table_input=source_project_dataset_table,\n                             default_project_id=self.project_id,\n                             var_name='source_project_dataset_table')\n\n        configuration = {", "suffix": "                'sourceTable': {\n                    'projectId': source_project,\n                    'datasetId': source_dataset,\n                    'tableId': source_table,\n                },\n                'compression': compression,\n                'destinationUris': destination_cloud_storage_uris,\n                'destinationFormat': export_format,\n            }\n        }\n\n        if labels:\n            configuration['labels'] = labels\n\n        if export_format == 'CSV':\n            # Only set fieldDelimiter and printHeader fields if using CSV.\n            # Google does not like it if you set these fields for other export\n            # formats.\n            configuration['extract']['fieldDelimiter'] = field_delimiter\n            configuration['extract']['printHeader'] = print_header\n\n        return self.run_with_configuration(configuration)", "gt": "            'extract': {"}
{"prefix": "def run_copy(self,\n                 source_project_dataset_tables,\n                 destination_project_dataset_table,\n                 write_disposition='WRITE_EMPTY',\n                 create_disposition='CREATE_IF_NEEDED',\n                 labels=None):\n        \"\"\"\n        Executes a BigQuery copy command to copy data from one BigQuery table\n        to another. See here:\n\n        https://cloud.google.com/bigquery/docs/reference/v2/jobs#configuration.copy\n\n        For more details about these parameters.\n\n        :param source_project_dataset_tables: One or more dotted\n            ``(project:|project.)<dataset>.<table>``\n            BigQuery tables to use as the source data. Use a list if there are\n            multiple source tables.\n            If ``<project>`` is not included, project will be the project defined\n            in the connection json.\n        :type source_project_dataset_tables: list|string\n        :param destination_project_dataset_table: The destination BigQuery\n            table. Format is: ``(project:|project.)<dataset>.<table>``\n        :type destination_project_dataset_table: str\n        :param write_disposition: The write disposition if the table already exists.\n        :type write_disposition: str\n        :param create_disposition: The create disposition if the table doesn't exist.\n        :type create_disposition: str\n        :param labels: a dictionary containing labels for the job/query,\n            passed to BigQuery\n        :type labels: dict\n        \"\"\"\n        source_project_dataset_tables = ([\n            source_project_dataset_tables\n        ] if not isinstance(source_project_dataset_tables, list) else\n            source_project_dataset_tables)\n\n        source_project_dataset_tables_fixup = []\n        for source_project_dataset_table in source_project_dataset_tables:\n            source_project, source_dataset, source_table = \\\n                _split_tablename(table_input=source_project_dataset_table,\n                                 default_project_id=self.project_id,\n                                 var_name='source_project_dataset_table')\n            source_project_dataset_tables_fixup.append({\n                'projectId':\n                source_project,\n                'datasetId':\n                source_dataset,\n                'tableId':\n                source_table\n            })\n\n        destination_project, destination_dataset, destination_table = \\\n            _split_tablename(table_input=destination_project_dataset_table,\n                             default_project_id=self.project_id)\n        configuration = {\n            'copy': {\n                'createDisposition': create_disposition,\n                'writeDisposition': write_disposition,\n                'sourceTables': source_project_dataset_tables_fixup,\n                'destinationTable': {\n                    'projectId': destination_project,\n                    'datasetId': destination_dataset,\n                    'tableId': destination_table\n                }\n            }\n        }\n\n        if labels:\n            configuration['labels'] = labels\n", "suffix": "", "gt": "        return self.run_with_configuration(configuration)"}
{"prefix": "def run_load(self,\n                 destination_project_dataset_table,\n                 source_uris,\n                 schema_fields=None,\n                 source_format='CSV',\n                 create_disposition='CREATE_IF_NEEDED',\n                 skip_leading_rows=0,\n                 write_disposition='WRITE_EMPTY',\n                 field_delimiter=',',\n                 max_bad_records=0,\n                 quote_character=None,\n                 ignore_unknown_values=False,\n                 allow_quoted_newlines=False,\n                 allow_jagged_rows=False,\n                 schema_update_options=(),\n                 src_fmt_configs=None,\n                 time_partitioning=None,\n                 cluster_fields=None,\n                 autodetect=False):\n        \"\"\"\n        Executes a BigQuery load command to load data from Google Cloud Storage\n        to BigQuery. See here:\n\n        https://cloud.google.com/bigquery/docs/reference/v2/jobs\n\n        For more details about these parameters.\n\n        :param destination_project_dataset_table:\n            The dotted ``(<project>.|<project>:)<dataset>.<table>($<partition>)`` BigQuery\n            table to load data into. If ``<project>`` is not included, project will be the\n            project defined in the connection json. If a partition is specified the\n            operator will automatically append the data, create a new partition or create\n            a new DAY partitioned table.\n        :type destination_project_dataset_table: str\n        :param schema_fields: The schema field list as defined here:\n            https://cloud.google.com/bigquery/docs/reference/v2/jobs#configuration.load\n            Required if autodetect=False; optional if autodetect=True.\n        :type schema_fields: list\n        :param autodetect: Attempt to autodetect the schema for CSV and JSON\n            source files.\n        :type autodetect: bool\n        :param source_uris: The source Google Cloud\n            Storage URI (e.g. gs://some-bucket/some-file.txt). A single wild\n            per-object name can be used.\n        :type source_uris: list\n        :param source_format: File format to export.\n        :type source_format: str\n        :param create_disposition: The create disposition if the table doesn't exist.\n        :type create_disposition: str\n        :param skip_leading_rows: Number of rows to skip when loading from a CSV.\n        :type skip_leading_rows: int\n        :param write_disposition: The write disposition if the table already exists.\n        :type write_disposition: str\n        :param field_delimiter: The delimiter to use when loading from a CSV.\n        :type field_delimiter: str\n        :param max_bad_records: The maximum number of bad records that BigQuery can\n            ignore when running the job.\n        :type max_bad_records: int\n        :param quote_character: The value that is used to quote data sections in a CSV\n            file.\n        :type quote_character: str\n        :param ignore_unknown_values: [Optional] Indicates if BigQuery should allow\n            extra values that are not represented in the table schema.\n            If true, the extra values are ignored. If false, records with extra columns\n            are treated as bad records, and if there are too many bad records, an\n            invalid error is returned in the job result.\n        :type ignore_unknown_values: bool\n        :param allow_quoted_newlines: Whether to allow quoted newlines (true) or not\n            (false).\n        :type allow_quoted_newlines: bool\n        :param allow_jagged_rows: Accept rows that are missing trailing optional columns.\n            The missing values are treated as nulls. If false, records with missing\n            trailing columns are treated as bad records, and if there are too many bad\n            records, an invalid error is returned in the job result. Only applicable when\n            soure_format is CSV.\n        :type allow_jagged_rows: bool\n        :param schema_update_options: Allows the schema of the destination\n            table to be updated as a side effect of the load job.\n        :type schema_update_options: tuple\n        :param src_fmt_configs: configure optional fields specific to the source format\n        :type src_fmt_configs: dict\n        :param time_partitioning: configure optional time partitioning fields i.e.\n            partition by field, type and  expiration as per API specifications.\n        :type time_partitioning: dict\n        :param cluster_fields: Request that the result of this load be stored sorted\n            by one or more columns. This is only available in combination with\n            time_partitioning. The order of columns given determines the sort order.\n        :type cluster_fields: list[str]\n        \"\"\"\n\n        # bigquery only allows certain source formats\n        # we check to make sure the passed source format is valid\n        # if it's not, we raise a ValueError\n        # Refer to this link for more details:\n        #   https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.query.tableDefinitions.(key).sourceFormat\n\n        if schema_fields is None and not autodetect:\n            raise ValueError(\n                'You must either pass a schema or autodetect=True.')\n\n        if src_fmt_configs is None:\n            src_fmt_configs = {}\n\n        source_format = source_format.upper()\n        allowed_formats = [\n            \"CSV\", \"NEWLINE_DELIMITED_JSON\", \"AVRO\", \"GOOGLE_SHEETS\",\n            \"DATASTORE_BACKUP\", \"PARQUET\"\n        ]\n        if source_format not in allowed_formats:\n            raise ValueError(\"{0} is not a valid source format. \"\n                             \"Please use one of the following types: {1}\"\n                             .format(source_format, allowed_formats))\n\n        # bigquery also allows you to define how you want a table's schema to change\n        # as a side effect of a load\n        # for more details:\n        # https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schemaUpdateOptions\n        allowed_schema_update_options = [\n            'ALLOW_FIELD_ADDITION', \"ALLOW_FIELD_RELAXATION\"\n        ]\n        if not set(allowed_schema_update_options).issuperset(\n                set(schema_update_options)):\n            raise ValueError(\n                \"{0} contains invalid schema update options.\"\n                \"Please only use one or more of the following options: {1}\"\n                .format(schema_update_options, allowed_schema_update_options))\n\n        destination_project, destination_dataset, destination_table = \\\n            _split_tablename(table_input=destination_project_dataset_table,\n                             default_project_id=self.project_id,\n                             var_name='destination_project_dataset_table')\n\n        configuration = {\n            'load': {\n                'autodetect': autodetect,\n                'createDisposition': create_disposition,\n                'destinationTable': {\n                    'projectId': destination_project,\n                    'datasetId': destination_dataset,\n                    'tableId': destination_table,\n                },\n                'sourceFormat': source_format,\n                'sourceUris': source_uris,\n                'writeDisposition': write_disposition,\n                'ignoreUnknownValues': ignore_unknown_values\n            }\n        }\n\n        time_partitioning = _cleanse_time_partitioning(\n            destination_project_dataset_table,\n            time_partitioning\n        )\n        if time_partitioning:\n            configuration['load'].update({\n                'timePartitioning': time_partitioning\n            })\n\n        if cluster_fields:\n            configuration['load'].update({'clustering': {'fields': cluster_fields}})\n\n        if schema_fields:\n            configuration['load']['schema'] = {'fields': schema_fields}\n\n        if schema_update_options:\n            if write_disposition not in [\"WRITE_APPEND\", \"WRITE_TRUNCATE\"]:\n                raise ValueError(\"schema_update_options is only \"\n                                 \"allowed if write_disposition is \"\n                                 \"'WRITE_APPEND' or 'WRITE_TRUNCATE'.\")\n            else:\n                self.log.info(\n                    \"Adding experimental 'schemaUpdateOptions': %s\",\n                    schema_update_options\n                )\n                configuration['load'][\n                    'schemaUpdateOptions'] = schema_update_options\n\n        if max_bad_records:", "suffix": "\n        # if following fields are not specified in src_fmt_configs,\n        # honor the top-level params for backward-compatibility\n        if 'skipLeadingRows' not in src_fmt_configs:\n            src_fmt_configs['skipLeadingRows'] = skip_leading_rows\n        if 'fieldDelimiter' not in src_fmt_configs:\n            src_fmt_configs['fieldDelimiter'] = field_delimiter\n        if 'ignoreUnknownValues' not in src_fmt_configs:\n            src_fmt_configs['ignoreUnknownValues'] = ignore_unknown_values\n        if quote_character is not None:\n            src_fmt_configs['quote'] = quote_character\n        if allow_quoted_newlines:\n            src_fmt_configs['allowQuotedNewlines'] = allow_quoted_newlines\n\n        src_fmt_to_configs_mapping = {\n            'CSV': [\n                'allowJaggedRows', 'allowQuotedNewlines', 'autodetect',\n                'fieldDelimiter', 'skipLeadingRows', 'ignoreUnknownValues',\n                'nullMarker', 'quote'\n            ],\n            'DATASTORE_BACKUP': ['projectionFields'],\n            'NEWLINE_DELIMITED_JSON': ['autodetect', 'ignoreUnknownValues'],\n            'PARQUET': ['autodetect', 'ignoreUnknownValues'],\n            'AVRO': ['useAvroLogicalTypes'],\n        }\n        valid_configs = src_fmt_to_configs_mapping[source_format]\n        src_fmt_configs = {\n            k: v\n            for k, v in src_fmt_configs.items() if k in valid_configs\n        }\n        configuration['load'].update(src_fmt_configs)\n\n        if allow_jagged_rows:\n            configuration['load']['allowJaggedRows'] = allow_jagged_rows\n\n        return self.run_with_configuration(configuration)", "gt": "            configuration['load']['maxBadRecords'] = max_bad_records"}
{"prefix": "def run_with_configuration(self, configuration):\n        \"\"\"\n        Executes a BigQuery SQL query. See here:\n\n        https://cloud.google.com/bigquery/docs/reference/v2/jobs\n\n        For more details about the configuration parameter.\n\n        :param configuration: The configuration parameter maps directly to\n            BigQuery's configuration field in the job object. See\n            https://cloud.google.com/bigquery/docs/reference/v2/jobs for\n            details.\n        \"\"\"\n        jobs = self.service.jobs()\n        job_data = {'configuration': configuration}\n", "suffix": "        query_reply = jobs \\\n            .insert(projectId=self.project_id, body=job_data) \\\n            .execute(num_retries=self.num_retries)\n        self.running_job_id = query_reply['jobReference']['jobId']\n        if 'location' in query_reply['jobReference']:\n            location = query_reply['jobReference']['location']\n        else:\n            location = self.location\n\n        # Wait for query to finish.\n        keep_polling_job = True\n        while keep_polling_job:\n            try:\n                if location:\n                    job = jobs.get(\n                        projectId=self.project_id,\n                        jobId=self.running_job_id,\n                        location=location).execute(num_retries=self.num_retries)\n                else:\n                    job = jobs.get(\n                        projectId=self.project_id,\n                        jobId=self.running_job_id).execute(num_retries=self.num_retries)\n                if job['status']['state'] == 'DONE':\n                    keep_polling_job = False\n                    # Check if job had errors.\n                    if 'errorResult' in job['status']:\n                        raise Exception(\n                            'BigQuery job failed. Final error was: {}. The job was: {}'.\n                            format(job['status']['errorResult'], job))\n                else:\n                    self.log.info('Waiting for job to complete : %s, %s',\n                                  self.project_id, self.running_job_id)\n                    time.sleep(5)\n\n            except HttpError as err:\n                if err.resp.status in [500, 503]:\n                    self.log.info(\n                        '%s: Retryable error, waiting for job to complete: %s',\n                        err.resp.status, self.running_job_id)\n                    time.sleep(5)\n                else:\n                    raise Exception(\n                        'BigQuery job status check failed. Final error was: {}'.\n                        format(err.resp.status))\n\n        return self.running_job_id", "gt": "        # Send query and wait for reply."}
{"prefix": "def cancel_query(self):\n        \"\"\"\n        Cancel all started queries that have not yet completed\n        \"\"\"\n        jobs = self.service.jobs()\n        if (self.running_job_id and\n                not self.poll_job_complete(self.running_job_id)):\n            self.log.info('Attempting to cancel job : %s, %s', self.project_id,\n                          self.running_job_id)\n            if self.location:\n                jobs.cancel(\n                    projectId=self.project_id,\n                    jobId=self.running_job_id,\n                    location=self.location).execute(num_retries=self.num_retries)\n            else:\n                jobs.cancel(\n                    projectId=self.project_id,\n                    jobId=self.running_job_id).execute(num_retries=self.num_retries)\n        else:\n            self.log.info('No running BigQuery jobs to cancel.')\n            return\n\n        # Wait for all the calls to cancel to finish", "suffix": "        polling_attempts = 0\n\n        job_complete = False\n        while polling_attempts < max_polling_attempts and not job_complete:\n            polling_attempts = polling_attempts + 1\n            job_complete = self.poll_job_complete(self.running_job_id)\n            if job_complete:\n                self.log.info('Job successfully canceled: %s, %s',\n                              self.project_id, self.running_job_id)\n            elif polling_attempts == max_polling_attempts:\n                self.log.info(\n                    \"Stopping polling due to timeout. Job with id %s \"\n                    \"has not completed cancel and may or may not finish.\",\n                    self.running_job_id)\n            else:\n                self.log.info('Waiting for canceled job with id %s to finish.',\n                              self.running_job_id)\n                time.sleep(5)", "gt": "        max_polling_attempts = 12"}
{"prefix": "def get_schema(self, dataset_id, table_id):\n        \"\"\"\n        Get the schema for a given datset.table.\n        see https://cloud.google.com/bigquery/docs/reference/v2/tables#resource\n", "suffix": "        :param table_id: the table ID of the requested table\n        :return: a table schema\n        \"\"\"\n        tables_resource = self.service.tables() \\\n            .get(projectId=self.project_id, datasetId=dataset_id, tableId=table_id) \\\n            .execute(num_retries=self.num_retries)\n        return tables_resource['schema']", "gt": "        :param dataset_id: the dataset ID of the requested table"}
{"prefix": "def get_tabledata(self, dataset_id, table_id,\n                      max_results=None, selected_fields=None, page_token=None,\n                      start_index=None):\n        \"\"\"\n        Get the data of a given dataset.table and optionally with selected columns.\n        see https://cloud.google.com/bigquery/docs/reference/v2/tabledata/list\n\n        :param dataset_id: the dataset ID of the requested table.\n        :param table_id: the table ID of the requested table.\n        :param max_results: the maximum results to return.\n        :param selected_fields: List of fields to return (comma-separated). If\n            unspecified, all fields are returned.\n        :param page_token: page token, returned from a previous call,\n            identifying the result set.\n        :param start_index: zero based index of the starting row to read.\n        :return: map containing the requested rows.\n        \"\"\"\n        optional_params = {}\n        if max_results:\n            optional_params['maxResults'] = max_results\n        if selected_fields:\n            optional_params['selectedFields'] = selected_fields\n        if page_token:\n            optional_params['pageToken'] = page_token", "suffix": "            optional_params['startIndex'] = start_index\n        return (self.service.tabledata().list(\n            projectId=self.project_id,\n            datasetId=dataset_id,\n            tableId=table_id,\n            **optional_params).execute(num_retries=self.num_retries))", "gt": "        if start_index:"}
{"prefix": "def run_table_delete(self, deletion_dataset_table,\n                         ignore_if_missing=False):\n        \"\"\"\n        Delete an existing table from the dataset;\n        If the table does not exist, return an error unless ignore_if_missing\n        is set to True.\n\n        :param deletion_dataset_table: A dotted\n            ``(<project>.|<project>:)<dataset>.<table>`` that indicates which table\n            will be deleted.", "suffix": "        :param ignore_if_missing: if True, then return success even if the\n            requested table does not exist.\n        :type ignore_if_missing: bool\n        :return:\n        \"\"\"\n        deletion_project, deletion_dataset, deletion_table = \\\n            _split_tablename(table_input=deletion_dataset_table,\n                             default_project_id=self.project_id)\n\n        try:\n            self.service.tables() \\\n                .delete(projectId=deletion_project,\n                        datasetId=deletion_dataset,\n                        tableId=deletion_table) \\\n                .execute(num_retries=self.num_retries)\n            self.log.info('Deleted table %s:%s.%s.', deletion_project,\n                          deletion_dataset, deletion_table)\n        except HttpError:\n            if not ignore_if_missing:\n                raise Exception('Table deletion failed. Table does not exist.')\n            else:\n                self.log.info('Table does not exist. Skipping.')", "gt": "        :type deletion_dataset_table: str"}
{"prefix": "def run_table_upsert(self, dataset_id, table_resource, project_id=None):\n        \"\"\"\n        creates a new, empty table in the dataset;\n        If the table already exists, update the existing table.\n        Since BigQuery does not natively allow table upserts, this is not an\n        atomic operation.\n\n        :param dataset_id: the dataset to upsert the table into.\n        :type dataset_id: str\n        :param table_resource: a table resource. see\n            https://cloud.google.com/bigquery/docs/reference/v2/tables#resource\n        :type table_resource: dict\n        :param project_id: the project to upsert the table into.  If None,\n            project will be self.project_id.\n        :return:\n        \"\"\"\n        # check to see if the table exists\n        table_id = table_resource['tableReference']['tableId']\n        project_id = project_id if project_id is not None else self.project_id\n        tables_list_resp = self.service.tables().list(\n            projectId=project_id, datasetId=dataset_id).execute(num_retries=self.num_retries)\n        while True:\n            for table in tables_list_resp.get('tables', []):\n                if table['tableReference']['tableId'] == table_id:\n                    # found the table, do update\n                    self.log.info('Table %s:%s.%s exists, updating.',\n                                  project_id, dataset_id, table_id)", "suffix": "                        projectId=project_id,\n                        datasetId=dataset_id,\n                        tableId=table_id,\n                        body=table_resource).execute(num_retries=self.num_retries)\n            # If there is a next page, we need to check the next page.\n            if 'nextPageToken' in tables_list_resp:\n                tables_list_resp = self.service.tables()\\\n                    .list(projectId=project_id,\n                          datasetId=dataset_id,\n                          pageToken=tables_list_resp['nextPageToken'])\\\n                    .execute(num_retries=self.num_retries)\n            # If there is no next page, then the table doesn't exist.\n            else:\n                # do insert\n                self.log.info('Table %s:%s.%s does not exist. creating.',\n                              project_id, dataset_id, table_id)\n                return self.service.tables().insert(\n                    projectId=project_id,\n                    datasetId=dataset_id,\n                    body=table_resource).execute(num_retries=self.num_retries)", "gt": "                    return self.service.tables().update("}
{"prefix": "def run_grant_dataset_view_access(self,\n                                      source_dataset,\n                                      view_dataset,\n                                      view_table,\n                                      source_project=None,\n                                      view_project=None):\n        \"\"\"\n        Grant authorized view access of a dataset to a view table.\n        If this view has already been granted access to the dataset, do nothing.\n        This method is not atomic.  Running it may clobber a simultaneous update.\n\n        :param source_dataset: the source dataset\n        :type source_dataset: str\n        :param view_dataset: the dataset that the view is in\n        :type view_dataset: str\n        :param view_table: the table of the view\n        :type view_table: str\n        :param source_project: the project of the source dataset. If None,\n            self.project_id will be used.\n        :type source_project: str\n        :param view_project: the project that the view is in. If None,\n            self.project_id will be used.\n        :type view_project: str\n        :return: the datasets resource of the source dataset.\n        \"\"\"\n\n        # Apply default values to projects\n        source_project = source_project if source_project else self.project_id", "suffix": "\n        # we don't want to clobber any existing accesses, so we have to get\n        # info on the dataset before we can add view access\n        source_dataset_resource = self.service.datasets().get(\n            projectId=source_project, datasetId=source_dataset).execute(num_retries=self.num_retries)\n        access = source_dataset_resource[\n            'access'] if 'access' in source_dataset_resource else []\n        view_access = {\n            'view': {\n                'projectId': view_project,\n                'datasetId': view_dataset,\n                'tableId': view_table\n            }\n        }\n        # check to see if the view we want to add already exists.\n        if view_access not in access:\n            self.log.info(\n                'Granting table %s:%s.%s authorized view access to %s:%s dataset.',\n                view_project, view_dataset, view_table, source_project,\n                source_dataset)\n            access.append(view_access)\n            return self.service.datasets().patch(\n                projectId=source_project,\n                datasetId=source_dataset,\n                body={\n                    'access': access\n                }).execute(num_retries=self.num_retries)\n        else:\n            # if view is already in access, do nothing.\n            self.log.info(\n                'Table %s:%s.%s already has authorized view access to %s:%s dataset.',\n                view_project, view_dataset, view_table, source_project, source_dataset)\n            return source_dataset_resource", "gt": "        view_project = view_project if view_project else self.project_id"}
{"prefix": "def create_empty_dataset(self, dataset_id=\"\", project_id=\"\",\n                             dataset_reference=None):\n        \"\"\"\n        Create a new empty dataset:", "suffix": "\n        :param project_id: The name of the project where we want to create\n            an empty a dataset. Don't need to provide, if projectId in dataset_reference.\n        :type project_id: str\n        :param dataset_id: The id of dataset. Don't need to provide,\n            if datasetId in dataset_reference.\n        :type dataset_id: str\n        :param dataset_reference: Dataset reference that could be provided\n            with request body. More info:\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets#resource\n        :type dataset_reference: dict\n        \"\"\"\n\n        if dataset_reference:\n            _validate_value('dataset_reference', dataset_reference, dict)\n        else:\n            dataset_reference = {}\n\n        if \"datasetReference\" not in dataset_reference:\n            dataset_reference[\"datasetReference\"] = {}\n\n        if not dataset_reference[\"datasetReference\"].get(\"datasetId\") and not dataset_id:\n            raise ValueError(\n                \"{} not provided datasetId. Impossible to create dataset\")\n\n        dataset_required_params = [(dataset_id, \"datasetId\", \"\"),\n                                   (project_id, \"projectId\", self.project_id)]\n        for param_tuple in dataset_required_params:\n            param, param_name, param_default = param_tuple\n            if param_name not in dataset_reference['datasetReference']:\n                if param_default and not param:\n                    self.log.info(\n                        \"%s was not specified. Will be used default value %s.\",\n                        param_name, param_default\n                    )\n                    param = param_default\n                dataset_reference['datasetReference'].update(\n                    {param_name: param})\n            elif param:\n                _api_resource_configs_duplication_check(\n                    param_name, param,\n                    dataset_reference['datasetReference'], 'dataset_reference')\n\n        dataset_id = dataset_reference.get(\"datasetReference\").get(\"datasetId\")\n        dataset_project_id = dataset_reference.get(\"datasetReference\").get(\n            \"projectId\")\n\n        self.log.info('Creating Dataset: %s in project: %s ', dataset_id,\n                      dataset_project_id)\n\n        try:\n            self.service.datasets().insert(\n                projectId=dataset_project_id,\n                body=dataset_reference).execute(num_retries=self.num_retries)\n            self.log.info('Dataset created successfully: In project %s '\n                          'Dataset %s', dataset_project_id, dataset_id)\n\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content)\n            )", "gt": "        https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/insert"}
{"prefix": "def delete_dataset(self, project_id, dataset_id):\n        \"\"\"\n        Delete a dataset of Big query in your project.\n        :param project_id: The name of the project where we have the dataset .\n        :type project_id: str\n        :param dataset_id: The dataset to be delete.\n        :type dataset_id: str\n        :return:\n        \"\"\"\n        project_id = project_id if project_id is not None else self.project_id\n        self.log.info('Deleting from project: %s  Dataset:%s',\n                      project_id, dataset_id)\n", "suffix": "            self.service.datasets().delete(\n                projectId=project_id,\n                datasetId=dataset_id).execute(num_retries=self.num_retries)\n            self.log.info('Dataset deleted successfully: In project %s '\n                          'Dataset %s', project_id, dataset_id)\n\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content)\n            )", "gt": "        try:"}
{"prefix": "def get_dataset(self, dataset_id, project_id=None):\n        \"\"\"\n        Method returns dataset_resource if dataset exist\n        and raised 404 error if dataset does not exist\n\n        :param dataset_id: The BigQuery Dataset ID", "suffix": "        :param project_id: The GCP Project ID\n        :type project_id: str\n        :return: dataset_resource\n\n            .. seealso::\n                For more information, see Dataset Resource content:\n                https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets#resource\n        \"\"\"\n\n        if not dataset_id or not isinstance(dataset_id, str):\n            raise ValueError(\"dataset_id argument must be provided and has \"\n                             \"a type 'str'. You provided: {}\".format(dataset_id))\n\n        dataset_project_id = project_id if project_id else self.project_id\n\n        try:\n            dataset_resource = self.service.datasets().get(\n                datasetId=dataset_id, projectId=dataset_project_id).execute(num_retries=self.num_retries)\n            self.log.info(\"Dataset Resource: %s\", dataset_resource)\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content))\n\n        return dataset_resource", "gt": "        :type dataset_id: str"}
{"prefix": "", "suffix": "        \"\"\"\n        Method returns full list of BigQuery datasets in the current project\n\n        .. seealso::\n            For more information, see:\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/list\n\n        :param project_id: Google Cloud Project for which you\n            try to get all datasets\n        :type project_id: str\n        :return: datasets_list\n\n            Example of returned datasets_list: ::\n\n                   {\n                      \"kind\":\"bigquery#dataset\",\n                      \"location\":\"US\",\n                      \"id\":\"your-project:dataset_2_test\",\n                      \"datasetReference\":{\n                         \"projectId\":\"your-project\",\n                         \"datasetId\":\"dataset_2_test\"\n                      }\n                   },\n                   {\n                      \"kind\":\"bigquery#dataset\",\n                      \"location\":\"US\",\n                      \"id\":\"your-project:dataset_1_test\",\n                      \"datasetReference\":{\n                         \"projectId\":\"your-project\",\n                         \"datasetId\":\"dataset_1_test\"\n                      }\n                   }\n                ]\n        \"\"\"\n        dataset_project_id = project_id if project_id else self.project_id\n\n        try:\n            datasets_list = self.service.datasets().list(\n                projectId=dataset_project_id).execute(num_retries=self.num_retries)['datasets']\n            self.log.info(\"Datasets List: %s\", datasets_list)\n\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content))\n\n        return datasets_list", "gt": "def get_datasets_list(self, project_id=None):"}
{"prefix": "def insert_all(self, project_id, dataset_id, table_id,\n                   rows, ignore_unknown_values=False,\n                   skip_invalid_rows=False, fail_on_error=False):\n        \"\"\"\n        Method to stream data into BigQuery one record at a time without needing\n        to run a load job\n\n        .. seealso::\n            For more information, see:\n            https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll\n\n        :param project_id: The name of the project where we have the table\n        :type project_id: str\n        :param dataset_id: The name of the dataset where we have the table\n        :type dataset_id: str\n        :param table_id: The name of the table\n        :type table_id: str\n        :param rows: the rows to insert\n        :type rows: list\n\n        **Example or rows**:\n            rows=[{\"json\": {\"a_key\": \"a_value_0\"}}, {\"json\": {\"a_key\": \"a_value_1\"}}]\n\n        :param ignore_unknown_values: [Optional] Accept rows that contain values\n            that do not match the schema. The unknown values are ignored.\n            The default value  is false, which treats unknown values as errors.\n        :type ignore_unknown_values: bool\n        :param skip_invalid_rows: [Optional] Insert all valid rows of a request,\n            even if invalid rows exist. The default value is false, which causes\n            the entire request to fail if any invalid rows exist.\n        :type skip_invalid_rows: bool\n        :param fail_on_error: [Optional] Force the task to fail if any errors occur.\n            The default value is false, which indicates the task should not fail\n            even if any insertion errors occur.\n        :type fail_on_error: bool\n        \"\"\"\n\n        dataset_project_id = project_id if project_id else self.project_id\n\n        body = {\n            \"rows\": rows,\n            \"ignoreUnknownValues\": ignore_unknown_values,\n            \"kind\": \"bigquery#tableDataInsertAllRequest\",", "suffix": "        }\n\n        try:\n            self.log.info(\n                'Inserting %s row(s) into Table %s:%s.%s',\n                len(rows), dataset_project_id, dataset_id, table_id\n            )\n\n            resp = self.service.tabledata().insertAll(\n                projectId=dataset_project_id, datasetId=dataset_id,\n                tableId=table_id, body=body\n            ).execute(num_retries=self.num_retries)\n\n            if 'insertErrors' not in resp:\n                self.log.info(\n                    'All row(s) inserted successfully: %s:%s.%s',\n                    dataset_project_id, dataset_id, table_id\n                )\n            else:\n                error_msg = '{} insert error(s) occurred: {}:{}.{}. Details: {}'.format(\n                    len(resp['insertErrors']),\n                    dataset_project_id, dataset_id, table_id, resp['insertErrors'])\n                if fail_on_error:\n                    raise AirflowException(\n                        'BigQuery job failed. Error was: {}'.format(error_msg)\n                    )\n                self.log.info(error_msg)\n        except HttpError as err:\n            raise AirflowException(\n                'BigQuery job failed. Error was: {}'.format(err.content)\n            )", "gt": "            \"skipInvalidRows\": skip_invalid_rows,"}
{"prefix": "def execute(self, operation, parameters=None):\n        \"\"\"\n        Executes a BigQuery query, and returns the job ID.", "suffix": "        :param operation: The query to execute.\n        :type operation: str\n        :param parameters: Parameters to substitute into the query.\n        :type parameters: dict\n        \"\"\"\n        sql = _bind_parameters(operation,\n                               parameters) if parameters else operation\n        self.job_id = self.run_query(sql)", "gt": ""}
{"prefix": "def executemany(self, operation, seq_of_parameters):\n        \"\"\"\n        Execute a BigQuery query multiple times with different parameters.\n\n        :param operation: The query to execute.\n        :type operation: str\n        :param seq_of_parameters: List of dictionary parameters to substitute into the\n            query.\n        :type seq_of_parameters: list", "suffix": "        for parameters in seq_of_parameters:\n            self.execute(operation, parameters)", "gt": "        \"\"\""}
{"prefix": "def next(self):\n        \"\"\"", "suffix": "        If the buffer is empty, attempts to paginate through the result set for\n        the next page, and load it into the buffer.\n        \"\"\"\n        if not self.job_id:\n            return None\n\n        if len(self.buffer) == 0:\n            if self.all_pages_loaded:\n                return None\n\n            query_results = (self.service.jobs().getQueryResults(\n                projectId=self.project_id,\n                jobId=self.job_id,\n                pageToken=self.page_token).execute(num_retries=self.num_retries))\n\n            if 'rows' in query_results and query_results['rows']:\n                self.page_token = query_results.get('pageToken')\n                fields = query_results['schema']['fields']\n                col_types = [field['type'] for field in fields]\n                rows = query_results['rows']\n\n                for dict_row in rows:\n                    typed_row = ([\n                        _bq_cast(vs['v'], col_types[idx])\n                        for idx, vs in enumerate(dict_row['f'])\n                    ])\n                    self.buffer.append(typed_row)\n\n                if not self.page_token:\n                    self.all_pages_loaded = True\n\n            else:\n                # Reset all state since we've exhausted the results.\n                self.page_token = None\n                self.job_id = None\n                self.page_token = None\n                return None\n\n        return self.buffer.pop(0)", "gt": "        Helper method for fetchone, which returns the next row from a buffer."}
{"prefix": "def fetchmany(self, size=None):\n        \"\"\"\n        Fetch the next set of rows of a query result, returning a sequence of sequences", "suffix": "        available. The number of rows to fetch per call is specified by the parameter.\n        If it is not given, the cursor's arraysize determines the number of rows to be\n        fetched. The method should try to fetch as many rows as indicated by the size\n        parameter. If this is not possible due to the specified number of rows not being\n        available, fewer rows may be returned. An :py:class:`~pyhive.exc.Error`\n        (or subclass) exception is raised if the previous call to\n        :py:meth:`execute` did not produce any result set or no call was issued yet.\n        \"\"\"\n        if size is None:\n            size = self.arraysize\n        result = []\n        for _ in range(size):\n            one = self.fetchone()\n            if one is None:\n                break\n            else:\n                result.append(one)\n        return result", "gt": "        (e.g. a list of tuples). An empty sequence is returned when no more rows are"}
{"prefix": "def fetchall(self):\n        \"\"\"\n        Fetch all (remaining) rows of a query result, returning them as a sequence of\n        sequences (e.g. a list of tuples).\n        \"\"\"\n        result = []\n        while True:\n            one = self.fetchone()\n            if one is None:\n                break\n            else:\n                result.append(one)", "suffix": "", "gt": "        return result"}
{"prefix": "def configure_manifest_files(app):\n    \"\"\"\n    Loads the manifest file and register the `url_for_asset_` template tag.\n    :param app:\n    :return:\n    \"\"\"\n\n    def parse_manifest_json():\n        # noinspection PyBroadException\n        try:\n            global manifest\n            manifest_file = os.path.join(os.path.dirname(__file__),\n                                         'static/dist/manifest.json')\n            with open(manifest_file, 'r') as f:\n                manifest.update(json.load(f))\n\n                for k in manifest.keys():\n                    manifest[k] = os.path.join(\"dist\", manifest[k])\n        except Exception:\n            print(\"Please make sure to build the frontend in \"\n                  \"static/ directory and restart the server\")\n            pass\n\n    def get_asset_url(filename):\n        if app.debug:\n            parse_manifest_json()\n        return url_for('static', filename=manifest.get(filename, ''))\n\n    parse_manifest_json()\n\n    @app.context_processor\n    def get_url_for_asset():\n        \"\"\"", "suffix": "        WebPack renders the assets after minification and modification\n        under the static/dist folder.\n        This template tag reads the asset name in manifest.json and returns\n        the appropriate file.\n        \"\"\"\n        return dict(url_for_asset=get_asset_url)", "gt": "        Template tag to return the asset URL."}
{"prefix": "", "suffix": "        \"\"\"\n        Queries Postgres and returns a cursor to the results.\n        \"\"\"\n        postgres = PostgresHook(postgres_conn_id=self.postgres_conn_id)\n        conn = postgres.get_conn()\n        cursor = conn.cursor()\n        cursor.execute(self.sql, self.parameters)\n        return cursor", "gt": "def _query_postgres(self):"}
{"prefix": "def _write_local_data_files(self, cursor):\n        \"\"\"\n        Takes a cursor, and writes results to a local file.\n\n        :return: A dictionary where keys are filenames to be used as object", "suffix": "            contain the data for the GCS objects.\n        \"\"\"\n        schema = list(map(lambda schema_tuple: schema_tuple[0], cursor.description))\n        tmp_file_handles = {}\n        row_no = 0\n\n        def _create_new_file():\n            handle = NamedTemporaryFile(delete=True)\n            filename = self.filename.format(len(tmp_file_handles))\n            tmp_file_handles[filename] = handle\n            return handle\n\n        # Don't create a file if there is nothing to write\n        if cursor.rowcount > 0:\n            tmp_file_handle = _create_new_file()\n\n            for row in cursor:\n                # Convert datetime objects to utc seconds, and decimals to floats\n                row = map(self.convert_types, row)\n                row_dict = dict(zip(schema, row))\n\n                s = json.dumps(row_dict, sort_keys=True).encode('utf-8')\n                tmp_file_handle.write(s)\n\n                # Append newline to make dumps BigQuery compatible.\n                tmp_file_handle.write(b'\\n')\n\n                # Stop if the file exceeds the file size limit.\n                if tmp_file_handle.tell() >= self.approx_max_file_size_bytes:\n                    tmp_file_handle = _create_new_file()\n                row_no += 1\n\n        self.log.info('Received %s rows over %s files', row_no, len(tmp_file_handles))\n\n        return tmp_file_handles", "gt": "            names in GCS, and values are file handles to local files that"}
{"prefix": "def _write_local_schema_file(self, cursor):\n        \"\"\"\n        Takes a cursor, and writes the BigQuery schema for the results to a\n        local file system.\n\n        :return: A dictionary where key is a filename to be used as an object\n            name in GCS, and values are file handles to local files that\n            contains the BigQuery schema fields in .json format.\n        \"\"\"\n        schema = []\n        for field in cursor.description:\n            # See PEP 249 for details about the description tuple.\n            field_name = field[0]\n            field_type = self.type_map(field[1])\n            field_mode = 'REPEATED' if field[1] in (1009, 1005, 1007,\n                                                    1016) else 'NULLABLE'\n            schema.append({\n                'name': field_name,\n                'type': field_type,\n                'mode': field_mode,\n            })\n", "suffix": "        tmp_schema_file_handle = NamedTemporaryFile(delete=True)\n        s = json.dumps(schema, sort_keys=True).encode('utf-8')\n        tmp_schema_file_handle.write(s)\n        return {self.schema_filename: tmp_schema_file_handle}", "gt": "        self.log.info('Using schema for %s: %s', self.schema_filename, schema)"}
{"prefix": "def convert_types(cls, value):\n        \"\"\"\n        Takes a value from Postgres, and converts it to a value that's safe for\n        JSON/Google Cloud Storage/BigQuery. Dates are converted to UTC seconds.", "suffix": "        \"\"\"\n        if type(value) in (datetime.datetime, datetime.date):\n            return time.mktime(value.timetuple())\n        elif type(value) == datetime.time:\n            formated_time = time.strptime(str(value), \"%H:%M:%S\")\n            return datetime.timedelta(\n                hours=formated_time.tm_hour,\n                minutes=formated_time.tm_min,\n                seconds=formated_time.tm_sec).seconds\n        elif isinstance(value, Decimal):\n            return float(value)\n        else:\n            return value", "gt": "        Decimals are converted to floats. Times are converted to seconds."}
{"prefix": "def _make_intermediate_dirs(sftp_client, remote_directory):", "suffix": "    Create all the intermediate directories in a remote host\n\n    :param sftp_client: A Paramiko SFTP client.\n    :param remote_directory: Absolute Path of the directory containing the file\n    :return:\n    \"\"\"\n    if remote_directory == '/':\n        sftp_client.chdir('/')\n        return\n    if remote_directory == '':\n        return\n    try:\n        sftp_client.chdir(remote_directory)\n    except IOError:\n        dirname, basename = os.path.split(remote_directory.rstrip('/'))\n        _make_intermediate_dirs(sftp_client, dirname)\n        sftp_client.mkdir(basename)\n        sftp_client.chdir(basename)\n        return", "gt": "    \"\"\""}
{"prefix": "def create_queue(self, queue_name, attributes=None):\n        \"\"\"\n        Create queue using connection object\n\n        :param queue_name: name of the queue.\n        :type queue_name: str\n        :param attributes: additional attributes for the queue (default: None)\n            For details of the attributes parameter see :py:meth:`botocore.client.SQS.create_queue`\n        :type attributes: dict\n", "suffix": "            For details of the returned value see :py:meth:`botocore.client.SQS.create_queue`\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().create_queue(QueueName=queue_name, Attributes=attributes or {})", "gt": "        :return: dict with the information about the queue"}
{"prefix": "def send_message(self, queue_url, message_body, delay_seconds=0, message_attributes=None):", "suffix": "        Send message to the queue\n\n        :param queue_url: queue url\n        :type queue_url: str\n        :param message_body: the contents of the message\n        :type message_body: str\n        :param delay_seconds: seconds to delay the message\n        :type delay_seconds: int\n        :param message_attributes: additional attributes for the message (default: None)\n            For details of the attributes parameter see :py:meth:`botocore.client.SQS.send_message`\n        :type message_attributes: dict\n\n        :return: dict with the information about the message sent\n            For details of the returned value see :py:meth:`botocore.client.SQS.send_message`\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().send_message(QueueUrl=queue_url,\n                                            MessageBody=message_body,\n                                            DelaySeconds=delay_seconds,\n                                            MessageAttributes=message_attributes or {})", "gt": "        \"\"\""}
{"prefix": "def _integrate_plugins():", "suffix": "    from airflow.plugins_manager import hooks_modules\n    for hooks_module in hooks_modules:\n        sys.modules[hooks_module.__name__] = hooks_module\n        globals()[hooks_module._name] = hooks_module", "gt": "    \"\"\"Integrate plugins to the context\"\"\""}
{"prefix": "def run_command(self, run_with=None, join_args=False):", "suffix": "        Run the task command.\n\n        :param run_with: list of tokens to run the task command with e.g. ``['bash', '-c']``\n        :type run_with: list\n        :param join_args: whether to concatenate the list of command tokens e.g. ``['airflow', 'run']`` vs\n            ``['airflow run']``\n        :param join_args: bool\n        :return: the process that was run\n        :rtype: subprocess.Popen\n        \"\"\"\n        run_with = run_with or []\n        cmd = [\" \".join(self._command)] if join_args else self._command\n        full_cmd = run_with + cmd\n\n        self.log.info('Running: %s', full_cmd)\n        proc = subprocess.Popen(\n            full_cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            universal_newlines=True,\n            close_fds=True,\n            env=os.environ.copy(),\n            preexec_fn=os.setsid\n        )\n\n        # Start daemon thread to read subprocess logging output\n        log_reader = threading.Thread(\n            target=self._read_task_logs,\n            args=(proc.stdout,),\n        )\n        log_reader.daemon = True\n        log_reader.start()\n        return proc", "gt": "        \"\"\""}
{"prefix": "def on_finish(self):", "suffix": "        A callback that should be called when this is done running.\n        \"\"\"\n        if self._cfg_path and os.path.isfile(self._cfg_path):\n            if self.run_as_user:\n                subprocess.call(['sudo', 'rm', self._cfg_path], close_fds=True)\n            else:\n                os.remove(self._cfg_path)", "gt": "        \"\"\""}
{"prefix": "def _main():\n    \"\"\"\n    Parse options and process commands\n    \"\"\"\n    # Parse arguments\n    usage = \"usage: nvd3.py [options]\"\n    parser = OptionParser(usage=usage,\n                          version=(\"python-nvd3 - Charts generator with \"\n                                   \"nvd3.js and d3.js\"))", "suffix": "                      action=\"store_false\", dest=\"verbose\", default=True,\n                      help=\"don't print messages to stdout\")\n\n    (options, args) = parser.parse_args()", "gt": "    parser.add_option(\"-q\", \"--quiet\","}
{"prefix": "def add_serie(self, y, x, name=None, extra=None, **kwargs):\n        \"\"\"\n        add serie - Series are list of data that will be plotted\n        y {1, 2, 3, 4, 5} / x {1, 2, 3, 4, 5}\n\n        **Attributes**:\n\n            * ``name`` - set Serie name\n            * ``x`` - x-axis data\n            * ``y`` - y-axis data\n\n            kwargs:\n\n            * ``shape`` - for scatterChart, you can set different shapes\n                          (circle, triangle etc...)\n            * ``size`` - for scatterChart, you can set size of different shapes\n            * ``type`` - for multiChart, type should be bar\n            * ``bar`` - to display bars in Chart\n            * ``color_list`` - define list of colors which will be\n                               used by pieChart\n            * ``color`` - set axis color\n            * ``disabled`` -\n\n            extra:\n\n            * ``tooltip`` - set tooltip flag\n            * ``date_format`` - set date_format for tooltip if x-axis is in\n              date format\n\n        \"\"\"\n        if not name:\n            name = \"Serie %d\" % (self.serie_no)\n\n        # For scatterChart shape & size fields are added in serie\n        if 'shape' in kwargs or 'size' in kwargs:\n            csize = kwargs.get('size', 1)\n            cshape = kwargs.get('shape', 'circle')\n\n            serie = [{\n                'x': x[i],\n                'y': j,\n                'shape': cshape,\n                'size': csize[i] if isinstance(csize, list) else csize\n            } for i, j in enumerate(y)]\n        else:\n            if self.model == 'pieChart':\n                serie = [{'label': x[i], 'value': y} for i, y in enumerate(y)]\n            else:\n                serie = [{'x': x[i], 'y': y} for i, y in enumerate(y)]\n\n        data_keyvalue = {'values': serie, 'key': name}\n\n        # multiChart\n        # Histogram type='bar' for the series\n        if 'type' in kwargs and kwargs['type']:\n            data_keyvalue['type'] = kwargs['type']\n", "suffix": "        # a chart can have 2 Y axis, left and right, by default only one Y Axis is used\n        if 'yaxis' in kwargs and kwargs['yaxis']:\n            data_keyvalue['yAxis'] = kwargs['yaxis']\n        else:\n            if self.model != 'pieChart':\n                data_keyvalue['yAxis'] = '1'\n\n        if 'bar' in kwargs and kwargs['bar']:\n            data_keyvalue['bar'] = 'true'\n\n        if 'disabled' in kwargs and kwargs['disabled']:\n            data_keyvalue['disabled'] = 'true'\n\n        if 'color' in kwargs and kwargs['color']:\n            data_keyvalue['color'] = kwargs['color']\n\n        if extra:\n            if self.model == 'pieChart':\n                if 'color_list' in extra and extra['color_list']:\n                    self.color_list = extra['color_list']\n\n            if extra.get('date_format'):\n                self.charttooltip_dateformat = extra['date_format']\n\n            if extra.get('tooltip'):\n                self.custom_tooltip_flag = True\n\n                if self.model != 'pieChart':\n                    _start = extra['tooltip']['y_start']\n                    _end = extra['tooltip']['y_end']\n                    _start = (\"'\" + str(_start) + \"' + \") if _start else ''\n                    _end = (\" + '\" + str(_end) + \"'\") if _end else ''\n\n                    if self.model == 'linePlusBarChart':\n                        if self.tooltip_condition_string:\n                            self.tooltip_condition_string += stab(5)\n                        self.tooltip_condition_string += stab(0) + \"if(key.indexOf('\" + name + \"') > -1 ){\\n\" +\\\n                            stab(6) + \"var y = \" + _start + \" String(graph.point.y) \" + _end + \";\\n\" +\\\n                            stab(5) + \"}\\n\"\n                    elif self.model == 'cumulativeLineChart':\n                        self.tooltip_condition_string += stab(0) + \"if(key == '\" + name + \"'){\\n\" +\\\n                            stab(6) + \"var y = \" + _start + \" String(e) \" + _end + \";\\n\" +\\\n                            stab(5) + \"}\\n\"\n                    else:\n                        self.tooltip_condition_string += stab(5) + \"if(key == '\" + name + \"'){\\n\" +\\\n                            stab(6) + \"var y = \" + _start + \" String(graph.point.y) \" + _end + \";\\n\" +\\\n                            stab(5) + \"}\\n\"\n\n                if self.model == 'pieChart':\n                    _start = extra['tooltip']['y_start']\n                    _end = extra['tooltip']['y_end']\n                    _start = (\"'\" + str(_start) + \"' + \") if _start else ''\n                    _end = (\" + '\" + str(_end) + \"'\") if _end else ''\n                    self.tooltip_condition_string += \"var y = \" + _start + \" String(y) \" + _end + \";\\n\"\n\n        # Increment series counter & append\n        self.serie_no += 1\n        self.series.append(data_keyvalue)", "gt": "        # Define on which Y axis the serie is related"}
{"prefix": "def buildcontent(self):\n        \"\"\"Build HTML content only, no header or body tags. To be useful this\n        will usually require the attribute `juqery_on_ready` to be set which", "suffix": "        \"\"\"\n        self.buildcontainer()\n        # if the subclass has a method buildjs this method will be\n        # called instead of the method defined here\n        # when this subclass method is entered it does call\n        # the method buildjschart defined here\n        self.buildjschart()\n        self.htmlcontent = self.template_content_nvd3.render(chart=self)", "gt": "        will wrap the js in $(function(){<regular_js>};)"}
{"prefix": "def buildhtml(self):\n        \"\"\"Build the HTML page\n        Create the htmlheader with css / js\n        Create html page", "suffix": "        \"\"\"\n        self.buildcontent()\n        self.content = self.htmlcontent\n        self.htmlcontent = self.template_page_nvd3.render(chart=self)", "gt": "        Add Js code for nvd3"}
{"prefix": "def buildhtmlheader(self):\n        \"\"\"generate HTML header content\"\"\"\n        self.htmlheader = ''\n        # If the JavaScript assets have already been injected, don't bother re-sourcing them.\n        global _js_initialized\n        if '_js_initialized' not in globals() or not _js_initialized:\n            for css in self.header_css:\n                self.htmlheader += css\n            for js in self.header_js:", "suffix": "", "gt": "                self.htmlheader += js"}
{"prefix": "def buildcontainer(self):", "suffix": "        if self.container:\n            return\n\n        # Create SVG div with style\n        if self.width:\n            if self.width[-1] != '%':\n                self.style += 'width:%spx;' % self.width\n            else:\n                self.style += 'width:%s;' % self.width\n        if self.height:\n            if self.height[-1] != '%':\n                self.style += 'height:%spx;' % self.height\n            else:\n                self.style += 'height:%s;' % self.height\n        if self.style:\n            self.style = 'style=\"%s\"' % self.style\n\n        self.container = self.containerheader + \\\n            '<div id=\"%s\"><svg %s></svg></div>\\n' % (self.name, self.style)", "gt": "        \"\"\"generate HTML div\"\"\""}
{"prefix": "def buildjschart(self):\n        \"\"\"generate javascript code for the chart\"\"\"\n        self.jschart = ''", "suffix": "        # add custom tooltip string in jschart\n        # default condition (if build_custom_tooltip is not called explicitly with date_flag=True)\n        if self.tooltip_condition_string == '':\n            self.tooltip_condition_string = 'var y = String(graph.point.y);\\n'\n\n        # Include data\n        self.series_js = json.dumps(self.series)", "gt": ""}
{"prefix": "def create_x_axis(self, name, label=None, format=None, date=False, custom_format=False):\n        \"\"\"Create X-axis\"\"\"\n        axis = {}\n        if custom_format and format:\n            axis['tickFormat'] = format\n        elif format:\n            if format == 'AM_PM':\n                axis['tickFormat'] = \"function(d) { return get_am_pm(parseInt(d)); }\"\n            else:\n                axis['tickFormat'] = \"d3.format(',%s')\" % format\n\n        if label:\n            axis['axisLabel'] = \"'\" + label + \"'\"\n\n        # date format : see https://github.com/mbostock/d3/wiki/Time-Formatting\n        if date:\n            self.dateformat = format\n            axis['tickFormat'] = (\"function(d) { return d3.time.format('%s')\"\n                                  \"(new Date(parseInt(d))) }\\n\"\n                                  \"\" % self.dateformat)\n            # flag is the x Axis is a date\n            if name[0] == 'x':\n                self.x_axis_date = True\n\n        # Add new axis to list of axis", "suffix": "\n        # Create x2Axis if focus_enable\n        if name == \"xAxis\" and self.focus_enable:\n            self.axislist['x2Axis'] = axis", "gt": "        self.axislist[name] = axis"}
{"prefix": "def create_y_axis(self, name, label=None, format=None, custom_format=False):\n        \"\"\"\n        Create Y-axis\n        \"\"\"\n        axis = {}\n\n        if custom_format and format:\n            axis['tickFormat'] = format\n        elif format:", "suffix": "\n        if label:\n            axis['axisLabel'] = \"'\" + label + \"'\"\n\n        # Add new axis to list of axis\n        self.axislist[name] = axis", "gt": "            axis['tickFormat'] = \"d3.format(',%s')\" % format"}
{"prefix": "def buildcontent(self):\n        \"\"\"Build HTML content only, no header or body tags. To be useful this\n        will usually require the attribute `juqery_on_ready` to be set which\n        will wrap the js in $(function(){<regular_js>};)\n        \"\"\"\n        self.buildcontainer()\n        # if the subclass has a method buildjs this method will be\n        # called instead of the method defined here\n        # when this subclass method is entered it does call", "suffix": "        self.buildjschart()\n        self.htmlcontent = self.template_chart_nvd3.render(chart=self)", "gt": "        # the method buildjschart defined here"}
{"prefix": "def get_conn(self):\n        \"\"\"", "suffix": "        \"\"\"\n        conn = self.get_connection(self.sqlite_conn_id)\n        conn = sqlite3.connect(conn.host)\n        return conn", "gt": "        Returns a sqlite connection object"}
{"prefix": "def action_logging(f):\n    \"\"\"\n    Decorator to log user actions\n    \"\"\"\n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n\n        with create_session() as session:\n            if g.user.is_anonymous:", "suffix": "            else:\n                user = g.user.username\n\n            log = Log(\n                event=f.__name__,\n                task_instance=None,\n                owner=user,\n                extra=str(list(request.args.items())),\n                task_id=request.args.get('task_id'),\n                dag_id=request.args.get('dag_id'))\n\n            if 'execution_date' in request.args:\n                log.execution_date = pendulum.parse(\n                    request.args.get('execution_date'))\n\n            session.add(log)\n\n        return f(*args, **kwargs)\n\n    return wrapper", "gt": "                user = 'anonymous'"}
{"prefix": "def gzipped(f):\n    \"\"\"\n    Decorator to make a view compressed\n    \"\"\"\n    @functools.wraps(f)\n    def view_func(*args, **kwargs):\n        @after_this_request\n        def zipper(response):\n            accept_encoding = request.headers.get('Accept-Encoding', '')\n\n            if 'gzip' not in accept_encoding.lower():\n                return response\n\n            response.direct_passthrough = False\n\n            if (response.status_code < 200 or response.status_code >= 300 or\n                    'Content-Encoding' in response.headers):", "suffix": "            gzip_buffer = IO()\n            gzip_file = gzip.GzipFile(mode='wb',\n                                      fileobj=gzip_buffer)\n            gzip_file.write(response.data)\n            gzip_file.close()\n\n            response.data = gzip_buffer.getvalue()\n            response.headers['Content-Encoding'] = 'gzip'\n            response.headers['Vary'] = 'Accept-Encoding'\n            response.headers['Content-Length'] = len(response.data)\n\n            return response\n\n        return f(*args, **kwargs)\n\n    return view_func", "gt": "                return response"}
{"prefix": "def has_dag_access(**dag_kwargs):\n    \"\"\"\n    Decorator to check whether the user has read / write permission on the dag.\n    \"\"\"\n    def decorator(f):\n        @functools.wraps(f)\n        def wrapper(self, *args, **kwargs):\n            has_access = self.appbuilder.sm.has_access\n            dag_id = request.args.get('dag_id')\n            # if it is false, we need to check whether user has write access on the dag\n            can_dag_edit = dag_kwargs.get('can_dag_edit', False)\n\n            # 1. check whether the user has can_dag_edit permissions on all_dags\n            # 2. if 1 false, check whether the user\n            #    has can_dag_edit permissions on the dag\n            # 3. if 2 false, check whether it is can_dag_read view,\n            #    and whether user has the permissions\n            if (\n                has_access('can_dag_edit', 'all_dags') or\n                has_access('can_dag_edit', dag_id) or (not can_dag_edit and\n                                                       (has_access('can_dag_read',\n                                                                   'all_dags') or\n                                                        has_access('can_dag_read',\n                                                                   dag_id)))):\n                return f(self, *args, **kwargs)", "suffix": "                flash(\"Access is Denied\", \"danger\")\n                return redirect(url_for(self.appbuilder.sm.auth_view.\n                                        __class__.__name__ + \".login\"))\n        return wrapper\n    return decorator", "gt": "            else:"}
{"prefix": "def get_last_dagrun(dag_id, session, include_externally_triggered=False):\n    \"\"\"\n    Returns the last dag run for a dag, None if there was none.\n    Last dag run can be any type of run eg. scheduled or backfilled.\n    Overridden DagRuns are ignored.\n    \"\"\"", "suffix": "    query = session.query(DR).filter(DR.dag_id == dag_id)\n    if not include_externally_triggered:\n        query = query.filter(DR.external_trigger == False)  # noqa\n    query = query.order_by(DR.execution_date.desc())\n    return query.first()", "gt": "    DR = DagRun"}
{"prefix": "def create_dagrun(self,\n                      run_id,\n                      state,\n                      execution_date,\n                      start_date=None,\n                      external_trigger=False,\n                      conf=None,\n                      session=None):\n        \"\"\"\n        Creates a dag run from this dag including the tasks associated with this dag.\n        Returns the dag run.\n", "suffix": "        :type run_id: str\n        :param execution_date: the execution date of this dag run\n        :type execution_date: datetime.datetime\n        :param state: the state of the dag run\n        :type state: airflow.utils.state.State\n        :param start_date: the date this dag run should be evaluated\n        :type start_date: datetime.datetime\n        :param external_trigger: whether this dag run is externally triggered\n        :type external_trigger: bool\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        \"\"\"\n\n        return self.get_dag().create_dagrun(run_id=run_id,\n                                            state=state,\n                                            execution_date=execution_date,\n                                            start_date=start_date,\n                                            external_trigger=external_trigger,\n                                            conf=conf,\n                                            session=session)", "gt": "        :param run_id: defines the the run id for this dag run"}
{"prefix": "def execute(self, context):\n        \"\"\"\n        Publish the message to SQS queue\n\n        :param context: the context object\n        :type context: dict\n        :return: dict with information about the message sent\n            For details of the returned dict see :py:meth:`botocore.client.SQS.send_message`\n        :rtype: dict\n        \"\"\"\n\n        hook = SQSHook(aws_conn_id=self.aws_conn_id)\n\n        result = hook.send_message(queue_url=self.sqs_queue,\n                                   message_body=self.message_content,\n                                   delay_seconds=self.delay_seconds,\n                                   message_attributes=self.message_attributes)\n\n        self.log.info('result is send_message is %s', result)", "suffix": "        return result", "gt": ""}
{"prefix": "def generate_pages(current_page, num_of_pages,\n                   search=None, showPaused=None, window=7):\n    \"\"\"\n    Generates the HTML for a paging component using a similar logic to the paging\n    auto-generated by Flask managed views. The paging component defines a number of\n    pages visible in the pager (window) and once the user goes to a page beyond the\n    largest visible, it would scroll to the right the page numbers and keeps the\n    current one in the middle of the pager component. When in the last pages,\n    the pages won't scroll and just keep moving until the last page. Pager also contains\n    <first, previous, ..., next, last> pages.\n    This component takes into account custom parameters such as search and showPaused,\n    which could be added to the pages link in order to maintain the state between\n    client and server. It also allows to make a bookmark on a specific paging state.\n    :param current_page:\n        the current page number, 0-indexed\n    :param num_of_pages:\n        the total number of pages\n    :param search:\n        the search query string, if any\n    :param showPaused:\n        false if paused dags will be hidden, otherwise true to show them\n    :param window:\n        the number of pages to be shown in the paging component (7 default)\n    :return:\n        the HTML string of the paging component\n    \"\"\"\n\n    void_link = 'javascript:void(0)'\n    first_node = Markup(\"\"\"<li class=\"paginate_button {disabled}\" id=\"dags_first\">\n    <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"0\" tabindex=\"0\">&laquo;</a>\n</li>\"\"\")\n\n    previous_node = Markup(\"\"\"<li class=\"paginate_button previous {disabled}\" id=\"dags_previous\">\n    <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"0\" tabindex=\"0\">&lt;</a>\n</li>\"\"\")\n\n    next_node = Markup(\"\"\"<li class=\"paginate_button next {disabled}\" id=\"dags_next\">\n    <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"3\" tabindex=\"0\">&gt;</a>\n</li>\"\"\")\n\n    last_node = Markup(\"\"\"<li class=\"paginate_button {disabled}\" id=\"dags_last\">\n    <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"3\" tabindex=\"0\">&raquo;</a>\n</li>\"\"\")\n\n    page_node = Markup(\"\"\"<li class=\"paginate_button {is_active}\">\n    <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"2\" tabindex=\"0\">{page_num}</a>\n</li>\"\"\")\n\n    output = [Markup('<ul class=\"pagination\" style=\"margin-top:0px;\">')]\n\n    is_disabled = 'disabled' if current_page <= 0 else ''\n    output.append(first_node.format(href_link=\"?{}\"\n                                    .format(get_params(page=0,\n                                                       search=search,\n                                                       showPaused=showPaused)),\n                                    disabled=is_disabled))\n\n    page_link = void_link\n    if current_page > 0:\n        page_link = '?{}'.format(get_params(page=(current_page - 1),\n                                            search=search,\n                                            showPaused=showPaused))\n\n    output.append(previous_node.format(href_link=page_link,\n                                       disabled=is_disabled))\n\n    mid = int(window / 2)\n    last_page = num_of_pages - 1\n\n    if current_page <= mid or num_of_pages < window:\n        pages = [i for i in range(0, min(num_of_pages, window))]\n    elif mid < current_page < last_page - mid:\n        pages = [i for i in range(current_page - mid, current_page + mid + 1)]\n    else:\n        pages = [i for i in range(num_of_pages - window, last_page + 1)]\n\n    def is_current(current, page):\n        return page == current\n\n    for page in pages:\n        vals = {\n            'is_active': 'active' if is_current(current_page, page) else '',\n            'href_link': void_link if is_current(current_page, page)\n                         else '?{}'.format(get_params(page=page,\n                                                      search=search,\n                                                      showPaused=showPaused)),\n            'page_num': page + 1\n        }\n        output.append(page_node.format(**vals))\n", "suffix": "\n    page_link = (void_link if current_page >= num_of_pages - 1\n                 else '?{}'.format(get_params(page=current_page + 1,\n                                              search=search,\n                                              showPaused=showPaused)))\n\n    output.append(next_node.format(href_link=page_link, disabled=is_disabled))\n    output.append(last_node.format(href_link=\"?{}\"\n                                   .format(get_params(page=last_page,\n                                                      search=search,\n                                                      showPaused=showPaused)),\n                                   disabled=is_disabled))\n\n    output.append(Markup('</ul>'))\n\n    return Markup('\\n'.join(output))", "gt": "    is_disabled = 'disabled' if current_page >= num_of_pages - 1 else ''"}
{"prefix": "def json_response(obj):\n    \"\"\"\n    returns a json response from a json serializable python object\n    \"\"\"\n    return Response(\n        response=json.dumps(\n            obj, indent=4, cls=AirflowJsonEncoder),\n        status=200,", "suffix": "", "gt": "        mimetype=\"application/json\")"}
{"prefix": "", "suffix": "    \"\"\"\n    Opens the given file. If the path contains a folder with a .zip suffix, then\n    the folder is treated as a zip archive, opening the file inside the archive.\n\n    :return: a file object, as in `open`, or as in `ZipFile.open`.\n    \"\"\"\n\n    _, archive, filename = ZIP_REGEX.search(f).groups()\n    if archive and zipfile.is_zipfile(archive):\n        return zipfile.ZipFile(archive, mode=mode).open(filename)\n    else:\n        return io.open(f, mode=mode)", "gt": "def open_maybe_zipped(f, mode='r'):"}
{"prefix": "def make_cache_key(*args, **kwargs):\n    \"\"\"", "suffix": "    \"\"\"\n    path = request.path\n    args = str(hash(frozenset(request.args.items())))\n    return (path + args).encode('ascii', 'ignore')", "gt": "    Used by cache to get a unique key per URL"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Returns Gcp Video Intelligence Service client\n", "suffix": "        \"\"\"\n        if not self._conn:\n            self._conn = VideoIntelligenceServiceClient(credentials=self._get_credentials())\n        return self._conn", "gt": "        :rtype: google.cloud.videointelligence_v1.VideoIntelligenceServiceClient"}
{"prefix": "def annotate_video(\n        self,\n        input_uri=None,\n        input_content=None,\n        features=None,\n        video_context=None,\n        output_uri=None,\n        location=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        Performs video annotation.\n\n        :param input_uri: Input video location. Currently, only Google Cloud Storage URIs are supported,\n            which must be specified in the following format: ``gs://bucket-id/object-id``.\n        :type input_uri: str\n        :param input_content: The video data bytes.\n            If unset, the input video(s) should be specified via ``input_uri``.\n            If set, ``input_uri`` should be unset.\n        :type input_content: bytes", "suffix": "        :type features: list[google.cloud.videointelligence_v1.VideoIntelligenceServiceClient.enums.Feature]\n        :param output_uri: Optional, location where the output (in JSON format) should be stored. Currently,\n            only Google Cloud Storage URIs are supported, which must be specified in the following format:\n            ``gs://bucket-id/object-id``.\n        :type output_uri: str\n        :param video_context: Optional, Additional video context and/or feature-specific parameters.\n        :type video_context: dict or google.cloud.videointelligence_v1.types.VideoContext\n        :param location: Optional, cloud region where annotation should take place. Supported cloud regions:\n            us-east1, us-west1, europe-west1, asia-east1.\n            If no region is specified, a region will be determined based on video file location.\n        :type location: str\n        :param retry: Retry object used to determine when/if to retry requests.\n            If None is specified, requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: Optional, The amount of time, in seconds, to wait for the request to complete.\n            Note that if retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        :param metadata: Optional, Additional metadata that is provided to the method.\n        :type metadata: seq[tuple[str, str]]\n        \"\"\"\n        client = self.get_conn()\n        return client.annotate_video(\n            input_uri=input_uri,\n            input_content=input_content,\n            features=features,\n            video_context=video_context,\n            output_uri=output_uri,\n            location_id=location,\n            retry=retry,\n            timeout=timeout,\n            metadata=metadata,\n        )", "gt": "        :param features: Requested video annotation features."}
{"prefix": "def _get_api_key(self):\n        \"\"\"\n        Get Opsgenie api_key for creating alert\n        \"\"\"\n        conn = self.get_connection(self.http_conn_id)", "suffix": "        if not api_key:\n            raise AirflowException('Opsgenie API Key is required for this hook, '\n                                   'please check your conn_id configuration.')\n        return api_key", "gt": "        api_key = conn.password"}
{"prefix": "def get_conn(self, headers=None):\n        \"\"\"\n        Overwrite HttpHook get_conn because this hook just needs base_url\n        and headers, and does not need generic params\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        \"\"\"\n        conn = self.get_connection(self.http_conn_id)", "suffix": "        session = requests.Session()\n        if headers:\n            session.headers.update(headers)\n        return session", "gt": "        self.base_url = conn.host if conn.host else 'https://api.opsgenie.com'"}
{"prefix": "def execute(self, payload={}):\n        \"\"\"\n        Execute the Opsgenie Alert call\n\n        :param payload: Opsgenie API Create Alert payload values\n            See https://docs.opsgenie.com/docs/alert-api#section-create-alert", "suffix": "        \"\"\"\n        api_key = self._get_api_key()\n        return self.run(endpoint='v2/alerts',\n                        data=json.dumps(payload),\n                        headers={'Content-Type': 'application/json',\n                                 'Authorization': 'GenieKey %s' % api_key})", "gt": "        :type payload: dict"}
{"prefix": "def poke(self, context):\n        \"\"\"\n        Execute the bash command in a temporary directory\n        which will be cleaned afterwards\n        \"\"\"\n        bash_command = self.bash_command\n        self.log.info(\"Tmp dir root location: \\n %s\", gettempdir())\n        with TemporaryDirectory(prefix='airflowtmp') as tmp_dir:\n            with NamedTemporaryFile(dir=tmp_dir, prefix=self.task_id) as f:\n                f.write(bytes(bash_command, 'utf_8'))\n                f.flush()\n                fname = f.name\n                script_location = tmp_dir + \"/\" + fname\n                self.log.info(\"Temporary script location: %s\", script_location)\n                self.log.info(\"Running command: %s\", bash_command)", "suffix": "                    ['bash', fname],\n                    stdout=PIPE, stderr=STDOUT,\n                    close_fds=True, cwd=tmp_dir,\n                    env=self.env, preexec_fn=os.setsid)\n\n                self.sp = sp\n\n                self.log.info(\"Output:\")\n                line = ''\n                for line in iter(sp.stdout.readline, b''):\n                    line = line.decode(self.output_encoding).strip()\n                    self.log.info(line)\n                sp.wait()\n                self.log.info(\"Command exited with return code %s\", sp.returncode)\n\n                return not sp.returncode", "gt": "                sp = Popen("}
{"prefix": "def _build_opsgenie_payload(self):\n        \"\"\"\n        Construct the Opsgenie JSON payload. All relevant parameters are combined here\n        to a valid Opsgenie JSON payload.\n\n        :return: Opsgenie payload (dict) to send\n        \"\"\"\n        payload = {}\n\n        for key in [\n            \"message\", \"alias\", \"description\", \"responders\",\n            \"visibleTo\", \"actions\", \"tags\", \"details\", \"entity\",\n            \"source\", \"priority\", \"user\", \"note\"\n        ]:\n            val = getattr(self, key)\n            if val:", "suffix": "        return payload", "gt": "                payload[key] = val"}
{"prefix": "def execute(self, context):\n        \"\"\"\n        Call the OpsgenieAlertHook to post message", "suffix": "        self.hook = OpsgenieAlertHook(self.opsgenie_conn_id)\n        self.hook.execute(self._build_opsgenie_payload())", "gt": "        \"\"\""}
{"prefix": "def get_conn(self):\n        \"\"\"\n        check if aws conn exists already or create one and return it\n\n        :return: boto3 session\n        \"\"\"", "suffix": "            self.conn = self.get_client_type('athena')\n        return self.conn", "gt": "        if not self.conn:"}
{"prefix": "def run_query(self, query, query_context, result_configuration, client_request_token=None):\n        \"\"\"\n        Run Presto query on athena with provided config and return submitted query_execution_id\n\n        :param query: Presto query to run", "suffix": "        :param query_context: Context in which query need to be run\n        :type query_context: dict\n        :param result_configuration: Dict with path to store results in and config related to encryption\n        :type result_configuration: dict\n        :param client_request_token: Unique token created by user to avoid multiple executions of same query\n        :type client_request_token: str\n        :return: str\n        \"\"\"\n        response = self.conn.start_query_execution(QueryString=query,\n                                                   ClientRequestToken=client_request_token,\n                                                   QueryExecutionContext=query_context,\n                                                   ResultConfiguration=result_configuration)\n        query_execution_id = response['QueryExecutionId']\n        return query_execution_id", "gt": "        :type query: str"}
{"prefix": "def check_query_status(self, query_execution_id):\n        \"\"\"\n        Fetch the status of submitted athena query. Returns None or one of valid query states.", "suffix": "        :param query_execution_id: Id of submitted athena query\n        :type query_execution_id: str\n        :return: str\n        \"\"\"\n        response = self.conn.get_query_execution(QueryExecutionId=query_execution_id)\n        state = None\n        try:\n            state = response['QueryExecution']['Status']['State']\n        except Exception as ex:\n            self.log.error('Exception while getting query state', ex)\n        finally:\n            return state", "gt": ""}
{"prefix": "def get_query_results(self, query_execution_id):\n        \"\"\"\n        Fetch submitted athena query results. returns none if query is in intermediate state or\n        failed/cancelled state else dict of query output\n\n        :param query_execution_id: Id of submitted athena query", "suffix": "        :return: dict\n        \"\"\"\n        query_state = self.check_query_status(query_execution_id)\n        if query_state is None:\n            self.log.error('Invalid Query state')\n            return None\n        elif query_state in self.INTERMEDIATE_STATES or query_state in self.FAILURE_STATES:\n            self.log.error('Query is in {state} state. Cannot fetch results'.format(state=query_state))\n            return None\n        return self.conn.get_query_results(QueryExecutionId=query_execution_id)", "gt": "        :type query_execution_id: str"}
{"prefix": "def poll_query_status(self, query_execution_id, max_tries=None):\n        \"\"\"\n        Poll the status of submitted athena query until query state reaches final state.\n        Returns one of the final states\n\n        :param query_execution_id: Id of submitted athena query\n        :type query_execution_id: str\n        :param max_tries: Number of times to poll for query state before function exits\n        :type max_tries: int\n        :return: str\n        \"\"\"\n        try_number = 1\n        final_query_state = None  # Query state when query reaches final state or max_tries reached\n        while True:\n            query_state = self.check_query_status(query_execution_id)\n            if query_state is None:\n                self.log.info('Trial {try_number}: Invalid query state. Retrying again'.format(\n                    try_number=try_number))\n            elif query_state in self.INTERMEDIATE_STATES:\n                self.log.info('Trial {try_number}: Query is still in an intermediate state - {state}'", "suffix": "            else:\n                self.log.info('Trial {try_number}: Query execution completed. Final state is {state}'\n                              .format(try_number=try_number, state=query_state))\n                final_query_state = query_state\n                break\n            if max_tries and try_number >= max_tries:  # Break loop if max_tries reached\n                final_query_state = query_state\n                break\n            try_number += 1\n            sleep(self.sleep_time)\n        return final_query_state", "gt": "                              .format(try_number=try_number, state=query_state))"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Returns an SFTP connection object\n        \"\"\"\n        if self.conn is None:\n            cnopts = pysftp.CnOpts()\n            if self.no_host_key_check:\n                cnopts.hostkeys = None\n            cnopts.compression = self.compress\n            conn_params = {\n                'host': self.remote_host,\n                'port': self.port,\n                'username': self.username,", "suffix": "            }\n            if self.password and self.password.strip():\n                conn_params['password'] = self.password\n            if self.key_file:\n                conn_params['private_key'] = self.key_file\n            if self.private_key_pass:\n                conn_params['private_key_pass'] = self.private_key_pass\n\n            self.conn = pysftp.Connection(**conn_params)\n        return self.conn", "gt": "                'cnopts': cnopts"}
{"prefix": "def describe_directory(self, path):\n        \"\"\"\n        Returns a dictionary of {filename: {attributes}} for all files\n        on the remote system (where the MLSD command is supported).\n        :param path: full path to the remote directory\n        :type path: str\n        \"\"\"\n        conn = self.get_conn()\n        flist = conn.listdir_attr(path)", "suffix": "        for f in flist:\n            modify = datetime.datetime.fromtimestamp(\n                f.st_mtime).strftime('%Y%m%d%H%M%S')\n            files[f.filename] = {\n                'size': f.st_size,\n                'type': 'dir' if stat.S_ISDIR(f.st_mode) else 'file',\n                'modify': modify}\n        return files", "gt": "        files = {}"}
{"prefix": "def list_directory(self, path):\n        \"\"\"\n        Returns a list of files on the remote system.\n        :param path: full path to the remote directory to list\n        :type path: str\n        \"\"\"\n        conn = self.get_conn()\n        files = conn.listdir(path)", "suffix": "", "gt": "        return files"}
{"prefix": "def create_directory(self, path, mode=777):\n        \"\"\"", "suffix": "        :param path: full path to the remote directory to create\n        :type path: str\n        :param mode: int representation of octal mode for directory\n        \"\"\"\n        conn = self.get_conn()\n        conn.mkdir(path, mode)", "gt": "        Creates a directory on the remote system."}
{"prefix": "def retrieve_file(self, remote_full_path, local_full_path):\n        \"\"\"\n        Transfers the remote file to a local location.\n        If local_full_path is a string path, the file will be put\n        at that location\n        :param remote_full_path: full path to the remote file\n        :type remote_full_path: str\n        :param local_full_path: full path to the local file\n        :type local_full_path: str", "suffix": "        conn = self.get_conn()\n        self.log.info('Retrieving file from FTP: %s', remote_full_path)\n        conn.get(remote_full_path, local_full_path)\n        self.log.info('Finished retrieving file from FTP: %s', remote_full_path)", "gt": "        \"\"\""}
{"prefix": "def store_file(self, remote_full_path, local_full_path):\n        \"\"\"\n        Transfers a local file to the remote location.\n        If local_full_path_or_buffer is a string path, the file will be read\n        from that location\n        :param remote_full_path: full path to the remote file", "suffix": "        :param local_full_path: full path to the local file\n        :type local_full_path: str\n        \"\"\"\n        conn = self.get_conn()\n        conn.put(local_full_path, remote_full_path)", "gt": "        :type remote_full_path: str"}
{"prefix": "def __handle_rate_limit_exception(self, rate_limit_exception):\n        \"\"\"", "suffix": "        for 60 seconds.\n        \"\"\"\n        retry_after = int(\n            rate_limit_exception.response.headers.get('Retry-After', 60))\n        self.log.info(\n            \"Hit Zendesk API rate limit. Pausing for %s seconds\",\n            retry_after\n        )\n        time.sleep(retry_after)", "gt": "        Sleep for the time specified in the exception. If not specified, wait"}
{"prefix": "def call(self, path, query=None, get_all_pages=True, side_loading=False):\n        \"\"\"\n        Call Zendesk API and return results\n\n        :param path: The Zendesk API to call\n        :param query: Query parameters\n        :param get_all_pages: Accumulate results over all pages before\n               returning. Due to strict rate limiting, this can often timeout.\n               Waits for recommended period between tries after a timeout.\n        :param side_loading: Retrieve related records as part of a single\n               request. In order to enable side-loading, add an 'include'\n               query parameter containing a comma-separated list of resources\n               to load. For more information on side-loading see\n               https://developer.zendesk.com/rest_api/docs/core/side_loading\n        \"\"\"\n        zendesk = self.get_conn()\n        first_request_successful = False\n\n        while not first_request_successful:\n            try:\n                results = zendesk.call(path, query)\n                first_request_successful = True\n            except RateLimitError as rle:\n                self.__handle_rate_limit_exception(rle)\n\n        # Find the key with the results\n        keys = [path.split(\"/\")[-1].split(\".json\")[0]]\n        next_page = results['next_page']\n        if side_loading:\n            keys += query['include'].split(',')\n        results = {key: results[key] for key in keys}", "suffix": "        if get_all_pages:\n            while next_page is not None:\n                try:\n                    # Need to split because the next page URL has\n                    # `github.zendesk...`\n                    # in it, but the call function needs it removed.\n                    next_url = next_page.split(self.__url)[1]\n                    self.log.info(\"Calling %s\", next_url)\n                    more_res = zendesk.call(next_url)\n                    for key in results:\n                        results[key].extend(more_res[key])\n                    if next_page == more_res['next_page']:\n                        # Unfortunately zdesk doesn't always throw ZendeskError\n                        # when we are done getting all the data. Sometimes the\n                        # next just refers to the current set of results.\n                        # Hence, need to deal with this special case\n                        break\n                    else:\n                        next_page = more_res['next_page']\n                except RateLimitError as rle:\n                    self.__handle_rate_limit_exception(rle)\n                except ZendeskError as ze:\n                    if b\"Use a start_time older than 5 minutes\" in ze.msg:\n                        # We have pretty up to date data\n                        break\n                    else:\n                        raise ze\n\n        return results", "gt": ""}
{"prefix": "def get_partitions(self,\n                       database_name,\n                       table_name,\n                       expression='',\n                       page_size=None,\n                       max_items=None):\n        \"\"\"\n        Retrieves the partition values for a table.\n\n        :param database_name: The name of the catalog database where the partitions reside.\n        :type database_name: str\n        :param table_name: The name of the partitions' table.\n        :type table_name: str\n        :param expression: An expression filtering the partitions to be returned.\n            Please see official AWS documentation for further information.\n            https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-catalog-partitions.html#aws-glue-api-catalog-partitions-GetPartitions\n        :type expression: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int\n        :return: set of partition values where each value is a tuple since\n            a partition may be composed of multiple columns. For example:\n            ``{('2018-01-01','1'), ('2018-01-01','2')}``\n        \"\"\"\n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('get_partitions')\n        response = paginator.paginate(\n            DatabaseName=database_name,\n            TableName=table_name,", "suffix": "            PaginationConfig=config\n        )\n\n        partitions = set()\n        for page in response:\n            for p in page['Partitions']:\n                partitions.add(tuple(p['Values']))\n\n        return partitions", "gt": "            Expression=expression,"}
{"prefix": "def check_for_partition(self, database_name, table_name, expression):", "suffix": "        Checks whether a partition exists\n\n        :param database_name: Name of hive database (schema) @table belongs to\n        :type database_name: str\n        :param table_name: Name of hive table @partition belongs to\n        :type table_name: str\n        :expression: Expression that matches the partitions to check for\n            (eg `a = 'b' AND c = 'd'`)\n        :type expression: str\n        :rtype: bool\n\n        >>> hook = AwsGlueCatalogHook()\n        >>> t = 'static_babynames_partitioned'\n        >>> hook.check_for_partition('airflow', t, \"ds='2015-01-01'\")\n        True\n        \"\"\"\n        partitions = self.get_partitions(database_name, table_name, expression, max_items=1)\n\n        if partitions:\n            return True\n        else:\n            return False", "gt": "        \"\"\""}
{"prefix": "def get_table(self, database_name, table_name):\n        \"\"\"\n        Get the information of the table\n\n        :param database_name: Name of hive database (schema) @table belongs to\n        :type database_name: str\n        :param table_name: Name of hive table\n        :type table_name: str\n        :rtype: dict\n\n        >>> hook = AwsGlueCatalogHook()", "suffix": "        >>> r['Name'] = 'table_foo'\n        \"\"\"\n\n        result = self.get_conn().get_table(DatabaseName=database_name, Name=table_name)\n\n        return result['Table']", "gt": "        >>> r = hook.get_table('db', 'table_foo')"}
{"prefix": "def get_table_location(self, database_name, table_name):\n        \"\"\"\n        Get the physical location of the table\n", "suffix": "        :type database_name: str\n        :param table_name: Name of hive table\n        :type table_name: str\n        :return: str\n        \"\"\"\n\n        table = self.get_table(database_name, table_name)\n\n        return table['StorageDescriptor']['Location']", "gt": "        :param database_name: Name of hive database (schema) @table belongs to"}
{"prefix": "def cluster_status(self, cluster_identifier):\n        \"\"\"\n        Return status of a cluster\n\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        \"\"\"\n        conn = self.get_conn()", "suffix": "            response = conn.describe_clusters(\n                ClusterIdentifier=cluster_identifier)['Clusters']\n            return response[0]['ClusterStatus'] if response else None\n        except conn.exceptions.ClusterNotFoundFault:\n            return 'cluster_not_found'", "gt": "        try:"}
{"prefix": "def delete_cluster(", "suffix": "            cluster_identifier,\n            skip_final_cluster_snapshot=True,\n            final_cluster_snapshot_identifier=''):\n        \"\"\"\n        Delete a cluster and optionally create a snapshot\n\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        :param skip_final_cluster_snapshot: determines cluster snapshot creation\n        :type skip_final_cluster_snapshot: bool\n        :param final_cluster_snapshot_identifier: name of final cluster snapshot\n        :type final_cluster_snapshot_identifier: str\n        \"\"\"\n        response = self.get_conn().delete_cluster(\n            ClusterIdentifier=cluster_identifier,\n            SkipFinalClusterSnapshot=skip_final_cluster_snapshot,\n            FinalClusterSnapshotIdentifier=final_cluster_snapshot_identifier\n        )\n        return response['Cluster'] if response['Cluster'] else None", "gt": "            self,"}
{"prefix": "def describe_cluster_snapshots(self, cluster_identifier):\n        \"\"\"\n        Gets a list of snapshots for a cluster\n\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        \"\"\"\n        response = self.get_conn().describe_cluster_snapshots(\n            ClusterIdentifier=cluster_identifier\n        )\n        if 'Snapshots' not in response:\n            return None", "suffix": "        snapshots = filter(lambda x: x['Status'], snapshots)\n        snapshots.sort(key=lambda x: x['SnapshotCreateTime'], reverse=True)\n        return snapshots", "gt": "        snapshots = response['Snapshots']"}
{"prefix": "", "suffix": "        \"\"\"\n        Restores a cluster from its snapshot\n\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        :param snapshot_identifier: unique identifier for a snapshot of a cluster\n        :type snapshot_identifier: str\n        \"\"\"\n        response = self.get_conn().restore_from_cluster_snapshot(\n            ClusterIdentifier=cluster_identifier,\n            SnapshotIdentifier=snapshot_identifier\n        )\n        return response['Cluster'] if response['Cluster'] else None", "gt": "def restore_from_cluster_snapshot(self, cluster_identifier, snapshot_identifier):"}
{"prefix": "def create_cluster_snapshot(self, snapshot_identifier, cluster_identifier):\n        \"\"\"\n        Creates a snapshot of a cluster\n", "suffix": "        :type snapshot_identifier: str\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        \"\"\"\n        response = self.get_conn().create_cluster_snapshot(\n            SnapshotIdentifier=snapshot_identifier,\n            ClusterIdentifier=cluster_identifier,\n        )\n        return response['Snapshot'] if response['Snapshot'] else None", "gt": "        :param snapshot_identifier: unique identifier for a snapshot of a cluster"}
{"prefix": "def execute(self, **kwargs):\n        \"\"\"\n        SlackAPIOperator calls will not fail even if the call is not unsuccessful.\n        It should not prevent a DAG from completing in success\n        \"\"\"\n        if not self.api_params:\n            self.construct_api_call_params()\n        slack = SlackHook(token=self.token, slack_conn_id=self.slack_conn_id)", "suffix": "", "gt": "        slack.call(self.method, self.api_params)"}
{"prefix": "def add_volume(self, volume):\n        \"\"\"\n        Args:\n            volume (Volume):", "suffix": "\n        self._add_volume(name=volume.name, configs=volume.configs)", "gt": "        \"\"\""}
{"prefix": "def add_mount(self,", "suffix": "        \"\"\"\n        Args:\n            volume_mount (VolumeMount):\n        \"\"\"\n        self._add_mount(\n            name=volume_mount.name,\n            mount_path=volume_mount.mount_path,\n            sub_path=volume_mount.sub_path,\n            read_only=volume_mount.read_only\n        )", "gt": "                  volume_mount):"}
{"prefix": "def create_job_flow(self, job_flow_overrides):\n        \"\"\"\n        Creates a job flow using the config from the EMR connection.\n        Keys of the json extra hash may have the arguments of the boto3\n        run_job_flow method.\n        Overrides for this config may be passed as the job_flow_overrides.\n        \"\"\"\n", "suffix": "            raise AirflowException('emr_conn_id must be present to use create_job_flow')\n\n        emr_conn = self.get_connection(self.emr_conn_id)\n\n        config = emr_conn.extra_dejson.copy()\n        config.update(job_flow_overrides)\n\n        response = self.get_conn().run_job_flow(**config)\n\n        return response", "gt": "        if not self.emr_conn_id:"}
{"prefix": "def filter_for_filesize(result, size=None):\n        \"\"\"\n        Will test the filepath result and test if its size is at least self.filesize\n\n        :param result: a list of dicts returned by Snakebite ls", "suffix": "        :return: (bool) depending on the matching criteria\n        \"\"\"\n        if size:\n            log = LoggingMixin().log\n            log.debug(\n                'Filtering for file size >= %s in files: %s',\n                size, map(lambda x: x['path'], result)\n            )\n            size *= settings.MEGABYTE\n            result = [x for x in result if x['length'] >= size]\n            log.debug('HdfsSensor.poke: after size filter result is %s', result)\n        return result", "gt": "        :param size: the file size in MB a file should be at least to trigger True"}
{"prefix": "", "suffix": "        \"\"\"\n        Will filter if instructed to do so the result to remove matching criteria\n\n        :param result: list of dicts returned by Snakebite ls\n        :type result: list[dict]\n        :param ignored_ext: list of ignored extensions\n        :type ignored_ext: list\n        :param ignore_copying: shall we ignore ?\n        :type ignore_copying: bool\n        :return: list of dicts which were not removed\n        :rtype: list[dict]\n        \"\"\"\n        if ignore_copying:\n            log = LoggingMixin().log\n            regex_builder = r\"^.*\\.(%s$)$\" % '$|'.join(ignored_ext)\n            ignored_extensions_regex = re.compile(regex_builder)\n            log.debug(\n                'Filtering result for ignored extensions: %s in files %s',\n                ignored_extensions_regex.pattern, map(lambda x: x['path'], result)\n            )\n            result = [x for x in result if not ignored_extensions_regex.match(x['path'])]\n            log.debug('HdfsSensor.poke: after ext filter result is %s', result)\n        return result", "gt": "def filter_for_ignored_ext(result, ignored_ext, ignore_copying):"}
{"prefix": "def execute(self, context):\n        \"\"\"\n        Executed by task_instance at runtime\n        \"\"\"\n        s3_conn = S3Hook(self.s3_conn_id)\n\n        # Grab collection and execute query according to whether or not it is a pipeline\n        if self.is_pipeline:\n            results = MongoHook(self.mongo_conn_id).aggregate(\n                mongo_collection=self.mongo_collection,\n                aggregate_query=self.mongo_query,", "suffix": "            )\n\n        else:\n            results = MongoHook(self.mongo_conn_id).find(\n                mongo_collection=self.mongo_collection,\n                query=self.mongo_query,\n                mongo_db=self.mongo_db\n            )\n\n        # Performs transform then stringifies the docs results into json format\n        docs_str = self._stringify(self.transform(results))\n\n        # Load Into S3\n        s3_conn.load_string(\n            string_data=docs_str,\n            key=self.s3_key,\n            bucket_name=self.s3_bucket,\n            replace=self.replace\n        )\n\n        return True", "gt": "                mongo_db=self.mongo_db"}
{"prefix": "def _stringify(iterable, joinable='\\n'):\n        \"\"\"", "suffix": "        returns a stringified version using python join\n        \"\"\"\n        return joinable.join(\n            [json.dumps(doc, default=json_util.default) for doc in iterable]\n        )", "gt": "        Takes an iterable (pymongo Cursor or Array) containing dictionaries and"}
{"prefix": "def get_pool(name, session=None):\n    \"\"\"Get pool by a given name.\"\"\"\n    if not (name and name.strip()):\n        raise AirflowBadRequest(\"Pool name shouldn't be empty\")\n\n    pool = session.query(Pool).filter_by(pool=name).first()\n    if pool is None:", "suffix": "\n    return pool", "gt": "        raise PoolNotFound(\"Pool '%s' doesn't exist\" % name)"}
{"prefix": "def create_pool(name, slots, description, session=None):\n    \"\"\"Create a pool with a given parameters.\"\"\"\n    if not (name and name.strip()):\n        raise AirflowBadRequest(\"Pool name shouldn't be empty\")\n\n    try:\n        slots = int(slots)\n    except ValueError:\n        raise AirflowBadRequest(\"Bad value for `slots`: %s\" % slots)\n\n    session.expire_on_commit = False\n    pool = session.query(Pool).filter_by(pool=name).first()\n    if pool is None:\n        pool = Pool(pool=name, slots=slots, description=description)", "suffix": "    else:\n        pool.slots = slots\n        pool.description = description\n\n    session.commit()\n\n    return pool", "gt": "        session.add(pool)"}
{"prefix": "def delete_pool(name, session=None):\n    \"\"\"Delete pool by a given name.\"\"\"\n    if not (name and name.strip()):\n        raise AirflowBadRequest(\"Pool name shouldn't be empty\")\n\n    pool = session.query(Pool).filter_by(pool=name).first()", "suffix": "        raise PoolNotFound(\"Pool '%s' doesn't exist\" % name)\n\n    session.delete(pool)\n    session.commit()\n\n    return pool", "gt": "    if pool is None:"}
{"prefix": "def _dict_to_proto(py_dict, proto):\n        \"\"\"\n        Converts a python dictionary to the proto supplied\n\n        :param py_dict: The dictionary to convert\n        :type py_dict: dict\n        :param proto: The proto object to merge with dictionary\n        :type proto: protobuf\n        :return: A parsed python dictionary in provided proto format\n        :raises:\n            ParseError: On JSON parsing problems.\n        \"\"\"\n        dict_json_str = json.dumps(py_dict)", "suffix": "", "gt": "        return json_format.Parse(dict_json_str, proto)"}
{"prefix": "def wait_for_operation(self, operation, project_id=None):\n        \"\"\"\n        Given an operation, continuously fetches the status from Google Cloud until either\n        completion or an error occurring\n\n        :param operation: The Operation to wait for\n        :type operation: google.cloud.container_V1.gapic.enums.Operation\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :return: A new, updated operation fetched from Google Cloud\n        \"\"\"\n        self.log.info(\"Waiting for OPERATION_NAME %s\", operation.name)\n        time.sleep(OPERATIONAL_POLL_INTERVAL)\n        while operation.status != Operation.Status.DONE:\n            if operation.status == Operation.Status.RUNNING or operation.status == \\\n                    Operation.Status.PENDING:\n                time.sleep(OPERATIONAL_POLL_INTERVAL)\n            else:", "suffix": "                    \"Operation has failed with status: %s\" % operation.status)\n            # To update status of operation\n            operation = self.get_operation(operation.name, project_id=project_id or self.project_id)\n        return operation", "gt": "                raise exceptions.GoogleCloudError("}
{"prefix": "def get_operation(self, operation_name, project_id=None):\n        \"\"\"\n        Fetches the operation from Google Cloud\n\n        :param operation_name: Name of operation to fetch\n        :type operation_name: str\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :return: The new, updated operation from Google Cloud", "suffix": "        return self.get_client().get_operation(project_id=project_id or self.project_id,\n                                               zone=self.location,\n                                               operation_id=operation_name)", "gt": "        \"\"\""}
{"prefix": "def _append_label(cluster_proto, key, val):\n        \"\"\"\n        Append labels to provided Cluster Protobuf\n\n        Labels must fit the regex ``[a-z]([-a-z0-9]*[a-z0-9])?`` (current\n         airflow version string follows semantic versioning spec: x.y.z).\n\n        :param cluster_proto: The proto to append resource_label airflow\n            version to\n        :type cluster_proto: google.cloud.container_v1.types.Cluster\n        :param key: The key label\n        :type key: str\n        :param val:\n        :type val: str\n        :return: The cluster proto updated with new label\n        \"\"\"\n        val = val.replace('.', '-').replace('+', '-')\n        cluster_proto.resource_labels.update({key: val})", "suffix": "", "gt": "        return cluster_proto"}
{"prefix": "def delete_cluster(self, name, project_id=None, retry=DEFAULT, timeout=DEFAULT):\n        \"\"\"\n        Deletes the cluster, including the Kubernetes endpoint and all\n        worker nodes. Firewalls and routes that were configured during\n        cluster creation are also deleted. Other Google Compute Engine\n        resources that might be in use by the cluster (e.g. load balancer\n        resources) will not be deleted if they weren\u2019t present at the\n        initial create time.\n\n        :param name: The name of the cluster to delete\n        :type name: str\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :param retry: Retry object used to determine when/if to retry requests.\n            If None is specified, requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to\n            complete. Note that if retry is specified, the timeout applies to each\n            individual attempt.", "suffix": "        :return: The full url to the delete operation if successful, else None\n        \"\"\"\n\n        self.log.info(\n            \"Deleting (project_id=%s, zone=%s, cluster_id=%s)\", self.project_id, self.location, name\n        )\n\n        try:\n            op = self.get_client().delete_cluster(project_id=project_id or self.project_id,\n                                                  zone=self.location,\n                                                  cluster_id=name,\n                                                  retry=retry,\n                                                  timeout=timeout)\n            op = self.wait_for_operation(op)\n            # Returns server-defined url for the resource\n            return op.self_link\n        except NotFound as error:\n            self.log.info('Assuming Success: %s', error.message)", "gt": "        :type timeout: float"}
{"prefix": "def create_cluster(self, cluster, project_id=None, retry=DEFAULT, timeout=DEFAULT):\n        \"\"\"\n        Creates a cluster, consisting of the specified number and type of Google Compute\n        Engine instances.\n\n        :param cluster: A Cluster protobuf or dict. If dict is provided, it must\n            be of the same form as the protobuf message", "suffix": "        :type cluster: dict or google.cloud.container_v1.types.Cluster\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :param retry: A retry object (``google.api_core.retry.Retry``) used to\n            retry requests.\n            If None is specified, requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to\n            complete. Note that if retry is specified, the timeout applies to each\n            individual attempt.\n        :type timeout: float\n        :return: The full url to the new, or existing, cluster\n        :raises:\n            ParseError: On JSON parsing problems when trying to convert dict\n            AirflowException: cluster is not dict type nor Cluster proto type\n        \"\"\"\n\n        if isinstance(cluster, dict):\n            cluster_proto = Cluster()\n            cluster = self._dict_to_proto(py_dict=cluster, proto=cluster_proto)\n        elif not isinstance(cluster, Cluster):\n            raise AirflowException(\n                \"cluster is not instance of Cluster proto or python dict\")\n\n        self._append_label(cluster, 'airflow-version', 'v' + version.version)\n\n        self.log.info(\n            \"Creating (project_id=%s, zone=%s, cluster_name=%s)\",\n            self.project_id, self.location, cluster.name\n        )\n        try:\n            op = self.get_client().create_cluster(project_id=project_id or self.project_id,\n                                                  zone=self.location,\n                                                  cluster=cluster,\n                                                  retry=retry,\n                                                  timeout=timeout)\n            op = self.wait_for_operation(op)\n\n            return op.target_link\n        except AlreadyExists as error:\n            self.log.info('Assuming Success: %s', error.message)\n            return self.get_cluster(name=cluster.name).self_link", "gt": "            :class:`google.cloud.container_v1.types.Cluster`"}
{"prefix": "def get_cluster(self, name, project_id=None, retry=DEFAULT, timeout=DEFAULT):\n        \"\"\"", "suffix": "\n        :param name: The name of the cluster to retrieve\n        :type name: str\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :param retry: A retry object used to retry requests. If None is specified,\n            requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to\n            complete. Note that if retry is specified, the timeout applies to each\n            individual attempt.\n        :type timeout: float\n        :return: google.cloud.container_v1.types.Cluster\n        \"\"\"\n        self.log.info(\n            \"Fetching cluster (project_id=%s, zone=%s, cluster_name=%s)\",\n            project_id or self.project_id, self.location, name\n        )\n\n        return self.get_client().get_cluster(project_id=project_id or self.project_id,\n                                             zone=self.location,\n                                             cluster_id=name,\n                                             retry=retry,\n                                             timeout=timeout).self_link", "gt": "        Gets details of specified cluster"}
{"prefix": "def _get_webhook_endpoint(self, http_conn_id, webhook_endpoint):\n        \"\"\"\n        Given a Discord http_conn_id, return the default webhook endpoint or override if a\n        webhook_endpoint is manually supplied.\n\n        :param http_conn_id: The provided connection ID\n        :param webhook_endpoint: The manually provided webhook endpoint\n        :return: Webhook endpoint (str) to use\n        \"\"\"\n        if webhook_endpoint:\n            endpoint = webhook_endpoint\n        elif http_conn_id:\n            conn = self.get_connection(http_conn_id)\n            extra = conn.extra_dejson", "suffix": "        else:\n            raise AirflowException('Cannot get webhook endpoint: No valid Discord '\n                                   'webhook endpoint or http_conn_id supplied.')\n\n        # make sure endpoint matches the expected Discord webhook format\n        if not re.match('^webhooks/[0-9]+/[a-zA-Z0-9_-]+$', endpoint):\n            raise AirflowException('Expected Discord webhook endpoint in the form '\n                                   'of \"webhooks/{webhook.id}/{webhook.token}\".')\n\n        return endpoint", "gt": "            endpoint = extra.get('webhook_endpoint', '')"}
{"prefix": "def _build_discord_payload(self):\n        \"\"\"\n        Construct the Discord JSON payload. All relevant parameters are combined here\n        to a valid Discord JSON payload.\n\n        :return: Discord payload (str) to send\n        \"\"\"", "suffix": "\n        if self.username:\n            payload['username'] = self.username\n        if self.avatar_url:\n            payload['avatar_url'] = self.avatar_url\n\n        payload['tts'] = self.tts\n\n        if len(self.message) <= 2000:\n            payload['content'] = self.message\n        else:\n            raise AirflowException('Discord message length must be 2000 or fewer '\n                                   'characters.')\n\n        return json.dumps(payload)", "gt": "        payload = {}"}
{"prefix": "def execute(self):\n        \"\"\"\n        Execute the Discord webhook call\n        \"\"\"\n        proxies = {}", "suffix": "            # we only need https proxy for Discord\n            proxies = {'https': self.proxy}\n\n        discord_payload = self._build_discord_payload()\n\n        self.run(endpoint=self.webhook_endpoint,\n                 data=discord_payload,\n                 headers={'Content-type': 'application/json'},\n                 extra_options={'proxies': proxies})", "gt": "        if self.proxy:"}
{"prefix": "def encrypt(self, key_name, plaintext, authenticated_data=None):\n        \"\"\"\n        Encrypts a plaintext message using Google Cloud KMS.\n\n        :param key_name: The Resource Name for the key (or key version)\n                         to be used for encyption. Of the form\n                         ``projects/*/locations/*/keyRings/*/cryptoKeys/**``\n        :type key_name: str\n        :param plaintext: The message to be encrypted.\n        :type plaintext: bytes\n        :param authenticated_data: Optional additional authenticated data that", "suffix": "        :type authenticated_data: bytes\n        :return: The base 64 encoded ciphertext of the original message.\n        :rtype: str\n        \"\"\"\n        keys = self.get_conn().projects().locations().keyRings().cryptoKeys()\n        body = {'plaintext': _b64encode(plaintext)}\n        if authenticated_data:\n            body['additionalAuthenticatedData'] = _b64encode(authenticated_data)\n\n        request = keys.encrypt(name=key_name, body=body)\n        response = request.execute(num_retries=self.num_retries)\n\n        ciphertext = response['ciphertext']\n        return ciphertext", "gt": "                                   must also be provided to decrypt the message."}
{"prefix": "", "suffix": "        \"\"\"\n        Remote Popen\n\n        :param cmd: command to remotely execute\n        :param kwargs: extra arguments to Popen (see subprocess.Popen)\n        :return: handle to subprocess\n        \"\"\"\n        masked_cmd = ' '.join(self.cmd_mask_password(cmd))\n        self.log.info(\"Executing command: {}\".format(masked_cmd))\n        self.sp = subprocess.Popen(\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            **kwargs)\n\n        for line in iter(self.sp.stdout):\n            self.log.info(line.strip())\n\n        self.sp.wait()\n\n        self.log.info(\"Command exited with return code %s\", self.sp.returncode)\n\n        if self.sp.returncode:\n            raise AirflowException(\"Sqoop command failed: {}\".format(masked_cmd))", "gt": "def Popen(self, cmd, **kwargs):"}
{"prefix": "def import_table(self, table, target_dir=None, append=False, file_type=\"text\",\n                     columns=None, split_by=None, where=None, direct=False,\n                     driver=None, extra_import_options=None):\n        \"\"\"\n        Imports table from remote location to target dir. Arguments are\n        copies of direct sqoop command line arguments\n\n        :param table: Table to read\n        :param target_dir: HDFS destination dir\n        :param append: Append data to an existing dataset in HDFS\n        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\".\n            Imports data to into the specified format. Defaults to text.\n        :param columns: <col,col,col\u2026> Columns to import from table\n        :param split_by: Column of the table used to split work units", "suffix": "        :param direct: Use direct connector if exists for the database\n        :param driver: Manually specify JDBC driver class to use\n        :param extra_import_options: Extra import options to pass as dict.\n            If a key doesn't have a value, just pass an empty string to it.\n            Don't include prefix of -- for sqoop options.\n        \"\"\"\n        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct,\n                               driver, extra_import_options)\n\n        cmd += [\"--table\", table]\n\n        if columns:\n            cmd += [\"--columns\", columns]\n        if where:\n            cmd += [\"--where\", where]\n\n        self.Popen(cmd)", "gt": "        :param where: WHERE clause to use during import"}
{"prefix": "def import_query(self, query, target_dir, append=False, file_type=\"text\",\n                     split_by=None, direct=None, driver=None, extra_import_options=None):\n        \"\"\"\n        Imports a specific query from the rdbms to hdfs\n\n        :param query: Free format query to run\n        :param target_dir: HDFS destination dir\n        :param append: Append data to an existing dataset in HDFS\n        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\"\n            Imports data to hdfs into the specified format. Defaults to text.\n        :param split_by: Column of the table used to split work units\n        :param direct: Use direct import fast path\n        :param driver: Manually specify JDBC driver class to use", "suffix": "            If a key doesn't have a value, just pass an empty string to it.\n            Don't include prefix of -- for sqoop options.\n        \"\"\"\n        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct,\n                               driver, extra_import_options)\n        cmd += [\"--query\", query]\n\n        self.Popen(cmd)", "gt": "        :param extra_import_options: Extra import options to pass as dict."}
{"prefix": "def export_table(self, table, export_dir, input_null_string,\n                     input_null_non_string, staging_table,\n                     clear_staging_table, enclosed_by,\n                     escaped_by, input_fields_terminated_by,\n                     input_lines_terminated_by,\n                     input_optionally_enclosed_by, batch,\n                     relaxed_isolation, extra_export_options=None):", "suffix": "        Exports Hive table to remote location. Arguments are copies of direct\n        sqoop command line Arguments\n\n        :param table: Table remote destination\n        :param export_dir: Hive table to export\n        :param input_null_string: The string to be interpreted as null for\n            string columns\n        :param input_null_non_string: The string to be interpreted as null\n            for non-string columns\n        :param staging_table: The table in which data will be staged before\n            being inserted into the destination table\n        :param clear_staging_table: Indicate that any data present in the\n            staging table can be deleted\n        :param enclosed_by: Sets a required field enclosing character\n        :param escaped_by: Sets the escape character\n        :param input_fields_terminated_by: Sets the field separator character\n        :param input_lines_terminated_by: Sets the end-of-line character\n        :param input_optionally_enclosed_by: Sets a field enclosing character\n        :param batch: Use batch mode for underlying statement execution\n        :param relaxed_isolation: Transaction isolation to read uncommitted\n            for the mappers\n        :param extra_export_options: Extra export options to pass as dict.\n            If a key doesn't have a value, just pass an empty string to it.\n            Don't include prefix of -- for sqoop options.\n        \"\"\"\n        cmd = self._export_cmd(table, export_dir, input_null_string,\n                               input_null_non_string, staging_table,\n                               clear_staging_table, enclosed_by, escaped_by,\n                               input_fields_terminated_by,\n                               input_lines_terminated_by,\n                               input_optionally_enclosed_by, batch,\n                               relaxed_isolation, extra_export_options)\n\n        self.Popen(cmd)", "gt": "        \"\"\""}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Text to Speech.\n", "suffix": "        :rtype: google.cloud.texttospeech_v1.TextToSpeechClient\n        \"\"\"\n        if not self._client:\n            self._client = TextToSpeechClient(credentials=self._get_credentials())\n        return self._client", "gt": "        :return: Google Cloud Text to Speech client object."}
{"prefix": "def synthesize_speech(self, input_data, voice, audio_config, retry=None, timeout=None):\n        \"\"\"\n        Synthesizes text input\n\n        :param input_data: text input to be synthesized. See more:\n            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.SynthesisInput\n        :type input_data: dict or google.cloud.texttospeech_v1.types.SynthesisInput\n        :param voice: configuration of voice to be used in synthesis. See more:", "suffix": "        :type voice: dict or google.cloud.texttospeech_v1.types.VoiceSelectionParams\n        :param audio_config: configuration of the synthesized audio. See more:\n            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.AudioConfig\n        :type audio_config: dict or google.cloud.texttospeech_v1.types.AudioConfig\n        :return: SynthesizeSpeechResponse See more:\n            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.SynthesizeSpeechResponse\n        :rtype: object\n        :param retry: (Optional) A retry object used to retry requests. If None is specified,\n                requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.\n            Note that if retry is specified, the timeout applies to each individual attempt.\n        :type timeout: float\n        \"\"\"\n        client = self.get_conn()\n        self.log.info(\"Synthesizing input: %s\" % input_data)\n        return client.synthesize_speech(\n            input_=input_data, voice=voice, audio_config=audio_config, retry=retry, timeout=timeout\n        )", "gt": "            https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.VoiceSelectionParams"}
{"prefix": "def close(self):\n        \"\"\"\n        Close and upload local log file to remote storage S3.\n        \"\"\"\n        # When application exit, system shuts down all handlers by\n        # calling close method. Here we check if logger is already\n        # closed to prevent uploading the log to remote storage multiple\n        # times when `logging.shutdown` is called.\n        if self.closed:\n            return\n\n        super().close()\n\n        if not self.upload_on_close:\n            return\n\n        local_loc = os.path.join(self.local_base, self.log_relative_path)\n        remote_loc = os.path.join(self.remote_base, self.log_relative_path)\n        if os.path.exists(local_loc):\n            # read log and remove old logs to get just the latest additions\n            with open(local_loc, 'r') as logfile:\n                log = logfile.read()\n            self.s3_write(log, remote_loc)\n", "suffix": "        self.closed = True", "gt": "        # Mark closed so we don't double write if close is called twice"}
{"prefix": "def s3_read(self, remote_log_location, return_error=False):\n        \"\"\"\n        Returns the log found at the remote_log_location. Returns '' if no\n        logs are found or there is an error.\n        :param remote_log_location: the log's location in remote storage\n        :type remote_log_location: str (path)\n        :param return_error: if True, returns a string error message if an\n            error occurs. Otherwise returns '' when an error occurs.", "suffix": "        \"\"\"\n        try:\n            return self.hook.read_key(remote_log_location)\n        except Exception:\n            msg = 'Could not read logs from {}'.format(remote_log_location)\n            self.log.exception(msg)\n            # return error if needed\n            if return_error:\n                return msg", "gt": "        :type return_error: bool"}
{"prefix": "def s3_write(self, log, remote_log_location, append=True):", "suffix": "        Writes the log to the remote_log_location. Fails silently if no hook\n        was created.\n        :param log: the log to write to the remote_log_location\n        :type log: str\n        :param remote_log_location: the log's location in remote storage\n        :type remote_log_location: str (path)\n        :param append: if False, any existing log file is overwritten. If True,\n            the new log is appended to any existing logs.\n        :type append: bool\n        \"\"\"\n        if append and self.s3_log_exists(remote_log_location):\n            old_log = self.s3_read(remote_log_location)\n            log = '\\n'.join([old_log, log]) if old_log else log\n\n        try:\n            self.hook.load_string(\n                log,\n                key=remote_log_location,\n                replace=True,\n                encrypt=configuration.conf.getboolean('core', 'ENCRYPT_S3_LOGS'),\n            )\n        except Exception:\n            self.log.exception('Could not write logs to %s', remote_log_location)", "gt": "        \"\"\""}
{"prefix": "def _get_init_containers(self):\n        \"\"\"When using git to retrieve the DAGs, use the GitSync Init Container\"\"\"\n        # If we're using volume claims to mount the dags, no init container is needed\n        if self.kube_config.dags_volume_claim or \\\n           self.kube_config.dags_volume_host or self.kube_config.dags_in_image:\n            return []\n\n        # Otherwise, define a git-sync init container\n        init_environment = [{\n            'name': 'GIT_SYNC_REPO',\n            'value': self.kube_config.git_repo\n        }, {\n            'name': 'GIT_SYNC_BRANCH',\n            'value': self.kube_config.git_branch\n        }, {\n            'name': 'GIT_SYNC_ROOT',\n            'value': self.kube_config.git_sync_root\n        }, {\n            'name': 'GIT_SYNC_DEST',\n            'value': self.kube_config.git_sync_dest\n        }, {\n            'name': 'GIT_SYNC_DEPTH',\n            'value': '1'\n        }, {\n            'name': 'GIT_SYNC_ONE_TIME',\n            'value': 'true'\n        }]\n        if self.kube_config.git_user:\n            init_environment.append({\n                'name': 'GIT_SYNC_USERNAME',\n                'value': self.kube_config.git_user\n            })\n        if self.kube_config.git_password:\n            init_environment.append({\n                'name': 'GIT_SYNC_PASSWORD',\n                'value': self.kube_config.git_password\n            })\n\n        volume_mounts = [{\n            'mountPath': self.kube_config.git_sync_root,\n            'name': self.dags_volume_name,\n            'readOnly': False\n        }]\n        if self.kube_config.git_ssh_key_secret_name:\n            volume_mounts.append({\n                'name': self.git_sync_ssh_secret_volume_name,\n                'mountPath': '/etc/git-secret/ssh',\n                'subPath': 'ssh'\n            })\n            init_environment.extend([\n                {\n                    'name': 'GIT_SSH_KEY_FILE',", "suffix": "                },\n                {\n                    'name': 'GIT_SYNC_SSH',\n                    'value': 'true'\n                }])\n        if self.kube_config.git_ssh_known_hosts_configmap_name:\n            volume_mounts.append({\n                'name': self.git_sync_ssh_known_hosts_volume_name,\n                'mountPath': '/etc/git-secret/known_hosts',\n                'subPath': 'known_hosts'\n            })\n            init_environment.extend([\n                {\n                    'name': 'GIT_KNOWN_HOSTS',\n                    'value': 'true'\n                },\n                {\n                    'name': 'GIT_SSH_KNOWN_HOSTS_FILE',\n                    'value': '/etc/git-secret/known_hosts'\n                }\n            ])\n        else:\n            init_environment.append({\n                'name': 'GIT_KNOWN_HOSTS',\n                'value': 'false'\n            })\n\n        return [{\n            'name': self.kube_config.git_sync_init_container_name,\n            'image': self.kube_config.git_sync_container,\n            'securityContext': {'runAsUser': 65533},  # git-sync user\n            'env': init_environment,\n            'volumeMounts': volume_mounts\n        }]", "gt": "                    'value': '/etc/git-secret/ssh'"}
{"prefix": "def _get_environment(self):\n        \"\"\"Defines any necessary environment variables for the pod executor\"\"\"\n        env = {}\n\n        for env_var_name, env_var_val in six.iteritems(self.kube_config.kube_env_vars):", "suffix": "\n        env[\"AIRFLOW__CORE__EXECUTOR\"] = \"LocalExecutor\"\n\n        if self.kube_config.airflow_configmap:\n            env['AIRFLOW_HOME'] = self.worker_airflow_home\n            env['AIRFLOW__CORE__DAGS_FOLDER'] = self.worker_airflow_dags\n        if (not self.kube_config.airflow_configmap and\n                'AIRFLOW__CORE__SQL_ALCHEMY_CONN' not in self.kube_config.kube_secrets):\n            env['AIRFLOW__CORE__SQL_ALCHEMY_CONN'] = conf.get(\"core\", \"SQL_ALCHEMY_CONN\")\n        if self.kube_config.git_dags_folder_mount_point:\n            # /root/airflow/dags/repo/dags\n            dag_volume_mount_path = os.path.join(\n                self.kube_config.git_dags_folder_mount_point,\n                self.kube_config.git_sync_dest,  # repo\n                self.kube_config.git_subpath     # dags\n            )\n            env['AIRFLOW__CORE__DAGS_FOLDER'] = dag_volume_mount_path\n        return env", "gt": "            env[env_var_name] = env_var_val"}
{"prefix": "def _get_secrets(self):\n        \"\"\"Defines any necessary secrets for the pod executor\"\"\"\n        worker_secrets = []\n", "suffix": "            k8s_secret_obj, k8s_secret_key = obj_key_pair.split('=')\n            worker_secrets.append(\n                Secret('env', env_var_name, k8s_secret_obj, k8s_secret_key)\n            )\n\n        if self.kube_config.env_from_secret_ref:\n            for secret_ref in self.kube_config.env_from_secret_ref.split(','):\n                worker_secrets.append(\n                    Secret('env', None, secret_ref)\n                )\n\n        return worker_secrets", "gt": "        for env_var_name, obj_key_pair in six.iteritems(self.kube_config.kube_secrets):"}
{"prefix": "def _get_security_context(self):\n        \"\"\"Defines the security context\"\"\"\n        security_context = {}\n\n        if self.kube_config.worker_run_as_user:\n            security_context['runAsUser'] = self.kube_config.worker_run_as_user\n\n        if self.kube_config.worker_fs_group:\n            security_context['fsGroup'] = self.kube_config.worker_fs_group\n\n        # set fs_group to 65533 if not explicitly specified and using git ssh keypair auth\n        if self.kube_config.git_ssh_key_secret_name and security_context.get('fsGroup') is None:\n            security_context['fsGroup'] = 65533", "suffix": "        return security_context", "gt": ""}
{"prefix": "def kill(self, ti):\n        \"\"\"\n        Kill (cancel) a Qubole command\n        :param ti: Task Instance of the dag, used to determine the Quboles command id\n        :return: response from Qubole\n        \"\"\"\n        if self.cmd is None:\n            if not ti and not self.task_instance:\n                raise Exception(\"Unable to cancel Qubole Command, context is unavailable!\")\n            elif not ti:", "suffix": "            cmd_id = ti.xcom_pull(key=\"qbol_cmd_id\", task_ids=ti.task_id)\n            self.cmd = self.cls.find(cmd_id)\n        if self.cls and self.cmd:\n            self.log.info('Sending KILL signal to Qubole Command Id: %s', self.cmd.id)\n            self.cmd.cancel()", "gt": "                ti = self.task_instance"}
{"prefix": "def get_results(self, ti=None, fp=None, inline=True, delim=None, fetch=True):\n        \"\"\"\n        Get results (or just s3 locations) of a command from Qubole and save into a file\n        :param ti: Task Instance of the dag, used to determine the Quboles command id\n        :param fp: Optional file pointer, will create one and return if None passed\n        :param inline: True to download actual results, False to get s3 locations only\n        :param delim: Replaces the CTL-A chars with the given delim, defaults to ','\n        :param fetch: when inline is True, get results directly from s3 (if large)\n        :return: file location containing actual results or s3 locations of results\n        \"\"\"\n        if fp is None:\n            iso = datetime.datetime.utcnow().isoformat()\n            logpath = os.path.expanduser(\n                configuration.conf.get('core', 'BASE_LOG_FOLDER')\n            )\n            resultpath = logpath + '/' + self.dag_id + '/' + self.task_id + '/results'\n            configuration.mkdir_p(resultpath)\n            fp = open(resultpath + '/' + iso, 'wb')\n\n        if self.cmd is None:\n            cmd_id = ti.xcom_pull(key=\"qbol_cmd_id\", task_ids=self.task_id)\n            self.cmd = self.cls.find(cmd_id)", "suffix": "        self.cmd.get_results(fp, inline, delim, fetch)\n        fp.flush()\n        fp.close()\n        return fp.name", "gt": ""}
{"prefix": "def get_log(self, ti):\n        \"\"\"\n        Get Logs of a command from Qubole\n        :param ti: Task Instance of the dag, used to determine the Quboles command id\n        :return: command log as text\n        \"\"\"\n        if self.cmd is None:\n            cmd_id = ti.xcom_pull(key=\"qbol_cmd_id\", task_ids=self.task_id)", "suffix": "", "gt": "        Command.get_log_id(self.cls, cmd_id)"}
{"prefix": "def get_jobs_id(self, ti):\n        \"\"\"", "suffix": "        :param ti: Task Instance of the dag, used to determine the Quboles command id\n        :return: Job information associated with command\n        \"\"\"\n        if self.cmd is None:\n            cmd_id = ti.xcom_pull(key=\"qbol_cmd_id\", task_ids=self.task_id)\n        Command.get_jobs_id(self.cls, cmd_id)", "gt": "        Get jobs associated with a Qubole commands"}
{"prefix": "def get_extra_links(self, operator, dttm):", "suffix": "        Get link to qubole command result page.\n\n        :param operator: operator\n        :param dttm: datetime\n        :return: url link\n        \"\"\"\n        conn = BaseHook.get_connection(operator.kwargs['qubole_conn_id'])\n        if conn and conn.host:\n            host = re.sub(r'api$', 'v2/analyze?command_id=', conn.host)\n        else:\n            host = 'https://api.qubole.com/v2/analyze?command_id='\n\n        ti = TaskInstance(task=operator, execution_date=dttm)\n        qds_command_id = ti.xcom_pull(task_ids=operator.task_id, key='qbol_cmd_id')\n        url = host + str(qds_command_id) if qds_command_id else ''\n        return url", "gt": "        \"\"\""}
{"prefix": "def heartbeat(self):\n        \"\"\"\n        Heartbeats update the job's entry in the database with a timestamp\n        for the latest_heartbeat and allows for the job to be killed\n        externally. This allows at the system level to monitor what is\n        actually active.\n\n        For instance, an old heartbeat for SchedulerJob would mean something\n        is wrong.\n\n        This also allows for any job to be killed externally, regardless\n        of who is running it or on which machine it is running.\n\n        Note that if your heartbeat is set to 60 seconds and you call this\n        method after 10 seconds of processing since the last heartbeat, it\n        will sleep 50 seconds to complete the 60 seconds and keep a steady\n        heart rate. If you go over 60 seconds before calling it, it won't\n        sleep at all.\n        \"\"\"\n        try:\n            with create_session() as session:\n                job = session.query(BaseJob).filter_by(id=self.id).one()\n                make_transient(job)\n                session.commit()\n\n            if job.state == State.SHUTDOWN:\n                self.kill()\n\n            is_unit_test = conf.getboolean('core', 'unit_test_mode')", "suffix": "                # Figure out how long to sleep for\n                sleep_for = 0\n                if job.latest_heartbeat:\n                    seconds_remaining = self.heartrate - \\\n                        (timezone.utcnow() - job.latest_heartbeat)\\\n                        .total_seconds()\n                    sleep_for = max(0, seconds_remaining)\n\n                sleep(sleep_for)\n\n            # Update last heartbeat time\n            with create_session() as session:\n                job = session.query(BaseJob).filter(BaseJob.id == self.id).first()\n                job.latest_heartbeat = timezone.utcnow()\n                session.merge(job)\n                session.commit()\n\n                self.heartbeat_callback(session=session)\n                self.log.debug('[heartbeat]')\n        except OperationalError as e:\n            self.log.error(\"Scheduler heartbeat got an exception: %s\", str(e))", "gt": "            if not is_unit_test:"}
{"prefix": "def reset_state_for_orphaned_tasks(self, filter_by_dag_run=None, session=None):\n        \"\"\"\n        This function checks if there are any tasks in the dagrun (or all)\n        that have a scheduled state but are not known by the\n        executor. If it finds those it will reset the state to None\n        so they will get picked up again.\n        The batch option is for performance reasons as the queries are made in\n        sequence.\n\n        :param filter_by_dag_run: the dag_run we want to process, None if all\n        :type filter_by_dag_run: airflow.models.DagRun\n        :return: the TIs reset (in expired SQLAlchemy state)\n        :rtype: list[airflow.models.TaskInstance]\n        \"\"\"\n        queued_tis = self.executor.queued_tasks\n        # also consider running as the state might not have changed in the db yet\n        running_tis = self.executor.running\n\n        resettable_states = [State.SCHEDULED, State.QUEUED]\n        TI = models.TaskInstance\n        DR = models.DagRun\n        if filter_by_dag_run is None:\n            resettable_tis = (\n                session\n                .query(TI)\n                .join(\n                    DR,\n                    and_(\n                        TI.dag_id == DR.dag_id,\n                        TI.execution_date == DR.execution_date))\n                .filter(\n                    DR.state == State.RUNNING,\n                    DR.run_id.notlike(BackfillJob.ID_PREFIX + '%'),\n                    TI.state.in_(resettable_states))).all()\n        else:\n            resettable_tis = filter_by_dag_run.get_task_instances(state=resettable_states,\n                                                                  session=session)\n        tis_to_reset = []\n        # Can't use an update here since it doesn't support joins\n        for ti in resettable_tis:\n            if ti.key not in queued_tis and ti.key not in running_tis:\n                tis_to_reset.append(ti)\n\n        if len(tis_to_reset) == 0:\n            return []\n\n        def query(result, items):\n            filter_for_tis = ([and_(TI.dag_id == ti.dag_id,\n                                    TI.task_id == ti.task_id,\n                                    TI.execution_date == ti.execution_date)\n                               for ti in items])\n            reset_tis = (\n                session\n                .query(TI)\n                .filter(or_(*filter_for_tis), TI.state.in_(resettable_states))\n                .with_for_update()\n                .all())\n            for ti in reset_tis:\n                ti.state = State.NONE\n                session.merge(ti)\n            return result + reset_tis\n\n        reset_tis = helpers.reduce_in_chunks(query,", "suffix": "                                             [],\n                                             self.max_tis_per_query)\n\n        task_instance_str = '\\n\\t'.join(\n            [repr(x) for x in reset_tis])\n        session.commit()\n\n        self.log.info(\n            \"Reset the following %s TaskInstances:\\n\\t%s\",\n            len(reset_tis), task_instance_str\n        )\n        return reset_tis", "gt": "                                             tis_to_reset,"}
{"prefix": "def _launch_process(result_queue,\n                        file_path,\n                        pickle_dags,\n                        dag_id_white_list,\n                        thread_name,\n                        zombies):\n        \"\"\"\n        Launch a process to process the given file.\n\n        :param result_queue: the queue to use for passing back the result\n        :type result_queue: multiprocessing.Queue\n        :param file_path: the file to process\n        :type file_path: unicode\n        :param pickle_dags: whether to pickle the DAGs found in the file and\n            save them to the DB\n        :type pickle_dags: bool\n        :param dag_id_white_list: if specified, only examine DAG ID's that are\n            in this list\n        :type dag_id_white_list: list[unicode]\n        :param thread_name: the name to use for the process that is launched\n        :type thread_name: unicode\n        :return: the process that was launched\n        :rtype: multiprocessing.Process\n        :param zombies: zombie task instances to kill\n        :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]\n        \"\"\"\n        def helper():\n            # This helper runs in the newly created process\n            log = logging.getLogger(\"airflow.processor\")\n\n            stdout = StreamLogWriter(log, logging.INFO)\n            stderr = StreamLogWriter(log, logging.WARN)\n\n            set_context(log, file_path)\n\n            try:\n                # redirect stdout/stderr to log\n                sys.stdout = stdout\n                sys.stderr = stderr\n\n                # Re-configure the ORM engine as there are issues with multiple processes\n                settings.configure_orm()\n\n                # Change the thread name to differentiate log lines. This is\n                # really a separate process, but changing the name of the\n                # process doesn't work, so changing the thread name instead.\n                threading.current_thread().name = thread_name\n                start_time = time.time()\n\n                log.info(\"Started process (PID=%s) to work on %s\",\n                         os.getpid(), file_path)\n                scheduler_job = SchedulerJob(dag_ids=dag_id_white_list, log=log)\n                result = scheduler_job.process_file(file_path,\n                                                    zombies,\n                                                    pickle_dags)\n                result_queue.put(result)\n                end_time = time.time()\n                log.info(\n                    \"Processing %s took %.3f seconds\", file_path, end_time - start_time\n                )\n            except Exception:\n                # Log exceptions through the logging framework.", "suffix": "                raise\n            finally:\n                sys.stdout = sys.__stdout__\n                sys.stderr = sys.__stderr__\n                # We re-initialized the ORM within this Process above so we need to\n                # tear it down manually here\n                settings.dispose_orm()\n\n        p = multiprocessing.Process(target=helper,\n                                    args=(),\n                                    name=\"{}-Process\".format(thread_name))\n        p.start()\n        return p", "gt": "                log.exception(\"Got an exception! Propagating...\")"}
{"prefix": "def start(self):\n        \"\"\"", "suffix": "        \"\"\"\n        self._process = DagFileProcessor._launch_process(\n            self._result_queue,\n            self.file_path,\n            self._pickle_dags,\n            self._dag_id_white_list,\n            \"DagFileProcessor{}\".format(self._instance_id),\n            self._zombies)\n        self._start_time = timezone.utcnow()", "gt": "        Launch the process and start processing the DAG."}
{"prefix": "def terminate(self, sigkill=False):\n        \"\"\"\n        Terminate (and then kill) the process launched to process the file.\n\n        :param sigkill: whether to issue a SIGKILL if SIGTERM doesn't work.\n        :type sigkill: bool\n        \"\"\"\n        if self._process is None:\n            raise AirflowException(\"Tried to call stop before starting!\")\n        # The queue will likely get corrupted, so remove the reference\n        self._result_queue = None\n        self._process.terminate()\n        # Arbitrarily wait 5s for the process to die\n        self._process.join(5)\n        if sigkill and self._process.is_alive():", "suffix": "            os.kill(self._process.pid, signal.SIGKILL)", "gt": "            self.log.warning(\"Killing PID %s\", self._process.pid)"}
{"prefix": "def done(self):\n        \"\"\"\n        Check if the process launched to process this file is done.\n\n        :return: whether the process is finished running\n        :rtype: bool\n        \"\"\"\n        if self._process is None:\n            raise AirflowException(\"Tried to see if it's done before starting!\")\n\n        if self._done:\n            return True\n\n        # In case result queue is corrupted.\n        if self._result_queue and not self._result_queue.empty():\n            self._result = self._result_queue.get_nowait()\n            self._done = True\n            self.log.debug(\"Waiting for %s\", self._process)\n            self._process.join()\n            return True\n\n        # Potential error case when process dies\n        if self._result_queue and not self._process.is_alive():\n            self._done = True\n            # Get the object from the queue or else join() can hang.\n            if not self._result_queue.empty():\n                self._result = self._result_queue.get_nowait()\n            self.log.debug(\"Waiting for %s\", self._process)\n            self._process.join()", "suffix": "\n        return False", "gt": "            return True"}
{"prefix": "def _exit_gracefully(self, signum, frame):\n        \"\"\"\n        Helper method to clean up processor_agent to avoid leaving orphan processes.", "suffix": "        self.log.info(\"Exiting gracefully upon receiving signal %s\", signum)\n        if self.processor_agent:\n            self.processor_agent.end()\n        sys.exit(os.EX_OK)", "gt": "        \"\"\""}
{"prefix": "def manage_slas(self, dag, session=None):\n        \"\"\"\n        Finding all tasks that have SLAs defined, and sending alert emails\n        where needed. New SLA misses are also recorded in the database.\n\n        Where assuming that the scheduler runs often, so we only check for\n        tasks that should have succeeded in the past hour.\n        \"\"\"\n        if not any([isinstance(ti.sla, timedelta) for ti in dag.tasks]):\n            self.log.info(\"Skipping SLA check for %s because no tasks in DAG have SLAs\", dag)\n            return\n\n        TI = models.TaskInstance\n        sq = (\n            session\n            .query(\n                TI.task_id,\n                func.max(TI.execution_date).label('max_ti'))\n            .with_hint(TI, 'USE INDEX (PRIMARY)', dialect_name='mysql')\n            .filter(TI.dag_id == dag.dag_id)\n            .filter(or_(\n                TI.state == State.SUCCESS,\n                TI.state == State.SKIPPED))\n            .filter(TI.task_id.in_(dag.task_ids))\n            .group_by(TI.task_id).subquery('sq')\n        )\n\n        max_tis = session.query(TI).filter(\n            TI.dag_id == dag.dag_id,\n            TI.task_id == sq.c.task_id,\n            TI.execution_date == sq.c.max_ti,\n        ).all()\n\n        ts = timezone.utcnow()\n        for ti in max_tis:\n            task = dag.get_task(ti.task_id)\n            dttm = ti.execution_date\n            if isinstance(task.sla, timedelta):\n                dttm = dag.following_schedule(dttm)\n                while dttm < timezone.utcnow():\n                    following_schedule = dag.following_schedule(dttm)\n                    if following_schedule + task.sla < timezone.utcnow():\n                        session.merge(SlaMiss(\n                            task_id=ti.task_id,\n                            dag_id=ti.dag_id,\n                            execution_date=dttm,\n                            timestamp=ts))\n                    dttm = dag.following_schedule(dttm)\n        session.commit()\n\n        slas = (\n            session\n            .query(SlaMiss)\n            .filter(SlaMiss.notification_sent == False, SlaMiss.dag_id == dag.dag_id)  # noqa: E712\n            .all()\n        )\n", "suffix": "            sla_dates = [sla.execution_date for sla in slas]\n            qry = (\n                session\n                .query(TI)\n                .filter(\n                    TI.state != State.SUCCESS,\n                    TI.execution_date.in_(sla_dates),\n                    TI.dag_id == dag.dag_id\n                ).all()\n            )\n            blocking_tis = []\n            for ti in qry:\n                if ti.task_id in dag.task_ids:\n                    ti.task = dag.get_task(ti.task_id)\n                    blocking_tis.append(ti)\n                else:\n                    session.delete(ti)\n                    session.commit()\n\n            task_list = \"\\n\".join([\n                sla.task_id + ' on ' + sla.execution_date.isoformat()\n                for sla in slas])\n            blocking_task_list = \"\\n\".join([\n                ti.task_id + ' on ' + ti.execution_date.isoformat()\n                for ti in blocking_tis])\n            # Track whether email or any alert notification sent\n            # We consider email or the alert callback as notifications\n            email_sent = False\n            notification_sent = False\n            if dag.sla_miss_callback:\n                # Execute the alert callback\n                self.log.info(' --------------> ABOUT TO CALL SLA MISS CALL BACK ')\n                try:\n                    dag.sla_miss_callback(dag, task_list, blocking_task_list, slas,\n                                          blocking_tis)\n                    notification_sent = True\n                except Exception:\n                    self.log.exception(\"Could not call sla_miss_callback for DAG %s\",\n                                       dag.dag_id)\n            email_content = \"\"\"\\\n            Here's a list of tasks that missed their SLAs:\n            <pre><code>{task_list}\\n<code></pre>\n            Blocking tasks:\n            <pre><code>{blocking_task_list}\\n{bug}<code></pre>\n            \"\"\".format(task_list=task_list, blocking_task_list=blocking_task_list,\n                       bug=asciiart.bug)\n            emails = set()\n            for task in dag.tasks:\n                if task.email:\n                    if isinstance(task.email, basestring):\n                        emails |= set(get_email_address_list(task.email))\n                    elif isinstance(task.email, (list, tuple)):\n                        emails |= set(task.email)\n            if emails:\n                try:\n                    send_email(\n                        emails,\n                        \"[airflow] SLA miss on DAG=\" + dag.dag_id,\n                        email_content)\n                    email_sent = True\n                    notification_sent = True\n                except Exception:\n                    self.log.exception(\"Could not send SLA Miss email notification for\"\n                                       \" DAG %s\", dag.dag_id)\n            # If we sent any notification, update the sla_miss table\n            if notification_sent:\n                for sla in slas:\n                    if email_sent:\n                        sla.email_sent = True\n                    sla.notification_sent = True\n                    session.merge(sla)\n            session.commit()", "gt": "        if slas:"}
{"prefix": "def update_import_errors(session, dagbag):\n        \"\"\"\n        For the DAGs in the given DagBag, record any associated import errors and clears\n        errors for files that no longer have them. These are usually displayed through the\n        Airflow UI so that users know that there are issues parsing DAGs.\n\n        :param session: session for ORM operations\n        :type session: sqlalchemy.orm.session.Session\n        :param dagbag: DagBag containing DAGs with import errors\n        :type dagbag: airflow.models.DagBag\n        \"\"\"\n        # Clear the errors of the processed files\n        for dagbag_file in dagbag.file_last_changed:\n            session.query(errors.ImportError).filter(", "suffix": "            ).delete()\n\n        # Add the errors of the processed files\n        for filename, stacktrace in six.iteritems(dagbag.import_errors):\n            session.add(errors.ImportError(\n                filename=filename,\n                stacktrace=stacktrace))\n        session.commit()", "gt": "                errors.ImportError.filename == dagbag_file"}
{"prefix": "def create_dag_run(self, dag, session=None):\n        \"\"\"\n        This method checks whether a new DagRun needs to be created\n        for a DAG based on scheduling interval.\n        Returns DagRun if one is scheduled. Otherwise returns None.\n        \"\"\"\n        if dag.schedule_interval and conf.getboolean('scheduler', 'USE_JOB_SCHEDULE'):\n            active_runs = DagRun.find(\n                dag_id=dag.dag_id,\n                state=State.RUNNING,\n                external_trigger=False,\n                session=session\n            )\n            # return if already reached maximum active runs and no timeout setting\n            if len(active_runs) >= dag.max_active_runs and not dag.dagrun_timeout:\n                return\n            timedout_runs = 0\n            for dr in active_runs:\n                if (\n                        dr.start_date and dag.dagrun_timeout and\n                        dr.start_date < timezone.utcnow() - dag.dagrun_timeout):", "suffix": "                    dr.end_date = timezone.utcnow()\n                    dag.handle_callback(dr, success=False, reason='dagrun_timeout',\n                                        session=session)\n                    timedout_runs += 1\n            session.commit()\n            if len(active_runs) - timedout_runs >= dag.max_active_runs:\n                return\n\n            # this query should be replaced by find dagrun\n            qry = (\n                session.query(func.max(DagRun.execution_date))\n                .filter_by(dag_id=dag.dag_id)\n                .filter(or_(\n                    DagRun.external_trigger == False,  # noqa: E712\n                    # add % as a wildcard for the like query\n                    DagRun.run_id.like(DagRun.ID_PREFIX + '%')\n                ))\n            )\n            last_scheduled_run = qry.scalar()\n\n            # don't schedule @once again\n            if dag.schedule_interval == '@once' and last_scheduled_run:\n                return None\n\n            # don't do scheduler catchup for dag's that don't have dag.catchup = True\n            if not (dag.catchup or dag.schedule_interval == '@once'):\n                # The logic is that we move start_date up until\n                # one period before, so that timezone.utcnow() is AFTER\n                # the period end, and the job can be created...\n                now = timezone.utcnow()\n                next_start = dag.following_schedule(now)\n                last_start = dag.previous_schedule(now)\n                if next_start <= now:\n                    new_start = last_start\n                else:\n                    new_start = dag.previous_schedule(last_start)\n\n                if dag.start_date:\n                    if new_start >= dag.start_date:\n                        dag.start_date = new_start\n                else:\n                    dag.start_date = new_start\n\n            next_run_date = None\n            if not last_scheduled_run:\n                # First run\n                task_start_dates = [t.start_date for t in dag.tasks]\n                if task_start_dates:\n                    next_run_date = dag.normalize_schedule(min(task_start_dates))\n                    self.log.debug(\n                        \"Next run date based on tasks %s\",\n                        next_run_date\n                    )\n            else:\n                next_run_date = dag.following_schedule(last_scheduled_run)\n\n            # make sure backfills are also considered\n            last_run = dag.get_last_dagrun(session=session)\n            if last_run and next_run_date:\n                while next_run_date <= last_run.execution_date:\n                    next_run_date = dag.following_schedule(next_run_date)\n\n            # don't ever schedule prior to the dag's start_date\n            if dag.start_date:\n                next_run_date = (dag.start_date if not next_run_date\n                                 else max(next_run_date, dag.start_date))\n                if next_run_date == dag.start_date:\n                    next_run_date = dag.normalize_schedule(dag.start_date)\n\n                self.log.debug(\n                    \"Dag start date: %s. Next run date: %s\",\n                    dag.start_date, next_run_date\n                )\n\n            # don't ever schedule in the future or if next_run_date is None\n            if not next_run_date or next_run_date > timezone.utcnow():\n                return\n\n            # this structure is necessary to avoid a TypeError from concatenating\n            # NoneType\n            if dag.schedule_interval == '@once':\n                period_end = next_run_date\n            elif next_run_date:\n                period_end = dag.following_schedule(next_run_date)\n\n            # Don't schedule a dag beyond its end_date (as specified by the dag param)\n            if next_run_date and dag.end_date and next_run_date > dag.end_date:\n                return\n\n            # Don't schedule a dag beyond its end_date (as specified by the task params)\n            # Get the min task end date, which may come from the dag.default_args\n            min_task_end_date = []\n            task_end_dates = [t.end_date for t in dag.tasks if t.end_date]\n            if task_end_dates:\n                min_task_end_date = min(task_end_dates)\n            if next_run_date and min_task_end_date and next_run_date > min_task_end_date:\n                return\n\n            if next_run_date and period_end and period_end <= timezone.utcnow():\n                next_run = dag.create_dagrun(\n                    run_id=DagRun.ID_PREFIX + next_run_date.isoformat(),\n                    execution_date=next_run_date,\n                    start_date=timezone.utcnow(),\n                    state=State.RUNNING,\n                    external_trigger=False\n                )\n                return next_run", "gt": "                    dr.state = State.FAILED"}
{"prefix": "def _process_task_instances(self, dag, queue, session=None):\n        \"\"\"\n        This method schedules the tasks for a single DAG by looking at the\n        active DAG runs and adding task instances that should run to the\n        queue.\n        \"\"\"\n\n        # update the state of the previously active dag runs\n        dag_runs = DagRun.find(dag_id=dag.dag_id, state=State.RUNNING, session=session)\n        active_dag_runs = []\n        for run in dag_runs:\n            self.log.info(\"Examining DAG run %s\", run)\n            # don't consider runs that are executed in the future\n            if run.execution_date > timezone.utcnow():\n                self.log.error(\n                    \"Execution date is in future: %s\",\n                    run.execution_date\n                )\n                continue\n\n            if len(active_dag_runs) >= dag.max_active_runs:\n                self.log.info(\"Number of active dag runs reached max_active_run.\")\n                break\n\n            # skip backfill dagruns for now as long as they are not really scheduled\n            if run.is_backfill:\n                continue\n\n            # todo: run.dag is transient but needs to be set\n            run.dag = dag\n            # todo: preferably the integrity check happens at dag collection time\n            run.verify_integrity(session=session)\n            run.update_state(session=session)\n            if run.state == State.RUNNING:\n                make_transient(run)\n                active_dag_runs.append(run)\n\n        for run in active_dag_runs:", "suffix": "            # this needs a fresh session sometimes tis get detached\n            tis = run.get_task_instances(state=(State.NONE,\n                                                State.UP_FOR_RETRY,\n                                                State.UP_FOR_RESCHEDULE))\n\n            # this loop is quite slow as it uses are_dependencies_met for\n            # every task (in ti.is_runnable). This is also called in\n            # update_state above which has already checked these tasks\n            for ti in tis:\n                task = dag.get_task(ti.task_id)\n\n                # fixme: ti.task is transient but needs to be set\n                ti.task = task\n\n                if ti.are_dependencies_met(\n                        dep_context=DepContext(flag_upstream_failed=True),\n                        session=session):\n                    self.log.debug('Queuing task: %s', ti)\n                    queue.append(ti.key)", "gt": "            self.log.debug(\"Examining active DAG run: %s\", run)"}
{"prefix": "def _change_state_for_tis_without_dagrun(self,\n                                             simple_dag_bag,\n                                             old_states,\n                                             new_state,\n                                             session=None):\n        \"\"\"\n        For all DAG IDs in the SimpleDagBag, look for task instances in the\n        old_states and set them to new_state if the corresponding DagRun\n        does not exist or exists but is not in the running state. This\n        normally should not happen, but it can if the state of DagRuns are\n        changed manually.\n\n        :param old_states: examine TaskInstances in this state\n        :type old_state: list[airflow.utils.state.State]\n        :param new_state: set TaskInstances to this state\n        :type new_state: airflow.utils.state.State\n        :param simple_dag_bag: TaskInstances associated with DAGs in the\n            simple_dag_bag and with states in the old_state will be examined\n        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag\n        \"\"\"\n        tis_changed = 0\n        query = session \\\n            .query(models.TaskInstance) \\\n            .outerjoin(models.DagRun, and_(\n                models.TaskInstance.dag_id == models.DagRun.dag_id,\n                models.TaskInstance.execution_date == models.DagRun.execution_date)) \\\n            .filter(models.TaskInstance.dag_id.in_(simple_dag_bag.dag_ids)) \\\n            .filter(models.TaskInstance.state.in_(old_states)) \\\n            .filter(or_(\n                models.DagRun.state != State.RUNNING,\n                models.DagRun.state.is_(None)))", "suffix": "            tis_to_change = query \\\n                .with_for_update() \\\n                .all()\n            for ti in tis_to_change:\n                ti.set_state(new_state, session=session)\n                tis_changed += 1\n        else:\n            subq = query.subquery()\n            tis_changed = session \\\n                .query(models.TaskInstance) \\\n                .filter(and_(\n                    models.TaskInstance.dag_id == subq.c.dag_id,\n                    models.TaskInstance.task_id == subq.c.task_id,\n                    models.TaskInstance.execution_date ==\n                    subq.c.execution_date)) \\\n                .update({models.TaskInstance.state: new_state},\n                        synchronize_session=False)\n            session.commit()\n\n        if tis_changed > 0:\n            self.log.warning(\n                \"Set %s task instances to state=%s as their associated DagRun was not in RUNNING state\",\n                tis_changed, new_state\n            )", "gt": "        if self.using_sqlite:"}
{"prefix": "def __get_concurrency_maps(self, states, session=None):\n        \"\"\"\n        Get the concurrency maps.\n\n        :param states: List of states to query for\n        :type states: list[airflow.utils.state.State]\n        :return: A map from (dag_id, task_id) to # of task instances and\n         a map from (dag_id, task_id) to # of task instances in the given state list\n        :rtype: dict[tuple[str, str], int]\n\n        \"\"\"\n        TI = models.TaskInstance\n        ti_concurrency_query = (\n            session\n            .query(TI.task_id, TI.dag_id, func.count('*'))\n            .filter(TI.state.in_(states))\n            .group_by(TI.task_id, TI.dag_id)", "suffix": "        dag_map = defaultdict(int)\n        task_map = defaultdict(int)\n        for result in ti_concurrency_query:\n            task_id, dag_id, count = result\n            dag_map[dag_id] += count\n            task_map[(dag_id, task_id)] = count\n        return dag_map, task_map", "gt": "        ).all()"}
{"prefix": "", "suffix": "        \"\"\"\n        Finds TIs that are ready for execution with respect to pool limits,\n        dag concurrency, executor state, and priority.\n\n        :param simple_dag_bag: TaskInstances associated with DAGs in the\n            simple_dag_bag will be fetched from the DB and executed\n        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag\n        :param executor: the executor that runs task instances\n        :type executor: BaseExecutor\n        :param states: Execute TaskInstances in these states\n        :type states: tuple[airflow.utils.state.State]\n        :return: list[airflow.models.TaskInstance]\n        \"\"\"\n        executable_tis = []\n\n        # Get all task instances associated with scheduled\n        # DagRuns which are not backfilled, in the given states,\n        # and the dag is not paused\n        TI = models.TaskInstance\n        DR = models.DagRun\n        DM = models.DagModel\n        ti_query = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(simple_dag_bag.dag_ids))\n            .outerjoin(\n                DR,\n                and_(DR.dag_id == TI.dag_id, DR.execution_date == TI.execution_date)\n            )\n            .filter(or_(DR.run_id == None,  # noqa: E711\n                    not_(DR.run_id.like(BackfillJob.ID_PREFIX + '%'))))\n            .outerjoin(DM, DM.dag_id == TI.dag_id)\n            .filter(or_(DM.dag_id == None,  # noqa: E711\n                    not_(DM.is_paused)))\n        )\n\n        # Additional filters on task instance state\n        if None in states:\n            ti_query = ti_query.filter(\n                or_(TI.state == None, TI.state.in_(states))  # noqa: E711\n            )\n        else:\n            ti_query = ti_query.filter(TI.state.in_(states))\n\n        task_instances_to_examine = ti_query.all()\n\n        if len(task_instances_to_examine) == 0:\n            self.log.debug(\"No tasks to consider for execution.\")\n            return executable_tis\n\n        # Put one task instance on each line\n        task_instance_str = \"\\n\\t\".join(\n            [repr(x) for x in task_instances_to_examine])\n        self.log.info(\n            \"%s tasks up for execution:\\n\\t%s\", len(task_instances_to_examine),\n            task_instance_str\n        )\n\n        # Get the pool settings\n        pools = {p.pool: p for p in session.query(models.Pool).all()}\n\n        pool_to_task_instances = defaultdict(list)\n        for task_instance in task_instances_to_examine:\n            pool_to_task_instances[task_instance.pool].append(task_instance)\n\n        states_to_count_as_running = [State.RUNNING, State.QUEUED]\n        # dag_id to # of running tasks and (dag_id, task_id) to # of running tasks.\n        dag_concurrency_map, task_concurrency_map = self.__get_concurrency_maps(\n            states=states_to_count_as_running, session=session)\n\n        # Go through each pool, and queue up a task for execution if there are\n        # any open slots in the pool.\n        for pool, task_instances in pool_to_task_instances.items():\n            pool_name = pool\n            if not pool:\n                # Arbitrary:\n                # If queued outside of a pool, trigger no more than\n                # non_pooled_task_slot_count\n                open_slots = models.Pool.default_pool_open_slots()\n                pool_name = models.Pool.default_pool_name\n            else:\n                if pool not in pools:\n                    self.log.warning(\n                        \"Tasks using non-existent pool '%s' will not be scheduled\",\n                        pool\n                    )\n                    open_slots = 0\n                else:\n                    open_slots = pools[pool].open_slots(session=session)\n\n            num_ready = len(task_instances)\n            self.log.info(\n                \"Figuring out tasks to run in Pool(name=%s) with %s open slots \"\n                \"and %s task instances ready to be queued\",\n                pool, open_slots, num_ready\n            )\n\n            priority_sorted_task_instances = sorted(\n                task_instances, key=lambda ti: (-ti.priority_weight, ti.execution_date))\n\n            # Number of tasks that cannot be scheduled because of no open slot in pool\n            num_starving_tasks = 0\n            for current_index, task_instance in enumerate(priority_sorted_task_instances):\n                if open_slots <= 0:\n                    self.log.info(\n                        \"Not scheduling since there are %s open slots in pool %s\",\n                        open_slots, pool\n                    )\n                    # Can't schedule any more since there are no more open slots.\n                    num_starving_tasks = len(priority_sorted_task_instances) - current_index\n                    break\n\n                # Check to make sure that the task concurrency of the DAG hasn't been\n                # reached.\n                dag_id = task_instance.dag_id\n                simple_dag = simple_dag_bag.get_dag(dag_id)\n\n                current_dag_concurrency = dag_concurrency_map[dag_id]\n                dag_concurrency_limit = simple_dag_bag.get_dag(dag_id).concurrency\n                self.log.info(\n                    \"DAG %s has %s/%s running and queued tasks\",\n                    dag_id, current_dag_concurrency, dag_concurrency_limit\n                )\n                if current_dag_concurrency >= dag_concurrency_limit:\n                    self.log.info(\n                        \"Not executing %s since the number of tasks running or queued \"\n                        \"from DAG %s is >= to the DAG's task concurrency limit of %s\",\n                        task_instance, dag_id, dag_concurrency_limit\n                    )\n                    continue\n\n                task_concurrency_limit = simple_dag.get_task_special_arg(\n                    task_instance.task_id,\n                    'task_concurrency')\n                if task_concurrency_limit is not None:\n                    current_task_concurrency = task_concurrency_map[\n                        (task_instance.dag_id, task_instance.task_id)\n                    ]\n\n                    if current_task_concurrency >= task_concurrency_limit:\n                        self.log.info(\"Not executing %s since the task concurrency for\"\n                                      \" this task has been reached.\", task_instance)\n                        continue\n\n                if self.executor.has_task(task_instance):\n                    self.log.debug(\n                        \"Not handling task %s as the executor reports it is running\",\n                        task_instance.key\n                    )\n                    continue\n                executable_tis.append(task_instance)\n                open_slots -= 1\n                dag_concurrency_map[dag_id] += 1\n                task_concurrency_map[(task_instance.dag_id, task_instance.task_id)] += 1\n\n            Stats.gauge('pool.starving_tasks.{pool_name}'.format(pool_name=pool_name),\n                        num_starving_tasks)\n\n        task_instance_str = \"\\n\\t\".join(\n            [repr(x) for x in executable_tis])\n        self.log.info(\n            \"Setting the following tasks to queued state:\\n\\t%s\", task_instance_str)\n        # so these dont expire on commit\n        for ti in executable_tis:\n            copy_dag_id = ti.dag_id\n            copy_execution_date = ti.execution_date\n            copy_task_id = ti.task_id\n            make_transient(ti)\n            ti.dag_id = copy_dag_id\n            ti.execution_date = copy_execution_date\n            ti.task_id = copy_task_id\n        return executable_tis", "gt": "def _find_executable_task_instances(self, simple_dag_bag, states, session=None):"}
{"prefix": "def _change_state_for_executable_task_instances(self, task_instances,\n                                                    acceptable_states, session=None):\n        \"\"\"\n        Changes the state of task instances in the list with one of the given states\n        to QUEUED atomically, and returns the TIs changed in SimpleTaskInstance format.\n\n        :param task_instances: TaskInstances to change the state of\n        :type task_instances: list[airflow.models.TaskInstance]\n        :param acceptable_states: Filters the TaskInstances updated to be in these states\n        :type acceptable_states: Iterable[State]\n        :rtype: list[airflow.utils.dag_processing.SimpleTaskInstance]\n        \"\"\"\n        if len(task_instances) == 0:\n            session.commit()\n            return []\n\n        TI = models.TaskInstance\n        filter_for_ti_state_change = (\n            [and_(\n                TI.dag_id == ti.dag_id,\n                TI.task_id == ti.task_id,\n                TI.execution_date == ti.execution_date)\n                for ti in task_instances])\n        ti_query = (\n            session\n            .query(TI)\n            .filter(or_(*filter_for_ti_state_change)))\n\n        if None in acceptable_states:\n            ti_query = ti_query.filter(\n                or_(TI.state == None, TI.state.in_(acceptable_states))  # noqa: E711\n            )\n        else:\n            ti_query = ti_query.filter(TI.state.in_(acceptable_states))\n\n        tis_to_set_to_queued = (\n            ti_query\n            .with_for_update()\n            .all())\n        if len(tis_to_set_to_queued) == 0:\n            self.log.info(\"No tasks were able to have their state changed to queued.\")\n            session.commit()\n            return []\n\n        # set TIs to queued state\n        for task_instance in tis_to_set_to_queued:", "suffix": "            task_instance.queued_dttm = (timezone.utcnow()\n                                         if not task_instance.queued_dttm\n                                         else task_instance.queued_dttm)\n            session.merge(task_instance)\n\n        # Generate a list of SimpleTaskInstance for the use of queuing\n        # them in the executor.\n        simple_task_instances = [SimpleTaskInstance(ti) for ti in\n                                 tis_to_set_to_queued]\n\n        task_instance_str = \"\\n\\t\".join(\n            [repr(x) for x in tis_to_set_to_queued])\n\n        session.commit()\n        self.log.info(\"Setting the following %s tasks to queued state:\\n\\t%s\",\n                      len(tis_to_set_to_queued), task_instance_str)\n        return simple_task_instances", "gt": "            task_instance.state = State.QUEUED"}
{"prefix": "def _enqueue_task_instances_with_queued_state(self, simple_dag_bag,", "suffix": "        \"\"\"\n        Takes task_instances, which should have been set to queued, and enqueues them\n        with the executor.\n\n        :param simple_task_instances: TaskInstances to enqueue\n        :type simple_task_instances: list[SimpleTaskInstance]\n        :param simple_dag_bag: Should contains all of the task_instances' dags\n        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag\n        \"\"\"\n        TI = models.TaskInstance\n        # actually enqueue them\n        for simple_task_instance in simple_task_instances:\n            simple_dag = simple_dag_bag.get_dag(simple_task_instance.dag_id)\n            command = TI.generate_command(\n                simple_task_instance.dag_id,\n                simple_task_instance.task_id,\n                simple_task_instance.execution_date,\n                local=True,\n                mark_success=False,\n                ignore_all_deps=False,\n                ignore_depends_on_past=False,\n                ignore_task_deps=False,\n                ignore_ti_state=False,\n                pool=simple_task_instance.pool,\n                file_path=simple_dag.full_filepath,\n                pickle_id=simple_dag.pickle_id)\n\n            priority = simple_task_instance.priority_weight\n            queue = simple_task_instance.queue\n            self.log.info(\n                \"Sending %s to executor with priority %s and queue %s\",\n                simple_task_instance.key, priority, queue\n            )\n\n            self.executor.queue_command(\n                simple_task_instance,\n                command,\n                priority=priority,\n                queue=queue)", "gt": "                                                  simple_task_instances):"}
{"prefix": "def _execute_task_instances(self,\n                                simple_dag_bag,\n                                states,\n                                session=None):\n        \"\"\"\n        Attempts to execute TaskInstances that should be executed by the scheduler.\n\n        There are three steps:\n        1. Pick TIs by priority with the constraint that they are in the expected states\n        and that we do exceed max_active_runs or pool limits.\n        2. Change the state for the TIs above atomically.\n        3. Enqueue the TIs in the executor.\n\n        :param simple_dag_bag: TaskInstances associated with DAGs in the\n            simple_dag_bag will be fetched from the DB and executed\n        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag\n        :param states: Execute TaskInstances in these states\n        :type states: tuple[airflow.utils.state.State]\n        :return: Number of task instance with state changed.", "suffix": "        executable_tis = self._find_executable_task_instances(simple_dag_bag, states,\n                                                              session=session)\n\n        def query(result, items):\n            simple_tis_with_state_changed = \\\n                self._change_state_for_executable_task_instances(items,\n                                                                 states,\n                                                                 session=session)\n            self._enqueue_task_instances_with_queued_state(\n                simple_dag_bag,\n                simple_tis_with_state_changed)\n            session.commit()\n            return result + len(simple_tis_with_state_changed)\n\n        return helpers.reduce_in_chunks(query, executable_tis, 0, self.max_tis_per_query)", "gt": "        \"\"\""}
{"prefix": "def _change_state_for_tasks_failed_to_execute(self, session):\n        \"\"\"", "suffix": "        we set them back to SCHEDULED to avoid creating hanging tasks.\n\n        :param session: session for ORM operations\n        \"\"\"\n        if self.executor.queued_tasks:\n            TI = models.TaskInstance\n            filter_for_ti_state_change = (\n                [and_(\n                    TI.dag_id == dag_id,\n                    TI.task_id == task_id,\n                    TI.execution_date == execution_date,\n                    # The TI.try_number will return raw try_number+1 since the\n                    # ti is not running. And we need to -1 to match the DB record.\n                    TI._try_number == try_number - 1,\n                    TI.state == State.QUEUED)\n                    for dag_id, task_id, execution_date, try_number\n                    in self.executor.queued_tasks.keys()])\n            ti_query = (session.query(TI)\n                        .filter(or_(*filter_for_ti_state_change)))\n            tis_to_set_to_scheduled = (ti_query\n                                       .with_for_update()\n                                       .all())\n            if len(tis_to_set_to_scheduled) == 0:\n                session.commit()\n                return\n\n            # set TIs to queued state\n            for task_instance in tis_to_set_to_scheduled:\n                task_instance.state = State.SCHEDULED\n\n            task_instance_str = \"\\n\\t\".join(\n                [repr(x) for x in tis_to_set_to_scheduled])\n\n            session.commit()\n            self.log.info(\"Set the following tasks to scheduled state:\\n\\t%s\", task_instance_str)", "gt": "        If there are tasks left over in the executor,"}
{"prefix": "def _process_dags(self, dagbag, dags, tis_out):\n        \"\"\"\n        Iterates over the dags and processes them. Processing includes:\n\n        1. Create appropriate DagRun(s) in the DB.\n        2. Create appropriate TaskInstance(s) in the DB.\n        3. Send emails for tasks that have missed SLAs.\n\n        :param dagbag: a collection of DAGs to process\n        :type dagbag: airflow.models.DagBag\n        :param dags: the DAGs from the DagBag to process\n        :type dags: airflow.models.DAG\n        :param tis_out: A queue to add generated TaskInstance objects", "suffix": "        :rtype: None\n        \"\"\"\n        for dag in dags:\n            dag = dagbag.get_dag(dag.dag_id)\n            if not dag:\n                self.log.error(\"DAG ID %s was not found in the DagBag\", dag.dag_id)\n                continue\n\n            if dag.is_paused:\n                self.log.info(\"Not processing DAG %s since it's paused\", dag.dag_id)\n                continue\n\n            self.log.info(\"Processing %s\", dag.dag_id)\n\n            dag_run = self.create_dag_run(dag)\n            if dag_run:\n                expected_start_date = dag.following_schedule(dag_run.execution_date)\n                if expected_start_date:\n                    schedule_delay = dag_run.start_date - expected_start_date\n                    Stats.timing(\n                        'dagrun.schedule_delay.{dag_id}'.format(dag_id=dag.dag_id),\n                        schedule_delay)\n                self.log.info(\"Created %s\", dag_run)\n            self._process_task_instances(dag, tis_out)\n            self.manage_slas(dag)", "gt": "        :type tis_out: multiprocessing.Queue[TaskInstance]"}
{"prefix": "def _process_executor_events(self, simple_dag_bag, session=None):\n        \"\"\"\n        Respond to executor events.\n        \"\"\"\n        # TODO: this shares quite a lot of code with _manage_executor_state\n\n        TI = models.TaskInstance", "suffix": "                                   .items()):\n            dag_id, task_id, execution_date, try_number = key\n            self.log.info(\n                \"Executor reports execution of %s.%s execution_date=%s \"\n                \"exited with status %s for try_number %s\",\n                dag_id, task_id, execution_date, state, try_number\n            )\n            if state == State.FAILED or state == State.SUCCESS:\n                qry = session.query(TI).filter(TI.dag_id == dag_id,\n                                               TI.task_id == task_id,\n                                               TI.execution_date == execution_date)\n                ti = qry.first()\n                if not ti:\n                    self.log.warning(\"TaskInstance %s went missing from the database\", ti)\n                    continue\n\n                # TODO: should we fail RUNNING as well, as we do in Backfills?\n                if ti.try_number == try_number and ti.state == State.QUEUED:\n                    msg = (\"Executor reports task instance {} finished ({}) \"\n                           \"although the task says its {}. Was the task \"\n                           \"killed externally?\".format(ti, state, ti.state))\n                    self.log.error(msg)\n                    try:\n                        simple_dag = simple_dag_bag.get_dag(dag_id)\n                        dagbag = models.DagBag(simple_dag.full_filepath)\n                        dag = dagbag.get_dag(dag_id)\n                        ti.task = dag.get_task(task_id)\n                        ti.handle_failure(msg)\n                    except Exception:\n                        self.log.error(\"Cannot load the dag bag to handle failure for %s\"\n                                       \". Setting task to FAILED without callbacks or \"\n                                       \"retries. Do you have enough resources?\", ti)\n                        ti.state = State.FAILED\n                        session.merge(ti)\n                        session.commit()", "gt": "        for key, state in list(self.executor.get_event_buffer(simple_dag_bag.dag_ids)"}
{"prefix": "def _execute_helper(self):\n        \"\"\"\n        The actual scheduler loop. The main steps in the loop are:\n            #. Harvest DAG parsing results through DagFileProcessorAgent\n            #. Find and queue executable tasks\n                #. Change task instance state in DB\n                #. Queue tasks in executor\n            #. Heartbeat executor\n                #. Execute queued tasks in executor asynchronously\n                #. Sync on the states of running tasks\n\n        Following is a graphic representation of these steps.\n\n        .. image:: ../docs/img/scheduler_loop.jpg\n\n        :rtype: None\n        \"\"\"\n        self.executor.start()\n", "suffix": "        self.reset_state_for_orphaned_tasks()\n\n        # Start after resetting orphaned tasks to avoid stressing out DB.\n        self.processor_agent.start()\n\n        execute_start_time = timezone.utcnow()\n\n        # Last time that self.heartbeat() was called.\n        last_self_heartbeat_time = timezone.utcnow()\n\n        # For the execute duration, parse and schedule DAGs\n        while True:\n            self.log.debug(\"Starting Loop...\")\n            loop_start_time = time.time()\n\n            if self.using_sqlite:\n                self.processor_agent.heartbeat()\n                # For the sqlite case w/ 1 thread, wait until the processor\n                # is finished to avoid concurrent access to the DB.\n                self.log.debug(\n                    \"Waiting for processors to finish since we're using sqlite\")\n                self.processor_agent.wait_until_finished()\n\n            self.log.debug(\"Harvesting DAG parsing results\")\n            simple_dags = self.processor_agent.harvest_simple_dags()\n            self.log.debug(\"Harvested {} SimpleDAGs\".format(len(simple_dags)))\n\n            # Send tasks for execution if available\n            simple_dag_bag = SimpleDagBag(simple_dags)\n            if len(simple_dags) > 0:\n                try:\n                    simple_dag_bag = SimpleDagBag(simple_dags)\n\n                    # Handle cases where a DAG run state is set (perhaps manually) to\n                    # a non-running state. Handle task instances that belong to\n                    # DAG runs in those states\n\n                    # If a task instance is up for retry but the corresponding DAG run\n                    # isn't running, mark the task instance as FAILED so we don't try\n                    # to re-run it.\n                    self._change_state_for_tis_without_dagrun(simple_dag_bag,\n                                                              [State.UP_FOR_RETRY],\n                                                              State.FAILED)\n                    # If a task instance is scheduled or queued or up for reschedule,\n                    # but the corresponding DAG run isn't running, set the state to\n                    # NONE so we don't try to re-run it.\n                    self._change_state_for_tis_without_dagrun(simple_dag_bag,\n                                                              [State.QUEUED,\n                                                               State.SCHEDULED,\n                                                               State.UP_FOR_RESCHEDULE],\n                                                              State.NONE)\n\n                    self._execute_task_instances(simple_dag_bag,\n                                                 (State.SCHEDULED,))\n                except Exception as e:\n                    self.log.error(\"Error queuing tasks\")\n                    self.log.exception(e)\n                    continue\n\n            # Call heartbeats\n            self.log.debug(\"Heartbeating the executor\")\n            self.executor.heartbeat()\n\n            self._change_state_for_tasks_failed_to_execute()\n\n            # Process events from the executor\n            self._process_executor_events(simple_dag_bag)\n\n            # Heartbeat the scheduler periodically\n            time_since_last_heartbeat = (timezone.utcnow() -\n                                         last_self_heartbeat_time).total_seconds()\n            if time_since_last_heartbeat > self.heartrate:\n                self.log.debug(\"Heartbeating the scheduler\")\n                self.heartbeat()\n                last_self_heartbeat_time = timezone.utcnow()\n\n            is_unit_test = conf.getboolean('core', 'unit_test_mode')\n            loop_end_time = time.time()\n            loop_duration = loop_end_time - loop_start_time\n            self.log.debug(\n                \"Ran scheduling loop in %.2f seconds\",\n                loop_duration)\n\n            if not is_unit_test:\n                self.log.debug(\"Sleeping for %.2f seconds\", self._processor_poll_interval)\n                time.sleep(self._processor_poll_interval)\n\n            # Exit early for a test mode, run one additional scheduler loop\n            # to reduce the possibility that parsed DAG was put into the queue\n            # by the DAG manager but not yet received by DAG agent.\n            if self.processor_agent.done:\n                self._last_loop = True\n\n            if self._last_loop:\n                self.log.info(\"Exiting scheduler loop as all files\"\n                              \" have been processed {} times\".format(self.num_runs))\n                break\n\n            if loop_duration < 1 and not is_unit_test:\n                sleep_length = 1 - loop_duration\n                self.log.debug(\n                    \"Sleeping for {0:.2f} seconds to prevent excessive logging\"\n                    .format(sleep_length))\n                sleep(sleep_length)\n\n        # Stop any processors\n        self.processor_agent.terminate()\n\n        # Verify that all files were processed, and if so, deactivate DAGs that\n        # haven't been touched by the scheduler as they likely have been\n        # deleted.\n        if self.processor_agent.all_files_processed:\n            self.log.info(\n                \"Deactivating DAGs that haven't been touched since %s\",\n                execute_start_time.isoformat()\n            )\n            models.DAG.deactivate_stale_dags(execute_start_time)\n\n        self.executor.end()\n\n        settings.Session.remove()", "gt": "        self.log.info(\"Resetting orphaned tasks for active dag runs\")"}
{"prefix": "def process_file(self, file_path, zombies, pickle_dags=False, session=None):\n        \"\"\"\n        Process a Python file containing Airflow DAGs.\n\n        This includes:\n\n        1. Execute the file and look for DAG objects in the namespace.\n        2. Pickle the DAG and save it to the DB (if necessary).\n        3. For each DAG, see what tasks should run and create appropriate task\n        instances in the DB.\n        4. Record any errors importing the file into ORM\n        5. Kill (in ORM) any task instances belonging to the DAGs that haven't\n        issued a heartbeat in a while.\n\n        Returns a list of SimpleDag objects that represent the DAGs found in\n        the file\n\n        :param file_path: the path to the Python file that should be executed\n        :type file_path: unicode\n        :param zombies: zombie task instances to kill.\n        :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]\n        :param pickle_dags: whether serialize the DAGs found in the file and\n            save them to the db\n        :type pickle_dags: bool\n        :return: a list of SimpleDags made from the Dags found in the file\n        :rtype: list[airflow.utils.dag_processing.SimpleDagBag]\n        \"\"\"\n        self.log.info(\"Processing file %s for tasks to queue\", file_path)\n        # As DAGs are parsed from this file, they will be converted into SimpleDags\n        simple_dags = []\n\n        try:\n            dagbag = models.DagBag(file_path, include_examples=False)\n        except Exception:\n            self.log.exception(\"Failed at reloading the DAG file %s\", file_path)\n            Stats.incr('dag_file_refresh_error', 1, 1)\n            return []\n\n        if len(dagbag.dags) > 0:\n            self.log.info(\"DAG(s) %s retrieved from %s\", dagbag.dags.keys(), file_path)\n        else:\n            self.log.warning(\"No viable dags retrieved from %s\", file_path)\n            self.update_import_errors(session, dagbag)\n            return []\n\n        # Save individual DAGs in the ORM and update DagModel.last_scheduled_time\n        for dag in dagbag.dags.values():\n            dag.sync_to_db()\n\n        paused_dag_ids = [dag.dag_id for dag in dagbag.dags.values()\n                          if dag.is_paused]\n\n        # Pickle the DAGs (if necessary) and put them into a SimpleDag\n        for dag_id in dagbag.dags:\n            # Only return DAGs that are not paused\n            if dag_id not in paused_dag_ids:\n                dag = dagbag.get_dag(dag_id)\n                pickle_id = None\n                if pickle_dags:\n                    pickle_id = dag.pickle(session).id\n                simple_dags.append(SimpleDag(dag, pickle_id=pickle_id))\n\n        if len(self.dag_ids) > 0:\n            dags = [dag for dag in dagbag.dags.values()\n                    if dag.dag_id in self.dag_ids and\n                    dag.dag_id not in paused_dag_ids]\n        else:\n            dags = [dag for dag in dagbag.dags.values()\n                    if not dag.parent_dag and\n                    dag.dag_id not in paused_dag_ids]\n\n        # Not using multiprocessing.Queue() since it's no longer a separate\n        # process and due to some unusual behavior. (empty() incorrectly\n        # returns true?)\n        ti_keys_to_schedule = []\n\n        self._process_dags(dagbag, dags, ti_keys_to_schedule)\n\n        for ti_key in ti_keys_to_schedule:\n            dag = dagbag.dags[ti_key[0]]\n            task = dag.get_task(ti_key[1])\n            ti = models.TaskInstance(task, ti_key[2])\n\n            ti.refresh_from_db(session=session, lock_for_update=True)\n            # We can defer checking the task dependency checks to the worker themselves\n            # since they can be expensive to run in the scheduler.\n            dep_context = DepContext(deps=QUEUE_DEPS, ignore_task_deps=True)\n\n            # Only schedule tasks that have their dependencies met, e.g. to avoid\n            # a task that recently got its state changed to RUNNING from somewhere\n            # other than the scheduler from getting its state overwritten.\n            # TODO(aoen): It's not great that we have to check all the task instance\n            # dependencies twice; once to get the task scheduled, and again to actually\n            # run the task. We should try to come up with a way to only check them once.\n            if ti.are_dependencies_met(\n                    dep_context=dep_context,\n                    session=session,\n                    verbose=True):\n                # Task starts out in the scheduled state. All tasks in the\n                # scheduled state will be sent to the executor\n                ti.state = State.SCHEDULED\n\n            # Also save this task instance to the DB.\n            self.log.info(\"Creating / updating %s in ORM\", ti)\n            session.merge(ti)\n        # commit batch\n        session.commit()\n\n        # Record import errors into the ORM\n        try:\n            self.update_import_errors(session, dagbag)", "suffix": "            self.log.exception(\"Error logging import errors!\")\n        try:\n            dagbag.kill_zombies(zombies)\n        except Exception:\n            self.log.exception(\"Error killing zombies!\")\n\n        return simple_dags", "gt": "        except Exception:"}
{"prefix": "def _update_counters(self, ti_status):\n        \"\"\"\n        Updates the counters per state of the tasks that were running. Can re-add\n        to tasks to run in case required.\n\n        :param ti_status: the internal status of the backfill job tasks\n        :type ti_status: BackfillJob._DagRunTaskStatus\n        \"\"\"\n        for key, ti in list(ti_status.running.items()):", "suffix": "            if ti.state == State.SUCCESS:\n                ti_status.succeeded.add(key)\n                self.log.debug(\"Task instance %s succeeded. Don't rerun.\", ti)\n                ti_status.running.pop(key)\n                continue\n            elif ti.state == State.SKIPPED:\n                ti_status.skipped.add(key)\n                self.log.debug(\"Task instance %s skipped. Don't rerun.\", ti)\n                ti_status.running.pop(key)\n                continue\n            elif ti.state == State.FAILED:\n                self.log.error(\"Task instance %s failed\", ti)\n                ti_status.failed.add(key)\n                ti_status.running.pop(key)\n                continue\n            # special case: if the task needs to run again put it back\n            elif ti.state == State.UP_FOR_RETRY:\n                self.log.warning(\"Task instance %s is up for retry\", ti)\n                ti_status.running.pop(key)\n                ti_status.to_run[key] = ti\n            # special case: if the task needs to be rescheduled put it back\n            elif ti.state == State.UP_FOR_RESCHEDULE:\n                self.log.warning(\"Task instance %s is up for reschedule\", ti)\n                ti_status.running.pop(key)\n                ti_status.to_run[key] = ti\n            # special case: The state of the task can be set to NONE by the task itself\n            # when it reaches concurrency limits. It could also happen when the state\n            # is changed externally, e.g. by clearing tasks from the ui. We need to cover\n            # for that as otherwise those tasks would fall outside of the scope of\n            # the backfill suddenly.\n            elif ti.state == State.NONE:\n                self.log.warning(\n                    \"FIXME: task instance %s state was set to none externally or \"\n                    \"reaching concurrency limits. Re-adding task to queue.\",\n                    ti\n                )\n                ti.set_state(State.SCHEDULED)\n                ti_status.running.pop(key)\n                ti_status.to_run[key] = ti", "gt": "            ti.refresh_from_db()"}
{"prefix": "def _manage_executor_state(self, running):\n        \"\"\"\n        Checks if the executor agrees with the state of task instances\n        that are running\n\n        :param running: dict of key, task to verify\n        \"\"\"\n        executor = self.executor\n\n        for key, state in list(executor.get_event_buffer().items()):\n            if key not in running:\n                self.log.warning(", "suffix": "                    key, state, running.values()\n                )\n                continue\n\n            ti = running[key]\n            ti.refresh_from_db()\n\n            self.log.debug(\"Executor state: %s task %s\", state, ti)\n\n            if state == State.FAILED or state == State.SUCCESS:\n                if ti.state == State.RUNNING or ti.state == State.QUEUED:\n                    msg = (\"Executor reports task instance {} finished ({}) \"\n                           \"although the task says its {}. Was the task \"\n                           \"killed externally?\".format(ti, state, ti.state))\n                    self.log.error(msg)\n                    ti.handle_failure(msg)", "gt": "                    \"%s state %s not in running=%s\","}
{"prefix": "def _get_dag_run(self, run_date, session=None):\n        \"\"\"\n        Returns a dag run for the given run date, which will be matched to an existing\n        dag run if available or create a new dag run otherwise. If the max_active_runs\n        limit is reached, this function will return None.\n\n        :param run_date: the execution date for the dag run\n        :type run_date: datetime.datetime\n        :param session: the database session object\n        :type session: sqlalchemy.orm.session.Session\n        :return: a DagRun in state RUNNING or None\n        \"\"\"\n        run_id = BackfillJob.ID_FORMAT_PREFIX.format(run_date.isoformat())\n\n        # consider max_active_runs but ignore when running subdags\n        respect_dag_max_active_limit = (True\n                                        if (self.dag.schedule_interval and\n                                            not self.dag.is_subdag)\n                                        else False)\n\n        current_active_dag_count = self.dag.get_num_active_runs(external_trigger=False)\n\n        # check if we are scheduling on top of a already existing dag_run\n        # we could find a \"scheduled\" run instead of a \"backfill\"\n        run = DagRun.find(dag_id=self.dag.dag_id,\n                          execution_date=run_date,\n                          session=session)\n\n        if run is not None and len(run) > 0:", "suffix": "            if run.state == State.RUNNING:\n                respect_dag_max_active_limit = False\n        else:\n            run = None\n\n        # enforce max_active_runs limit for dag, special cases already\n        # handled by respect_dag_max_active_limit\n        if (respect_dag_max_active_limit and\n                current_active_dag_count >= self.dag.max_active_runs):\n            return None\n\n        run = run or self.dag.create_dagrun(\n            run_id=run_id,\n            execution_date=run_date,\n            start_date=timezone.utcnow(),\n            state=State.RUNNING,\n            external_trigger=False,\n            session=session,\n            conf=self.conf,\n        )\n\n        # set required transient field\n        run.dag = self.dag\n\n        # explicitly mark as backfill and running\n        run.state = State.RUNNING\n        run.run_id = run_id\n        run.verify_integrity(session=session)\n        return run", "gt": "            run = run[0]"}
{"prefix": "def _task_instances_for_dag_run(self, dag_run, session=None):\n        \"\"\"\n        Returns a map of task instance key to task instance object for the tasks to\n        run in the given dag run.\n\n        :param dag_run: the dag run to get the tasks from\n        :type dag_run: airflow.models.DagRun\n        :param session: the database session object\n        :type session: sqlalchemy.orm.session.Session\n        \"\"\"\n        tasks_to_run = {}\n\n        if dag_run is None:\n            return tasks_to_run\n\n        # check if we have orphaned tasks\n        self.reset_state_for_orphaned_tasks(filter_by_dag_run=dag_run, session=session)\n\n        # for some reason if we don't refresh the reference to run is lost\n        dag_run.refresh_from_db()\n        make_transient(dag_run)\n\n        # TODO(edgarRd): AIRFLOW-1464 change to batch query to improve perf\n        for ti in dag_run.get_task_instances():\n            # all tasks part of the backfill are scheduled to run\n            if ti.state == State.NONE:\n                ti.set_state(State.SCHEDULED, session=session)\n            if ti.state != State.REMOVED:\n                tasks_to_run[ti.key] = ti", "suffix": "        return tasks_to_run", "gt": ""}
{"prefix": "def _process_backfill_task_instances(self,\n                                         ti_status,\n                                         executor,\n                                         pickle_id,\n                                         start_date=None, session=None):\n        \"\"\"\n        Process a set of task instances from a set of dag runs. Special handling is done\n        to account for different task instance states that could be present when running\n        them in a backfill process.\n\n        :param ti_status: the internal status of the job\n        :type ti_status: BackfillJob._DagRunTaskStatus\n        :param executor: the executor to run the task instances\n        :type executor: BaseExecutor\n        :param pickle_id: the pickle_id if dag is pickled, None otherwise\n        :type pickle_id: int\n        :param start_date: the start date of the backfill job\n        :type start_date: datetime.datetime\n        :param session: the current session object\n        :type session: sqlalchemy.orm.session.Session\n        :return: the list of execution_dates for the finished dag runs\n        :rtype: list\n        \"\"\"\n\n        executed_run_dates = []\n\n        while ((len(ti_status.to_run) > 0 or len(ti_status.running) > 0) and\n                len(ti_status.deadlocked) == 0):\n            self.log.debug(\"*** Clearing out not_ready list ***\")\n            ti_status.not_ready.clear()\n\n            # we need to execute the tasks bottom to top\n            # or leaf to root, as otherwise tasks might be\n            # determined deadlocked while they are actually\n            # waiting for their upstream to finish\n            @provide_session\n            def _per_task_process(task, key, ti, session=None):\n                ti.refresh_from_db()\n\n                task = self.dag.get_task(ti.task_id)\n                ti.task = task\n\n                ignore_depends_on_past = (\n                    self.ignore_first_depends_on_past and\n                    ti.execution_date == (start_date or ti.start_date))\n                self.log.debug(\n                    \"Task instance to run %s state %s\", ti, ti.state)\n\n                # The task was already marked successful or skipped by a\n                # different Job. Don't rerun it.\n                if ti.state == State.SUCCESS:\n                    ti_status.succeeded.add(key)\n                    self.log.debug(\"Task instance %s succeeded. Don't rerun.\", ti)\n                    ti_status.to_run.pop(key)\n                    if key in ti_status.running:\n                        ti_status.running.pop(key)\n                    return\n                elif ti.state == State.SKIPPED:\n                    ti_status.skipped.add(key)\n                    self.log.debug(\"Task instance %s skipped. Don't rerun.\", ti)\n                    ti_status.to_run.pop(key)\n                    if key in ti_status.running:\n                        ti_status.running.pop(key)\n                    return\n\n                # guard against externally modified tasks instances or\n                # in case max concurrency has been reached at task runtime\n                elif ti.state == State.NONE:\n                    self.log.warning(\n                        \"FIXME: task instance {} state was set to None \"\n                        \"externally. This should not happen\"\n                    )\n                    ti.set_state(State.SCHEDULED, session=session)\n                if self.rerun_failed_tasks:\n                    # Rerun failed tasks or upstreamed failed tasks\n                    if ti.state in (State.FAILED, State.UPSTREAM_FAILED):\n                        self.log.error(\"Task instance {ti} \"\n                                       \"with state {state}\".format(ti=ti,\n                                                                   state=ti.state))\n                        if key in ti_status.running:\n                            ti_status.running.pop(key)\n                        # Reset the failed task in backfill to scheduled state\n                        ti.set_state(State.SCHEDULED, session=session)\n                else:\n                    # Default behaviour which works for subdag.\n                    if ti.state in (State.FAILED, State.UPSTREAM_FAILED):\n                        self.log.error(\"Task instance {ti} \"\n                                       \"with {state} state\".format(ti=ti,\n                                                                   state=ti.state))\n                        ti_status.failed.add(key)\n                        ti_status.to_run.pop(key)\n                        if key in ti_status.running:\n                            ti_status.running.pop(key)\n                        return\n\n                backfill_context = DepContext(\n                    deps=RUN_DEPS,\n                    ignore_depends_on_past=ignore_depends_on_past,\n                    ignore_task_deps=self.ignore_task_deps,\n                    flag_upstream_failed=True)\n\n                # Is the task runnable? -- then run it\n                # the dependency checker can change states of tis\n                if ti.are_dependencies_met(\n                        dep_context=backfill_context,\n                        session=session,\n                        verbose=self.verbose):\n                    ti.refresh_from_db(lock_for_update=True, session=session)\n                    if ti.state in (State.SCHEDULED, State.UP_FOR_RETRY, State.UP_FOR_RESCHEDULE):\n                        if executor.has_task(ti):\n                            self.log.debug(\n                                \"Task Instance %s already in executor \"\n                                \"waiting for queue to clear\",\n                                ti\n                            )\n                        else:\n                            self.log.debug('Sending %s to executor', ti)\n                            # Skip scheduled state, we are executing immediately\n                            ti.state = State.QUEUED\n                            ti.queued_dttm = timezone.utcnow() if not ti.queued_dttm else ti.queued_dttm\n                            session.merge(ti)\n\n                            cfg_path = None\n                            if executor.__class__ in (executors.LocalExecutor,\n                                                      executors.SequentialExecutor):\n                                cfg_path = tmp_configuration_copy()\n\n                            executor.queue_task_instance(\n                                ti,\n                                mark_success=self.mark_success,\n                                pickle_id=pickle_id,\n                                ignore_task_deps=self.ignore_task_deps,\n                                ignore_depends_on_past=ignore_depends_on_past,\n                                pool=self.pool,\n                                cfg_path=cfg_path)\n                            ti_status.running[key] = ti\n                            ti_status.to_run.pop(key)\n                    session.commit()\n                    return\n\n                if ti.state == State.UPSTREAM_FAILED:\n                    self.log.error(\"Task instance %s upstream failed\", ti)\n                    ti_status.failed.add(key)\n                    ti_status.to_run.pop(key)\n                    if key in ti_status.running:\n                        ti_status.running.pop(key)\n                    return\n\n                # special case\n                if ti.state == State.UP_FOR_RETRY:\n                    self.log.debug(\n                        \"Task instance %s retry period not \"\n                        \"expired yet\", ti)\n                    if key in ti_status.running:\n                        ti_status.running.pop(key)\n                    ti_status.to_run[key] = ti\n                    return\n\n                # special case", "suffix": "                    self.log.debug(\n                        \"Task instance %s reschedule period not \"\n                        \"expired yet\", ti)\n                    if key in ti_status.running:\n                        ti_status.running.pop(key)\n                    ti_status.to_run[key] = ti\n                    return\n\n                # all remaining tasks\n                self.log.debug('Adding %s to not_ready', ti)\n                ti_status.not_ready.add(key)\n\n            non_pool_slots = conf.getint('core', 'non_pooled_backfill_task_slot_count')\n\n            try:\n                for task in self.dag.topological_sort():\n                    for key, ti in list(ti_status.to_run.items()):\n                        if task.task_id != ti.task_id:\n                            continue\n                        if task.pool:\n                            pool = session.query(models.Pool) \\\n                                .filter(models.Pool.pool == task.pool) \\\n                                .first()\n                            if not pool:\n                                raise PoolNotFound('Unknown pool: {}'.format(task.pool))\n\n                            open_slots = pool.open_slots(session=session)\n                            if open_slots <= 0:\n                                raise NoAvailablePoolSlot(\n                                    \"Not scheduling since there are \"\n                                    \"%s open slots in pool %s\".format(\n                                        open_slots, task.pool))\n                        else:\n                            if non_pool_slots <= 0:\n                                raise NoAvailablePoolSlot(\n                                    \"Not scheduling since there are no \"\n                                    \"non_pooled_backfill_task_slot_count.\")\n                            non_pool_slots -= 1\n\n                        num_running_tasks = DAG.get_num_task_instances(\n                            self.dag_id,\n                            states=(State.QUEUED, State.RUNNING))\n\n                        if num_running_tasks >= self.dag.concurrency:\n                            raise DagConcurrencyLimitReached(\n                                \"Not scheduling since concurrency limit \"\n                                \"is reached.\"\n                            )\n\n                        _per_task_process(task, key, ti)\n            except (NoAvailablePoolSlot, DagConcurrencyLimitReached) as e:\n                self.log.debug(e)\n\n            # execute the tasks in the queue\n            self.heartbeat()\n            executor.heartbeat()\n\n            # If the set of tasks that aren't ready ever equals the set of\n            # tasks to run and there are no running tasks then the backfill\n            # is deadlocked\n            if (ti_status.not_ready and\n                    ti_status.not_ready == set(ti_status.to_run) and\n                    len(ti_status.running) == 0):\n                self.log.warning(\n                    \"Deadlock discovered for ti_status.to_run=%s\",\n                    ti_status.to_run.values()\n                )\n                ti_status.deadlocked.update(ti_status.to_run.values())\n                ti_status.to_run.clear()\n\n            # check executor state\n            self._manage_executor_state(ti_status.running)\n\n            # update the task counters\n            self._update_counters(ti_status=ti_status)\n\n            # update dag run state\n            _dag_runs = ti_status.active_runs[:]\n            for run in _dag_runs:\n                run.update_state(session=session)\n                if run.state in State.finished():\n                    ti_status.finished_runs += 1\n                    ti_status.active_runs.remove(run)\n                    executed_run_dates.append(run.execution_date)\n\n            self._log_progress(ti_status)\n\n        # return updated status\n        return executed_run_dates", "gt": "                if ti.state == State.UP_FOR_RESCHEDULE:"}
{"prefix": "def _execute_for_run_dates(self, run_dates, ti_status, executor, pickle_id,\n                               start_date, session=None):\n        \"\"\"\n        Computes the dag runs and their respective task instances for\n        the given run dates and executes the task instances.\n        Returns a list of execution dates of the dag runs that were executed.\n\n        :param run_dates: Execution dates for dag runs", "suffix": "        :param ti_status: internal BackfillJob status structure to tis track progress\n        :type ti_status: BackfillJob._DagRunTaskStatus\n        :param executor: the executor to use, it must be previously started\n        :type executor: BaseExecutor\n        :param pickle_id: numeric id of the pickled dag, None if not pickled\n        :type pickle_id: int\n        :param start_date: backfill start date\n        :type start_date: datetime.datetime\n        :param session: the current session object\n        :type session: sqlalchemy.orm.session.Session\n        \"\"\"\n        for next_run_date in run_dates:\n            dag_run = self._get_dag_run(next_run_date, session=session)\n            tis_map = self._task_instances_for_dag_run(dag_run,\n                                                       session=session)\n            if dag_run is None:\n                continue\n\n            ti_status.active_runs.append(dag_run)\n            ti_status.to_run.update(tis_map or {})\n\n        processed_dag_run_dates = self._process_backfill_task_instances(\n            ti_status=ti_status,\n            executor=executor,\n            pickle_id=pickle_id,\n            start_date=start_date,\n            session=session)\n\n        ti_status.executed_dag_run_dates.update(processed_dag_run_dates)", "gt": "        :type run_dates: list"}
{"prefix": "def _set_unfinished_dag_runs_to_failed(self, dag_runs, session=None):\n        \"\"\"\n        Go through the dag_runs and update the state based on the task_instance state.\n        Then set DAG runs that are not finished to failed.", "suffix": "        :param dag_runs: DAG runs\n        :param session: session\n        :return: None\n        \"\"\"\n        for dag_run in dag_runs:\n            dag_run.update_state()\n            if dag_run.state not in State.finished():\n                dag_run.set_state(State.FAILED)\n            session.merge(dag_run)", "gt": ""}
{"prefix": "def _execute(self, session=None):\n        \"\"\"\n        Initializes all components required to run a dag for a specified date range and", "suffix": "        \"\"\"\n        ti_status = BackfillJob._DagRunTaskStatus()\n\n        start_date = self.bf_start_date\n\n        # Get intervals between the start/end dates, which will turn into dag runs\n        run_dates = self.dag.get_run_dates(start_date=start_date,\n                                           end_date=self.bf_end_date)\n        if self.run_backwards:\n            tasks_that_depend_on_past = [t.task_id for t in self.dag.task_dict.values() if t.depends_on_past]\n            if tasks_that_depend_on_past:\n                raise AirflowException(\n                    'You cannot backfill backwards because one or more tasks depend_on_past: {}'.format(\n                        \",\".join(tasks_that_depend_on_past)))\n            run_dates = run_dates[::-1]\n\n        if len(run_dates) == 0:\n            self.log.info(\"No run dates were found for the given dates and dag interval.\")\n            return\n\n        # picklin'\n        pickle_id = None\n        if not self.donot_pickle and self.executor.__class__ not in (\n                executors.LocalExecutor, executors.SequentialExecutor):\n            pickle = DagPickle(self.dag)\n            session.add(pickle)\n            session.commit()\n            pickle_id = pickle.id\n\n        executor = self.executor\n        executor.start()\n\n        ti_status.total_runs = len(run_dates)  # total dag runs in backfill\n\n        try:\n            remaining_dates = ti_status.total_runs\n            while remaining_dates > 0:\n                dates_to_process = [run_date for run_date in run_dates\n                                    if run_date not in ti_status.executed_dag_run_dates]\n\n                self._execute_for_run_dates(run_dates=dates_to_process,\n                                            ti_status=ti_status,\n                                            executor=executor,\n                                            pickle_id=pickle_id,\n                                            start_date=start_date,\n                                            session=session)\n\n                remaining_dates = (\n                    ti_status.total_runs - len(ti_status.executed_dag_run_dates)\n                )\n                err = self._collect_errors(ti_status=ti_status, session=session)\n                if err:\n                    raise AirflowException(err)\n\n                if remaining_dates > 0:\n                    self.log.info(\n                        \"max_active_runs limit for dag %s has been reached \"\n                        \" - waiting for other dag runs to finish\",\n                        self.dag_id\n                    )\n                    time.sleep(self.delay_on_limit_secs)\n        except (KeyboardInterrupt, SystemExit):\n            self.log.warning(\"Backfill terminated by user.\")\n\n            # TODO: we will need to terminate running task instances and set the\n            # state to failed.\n            self._set_unfinished_dag_runs_to_failed(ti_status.active_runs)\n        finally:\n            session.commit()\n            executor.end()\n\n        self.log.info(\"Backfill done. Exiting.\")", "gt": "        calls helper method to execute the tasks."}
{"prefix": "def heartbeat_callback(self, session=None):", "suffix": "\n        if self.terminating:\n            # ensure termination if processes are created later\n            self.task_runner.terminate()\n            return\n\n        self.task_instance.refresh_from_db()\n        ti = self.task_instance\n\n        fqdn = get_hostname()\n        same_hostname = fqdn == ti.hostname\n        same_process = ti.pid == os.getpid()\n\n        if ti.state == State.RUNNING:\n            if not same_hostname:\n                self.log.warning(\"The recorded hostname %s \"\n                                 \"does not match this instance's hostname \"\n                                 \"%s\", ti.hostname, fqdn)\n                raise AirflowException(\"Hostname of job runner does not match\")\n            elif not same_process:\n                current_pid = os.getpid()\n                self.log.warning(\"Recorded pid %s does not match \"\n                                 \"the current pid %s\", ti.pid, current_pid)\n                raise AirflowException(\"PID of job runner does not match\")\n        elif (\n                self.task_runner.return_code() is None and\n                hasattr(self.task_runner, 'process')\n        ):\n            self.log.warning(\n                \"State of this instance has been externally set to %s. \"\n                \"Taking the poison pill.\",\n                ti.state\n            )\n            self.task_runner.terminate()\n            self.terminating = True", "gt": "        \"\"\"Self destruct task if state has been moved away from running externally\"\"\""}
{"prefix": "def _get_client(self, project_id):\n        \"\"\"\n        Provides a client for interacting with the Cloud Spanner API.", "suffix": "        :param project_id: The ID of the  GCP project.\n        :type project_id: str\n        :return: google.cloud.spanner_v1.client.Client\n        :rtype: object\n        \"\"\"\n        if not self._client:\n            self._client = Client(project=project_id, credentials=self._get_credentials())\n        return self._client", "gt": ""}
{"prefix": "def get_instance(self, instance_id, project_id=None):\n        \"\"\"\n        Gets information about a particular instance.\n\n        :param project_id: Optional, The ID of the  GCP project that owns the Cloud Spanner\n            database.  If set to None or missing, the default project_id from the GCP connection is used.", "suffix": "        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :return: google.cloud.spanner_v1.instance.Instance\n        :rtype: object\n        \"\"\"\n        instance = self._get_client(project_id=project_id).instance(instance_id=instance_id)\n        if not instance.exists():\n            return None\n        return instance", "gt": "        :type project_id: str"}
{"prefix": "def _apply_to_instance(self, project_id, instance_id, configuration_name, node_count,\n                           display_name, func):\n        \"\"\"\n        Invokes a method on a given instance by applying a specified Callable.\n\n        :param project_id: The ID of the  GCP project that owns the Cloud Spanner\n            database.\n        :type project_id: str\n        :param instance_id: The ID of the instance.\n        :type instance_id: str\n        :param configuration_name: Name of the instance configuration defining how the\n            instance will be created. Required for instances which do not yet exist.\n        :type configuration_name: str\n        :param node_count: (Optional) Number of nodes allocated to the instance.\n        :type node_count: int\n        :param display_name: (Optional) The display name for the instance in the Cloud\n            Console UI. (Must be between 4 and 30 characters.) If this value is not set\n            in the constructor, will fall back to the instance ID.\n        :type display_name: str\n        :param func: Method of the instance to be called.\n        :type func: Callable\n        \"\"\"\n        # noinspection PyUnresolvedReferences\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id, configuration_name=configuration_name,\n            node_count=node_count, display_name=display_name)\n        try:\n            operation = func(instance)  # type: Operation\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e", "suffix": "        if operation:\n            result = operation.result()\n            self.log.info(result)", "gt": ""}
{"prefix": "def create_instance(self, instance_id, configuration_name, node_count,\n                        display_name, project_id=None):\n        \"\"\"\n        Creates a new Cloud Spanner instance.\n\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param configuration_name: The name of the instance configuration defining how the\n            instance will be created. Possible configuration values can be retrieved via\n            https://cloud.google.com/spanner/docs/reference/rest/v1/projects.instanceConfigs/list\n        :type configuration_name: str\n        :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner\n            instance.\n        :type node_count: int\n        :param display_name: (Optional) The display name for the instance in the GCP\n            Console. Must be between 4 and 30 characters.  If this value is not set in\n            the constructor, the name falls back to the instance ID.\n        :type display_name: str\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None", "suffix": "        self._apply_to_instance(project_id, instance_id, configuration_name,\n                                node_count, display_name, lambda x: x.create())", "gt": "        \"\"\""}
{"prefix": "def update_instance(self, instance_id, configuration_name, node_count,\n                        display_name, project_id=None):\n        \"\"\"\n        Updates an existing Cloud Spanner instance.\n\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param configuration_name: The name of the instance configuration defining how the\n            instance will be created. Possible configuration values can be retrieved via\n            https://cloud.google.com/spanner/docs/reference/rest/v1/projects.instanceConfigs/list\n        :type configuration_name: str\n        :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner\n            instance.\n        :type node_count: int\n        :param display_name: (Optional) The display name for the instance in the GCP", "suffix": "            the constructor, the name falls back to the instance ID.\n        :type display_name: str\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        return self._apply_to_instance(project_id, instance_id, configuration_name,\n                                       node_count, display_name, lambda x: x.update())", "gt": "            Console. Must be between 4 and 30 characters. If this value is not set in"}
{"prefix": "def delete_instance(self, instance_id, project_id=None):\n        \"\"\"\n        Deletes an existing Cloud Spanner instance.\n\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n", "suffix": "        try:\n            instance.delete()\n            return\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e", "gt": "        instance = self._get_client(project_id=project_id).instance(instance_id)"}
{"prefix": "def get_database(self, instance_id, database_id, project_id=None):\n        \"\"\"\n        Retrieves a database in Cloud Spanner. If the database does not exist\n        in the specified instance, it returns None.\n\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database in Cloud Spanner.\n        :type database_id: str\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: Database object or None if database does not exist\n        :rtype: google.cloud.spanner_v1.database.Database or None\n        \"\"\"\n\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id)\n        if not database.exists():", "suffix": "        else:\n            return database", "gt": "            return None"}
{"prefix": "def create_database(self, instance_id, database_id, ddl_statements, project_id=None):\n        \"\"\"\n        Creates a new database in Cloud Spanner.\n\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database to create in Cloud Spanner.\n        :type database_id: str\n        :param ddl_statements: The string list containing DDL for the new database.\n        :type ddl_statements: list[str]\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :return: None\n        \"\"\"\n\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id,\n                                     ddl_statements=ddl_statements)\n        try:\n            operation = database.create()  # type: Operation\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e\n\n        if operation:\n            result = operation.result()", "suffix": "        return", "gt": "            self.log.info(result)"}
{"prefix": "def update_database(self, instance_id, database_id, ddl_statements,\n                        project_id=None,\n                        operation_id=None):\n        \"\"\"\n        Updates DDL of a database in Cloud Spanner.\n\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database in Cloud Spanner.\n        :type database_id: str\n        :param ddl_statements: The string list containing DDL for the new database.\n        :type ddl_statements: list[str]\n        :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :param operation_id: (Optional) The unique per database operation ID that can be\n            specified to implement idempotency check.\n        :type operation_id: str\n        :return: None\n        \"\"\"\n\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id)\n        try:\n            operation = database.update_ddl(\n                ddl_statements=ddl_statements, operation_id=operation_id)\n            if operation:\n                result = operation.result()\n                self.log.info(result)\n            return\n        except AlreadyExists as e:", "suffix": "                self.log.info(\"Replayed update_ddl message - the operation id %s \"\n                              \"was already done before.\", operation_id)\n                return\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e", "gt": "            if e.code == 409 and operation_id in e.message:"}
{"prefix": "def delete_database(self, instance_id, database_id, project_id=None):\n        \"\"\"\n        Drops a database in Cloud Spanner.\n\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database in Cloud Spanner.\n        :type database_id: str\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :return: True if everything succeeded\n        :rtype: bool\n        \"\"\"\n\n        instance = self._get_client(project_id=project_id).\\\n            instance(instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id)\n        if not database.exists():", "suffix": "                          \"Exiting.\".format(database_id, instance_id))\n            return\n        try:\n            operation = database.drop()  # type: Operation\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e\n\n        if operation:\n            result = operation.result()\n            self.log.info(result)\n        return", "gt": "            self.log.info(\"The database {} is already deleted from instance {}. \""}
{"prefix": "def execute_dml(self, instance_id, database_id, queries, project_id=None):\n        \"\"\"\n        Executes an arbitrary DML query (INSERT, UPDATE, DELETE).", "suffix": "        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database in Cloud Spanner.\n        :type database_id: str\n        :param queries: The queries to execute.\n        :type queries: str\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        \"\"\"\n        self._get_client(project_id=project_id).instance(instance_id=instance_id).\\\n            database(database_id=database_id).run_in_transaction(\n            lambda transaction: self._execute_sql_in_transaction(transaction, queries))", "gt": ""}
{"prefix": "def poke(self, context):\n        \"\"\"\n        Pokes for a mail attachment on the mail server.\n\n        :param context: The context that is being provided when poking.\n        :type context: dict\n        :return: True if attachment with the given name is present and False if not.", "suffix": "        \"\"\"\n        self.log.info('Poking for %s', self.attachment_name)\n\n        with ImapHook(imap_conn_id=self.conn_id) as imap_hook:\n            return imap_hook.has_mail_attachment(\n                name=self.attachment_name,\n                mail_folder=self.mail_folder,\n                check_regex=self.check_regex\n            )", "gt": "        :rtype: bool"}
{"prefix": "def prepare_additional_parameters(additional_properties, language_hints, web_detection_params):\n    \"\"\"\n    Creates additional_properties parameter based on language_hints, web_detection_params and\n    additional_properties parameters specified by the user\n    \"\"\"\n    if language_hints is None and web_detection_params is None:\n        return additional_properties\n\n    if additional_properties is None:\n        return {}\n\n    merged_additional_parameters = deepcopy(additional_properties)\n\n    if 'image_context' not in merged_additional_parameters:\n        merged_additional_parameters['image_context'] = {}\n\n    merged_additional_parameters['image_context']['language_hints'] = merged_additional_parameters[\n        'image_context'\n    ].get('language_hints', language_hints)", "suffix": "        'image_context'\n    ].get('web_detection_params', web_detection_params)\n\n    return merged_additional_parameters", "gt": "    merged_additional_parameters['image_context']['web_detection_params'] = merged_additional_parameters["}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Returns a cassandra Session object\n        \"\"\"\n        if self.session and not self.session.is_shutdown:", "suffix": "        self.session = self.cluster.connect(self.keyspace)\n        return self.session", "gt": "            return self.session"}
{"prefix": "def table_exists(self, table):\n        \"\"\"", "suffix": "\n        :param table: Target Cassandra table.\n                      Use dot notation to target a specific keyspace.\n        :type table: str\n        \"\"\"\n        keyspace = self.keyspace\n        if '.' in table:\n            keyspace, table = table.split('.', 1)\n        cluster_metadata = self.get_conn().cluster.metadata\n        return (keyspace in cluster_metadata.keyspaces and\n                table in cluster_metadata.keyspaces[keyspace].tables)", "gt": "        Checks if a table exists in Cassandra"}
{"prefix": "def record_exists(self, table, keys):", "suffix": "        Checks if a record exists in Cassandra\n\n        :param table: Target Cassandra table.\n                      Use dot notation to target a specific keyspace.\n        :type table: str\n        :param keys: The keys and their values to check the existence.\n        :type keys: dict\n        \"\"\"\n        keyspace = self.keyspace\n        if '.' in table:\n            keyspace, table = table.split('.', 1)\n        ks = \" AND \".join(\"{}=%({})s\".format(key, key) for key in keys.keys())\n        cql = \"SELECT * FROM {keyspace}.{table} WHERE {keys}\".format(\n            keyspace=keyspace, table=table, keys=ks)\n\n        try:\n            rs = self.get_conn().execute(cql, keys)\n            return rs.one() is not None\n        except Exception:\n            return False", "gt": "        \"\"\""}
{"prefix": "def _build_spark_submit_command(self, application):\n        \"\"\"\n        Construct the spark-submit command to execute.\n        :param application: command to append to the spark-submit command\n        :type application: str\n        :return: full command to be executed\n        \"\"\"\n        connection_cmd = self._get_spark_binary_path()\n\n        # The url ot the spark master\n        connection_cmd += [\"--master\", self._connection['master']]\n\n        if self._conf:\n            for key in self._conf:\n                connection_cmd += [\"--conf\", \"{}={}\".format(key, str(self._conf[key]))]\n        if self._env_vars and (self._is_kubernetes or self._is_yarn):\n            if self._is_yarn:\n                tmpl = \"spark.yarn.appMasterEnv.{}={}\"\n            else:\n                tmpl = \"spark.kubernetes.driverEnv.{}={}\"\n            for key in self._env_vars:\n                connection_cmd += [\n                    \"--conf\",\n                    tmpl.format(key, str(self._env_vars[key]))]\n        elif self._env_vars and self._connection['deploy_mode'] != \"cluster\":\n            self._env = self._env_vars  # Do it on Popen of the process\n        elif self._env_vars and self._connection['deploy_mode'] == \"cluster\":\n            raise AirflowException(\n                \"SparkSubmitHook env_vars is not supported in standalone-cluster mode.\")\n        if self._is_kubernetes:\n            connection_cmd += [\"--conf\", \"spark.kubernetes.namespace={}\".format(\n                self._connection['namespace'])]\n        if self._files:\n            connection_cmd += [\"--files\", self._files]\n        if self._py_files:\n            connection_cmd += [\"--py-files\", self._py_files]\n        if self._archives:\n            connection_cmd += [\"--archives\", self._archives]\n        if self._driver_class_path:\n            connection_cmd += [\"--driver-class-path\", self._driver_class_path]\n        if self._jars:\n            connection_cmd += [\"--jars\", self._jars]\n        if self._packages:\n            connection_cmd += [\"--packages\", self._packages]\n        if self._exclude_packages:\n            connection_cmd += [\"--exclude-packages\", self._exclude_packages]\n        if self._repositories:", "suffix": "        if self._num_executors:\n            connection_cmd += [\"--num-executors\", str(self._num_executors)]\n        if self._total_executor_cores:\n            connection_cmd += [\"--total-executor-cores\", str(self._total_executor_cores)]\n        if self._executor_cores:\n            connection_cmd += [\"--executor-cores\", str(self._executor_cores)]\n        if self._executor_memory:\n            connection_cmd += [\"--executor-memory\", self._executor_memory]\n        if self._driver_memory:\n            connection_cmd += [\"--driver-memory\", self._driver_memory]\n        if self._keytab:\n            connection_cmd += [\"--keytab\", self._keytab]\n        if self._principal:\n            connection_cmd += [\"--principal\", self._principal]\n        if self._name:\n            connection_cmd += [\"--name\", self._name]\n        if self._java_class:\n            connection_cmd += [\"--class\", self._java_class]\n        if self._verbose:\n            connection_cmd += [\"--verbose\"]\n        if self._connection['queue']:\n            connection_cmd += [\"--queue\", self._connection['queue']]\n        if self._connection['deploy_mode']:\n            connection_cmd += [\"--deploy-mode\", self._connection['deploy_mode']]\n\n        # The actual script to execute\n        connection_cmd += [application]\n\n        # Append any application arguments\n        if self._application_args:\n            connection_cmd += self._application_args\n\n        self.log.info(\"Spark-Submit cmd: %s\", connection_cmd)\n\n        return connection_cmd", "gt": "            connection_cmd += [\"--repositories\", self._repositories]"}
{"prefix": "def _build_track_driver_status_command(self):\n        \"\"\"\n        Construct the command to poll the driver status.\n\n        :return: full command to be executed\n        \"\"\"\n        connection_cmd = self._get_spark_binary_path()\n\n        # The url ot the spark master\n        connection_cmd += [\"--master\", self._connection['master']]\n\n        # The driver id so we can poll for its status\n        if self._driver_id:\n            connection_cmd += [\"--status\", self._driver_id]", "suffix": "            raise AirflowException(\n                \"Invalid status: attempted to poll driver \" +\n                \"status but no driver id is known. Giving up.\")\n\n        self.log.debug(\"Poll driver status cmd: %s\", connection_cmd)\n\n        return connection_cmd", "gt": "        else:"}
{"prefix": "def submit(self, application=\"\", **kwargs):\n        \"\"\"\n        Remote Popen to execute the spark-submit job\n\n        :param application: Submitted application, jar or py file\n        :type application: str\n        :param kwargs: extra arguments to Popen (see subprocess.Popen)\n        \"\"\"\n        spark_submit_cmd = self._build_spark_submit_command(application)\n\n        if hasattr(self, '_env'):\n            env = os.environ.copy()\n            env.update(self._env)\n            kwargs[\"env\"] = env\n\n        self._submit_sp = subprocess.Popen(spark_submit_cmd,\n                                           stdout=subprocess.PIPE,\n                                           stderr=subprocess.STDOUT,\n                                           bufsize=-1,\n                                           universal_newlines=True,\n                                           **kwargs)\n\n        self._process_spark_submit_log(iter(self._submit_sp.stdout.readline, ''))\n        returncode = self._submit_sp.wait()\n\n        # Check spark-submit return code. In Kubernetes mode, also check the value\n        # of exit code in the log, as it may differ.\n        if returncode or (self._is_kubernetes and self._spark_exit_code != 0):\n            raise AirflowException(\n                \"Cannot execute: {}. Error code is: {}.\".format(\n                    spark_submit_cmd, returncode\n                )\n            )\n\n        self.log.debug(\"Should track driver: {}\".format(self._should_track_driver_status))\n\n        # We want the Airflow job to wait until the Spark driver is finished\n        if self._should_track_driver_status:\n            if self._driver_id is None:\n                raise AirflowException(\n                    \"No driver id is known: something went wrong when executing \" +\n                    \"the spark submit command\"\n                )", "suffix": "            # We start with the SUBMITTED status as initial status\n            self._driver_status = \"SUBMITTED\"\n\n            # Start tracking the driver status (blocking function)\n            self._start_driver_status_tracking()\n\n            if self._driver_status != \"FINISHED\":\n                raise AirflowException(\n                    \"ERROR : Driver {} badly exited with status {}\"\n                    .format(self._driver_id, self._driver_status)\n                )", "gt": ""}
{"prefix": "def _process_spark_submit_log(self, itr):\n        \"\"\"\n        Processes the log files and extracts useful information out of it.\n\n        If the deploy-mode is 'client', log the output of the submit command as those\n        are the output logs of the Spark worker directly.\n\n        Remark: If the driver needs to be tracked for its status, the log-level of the\n        spark deploy needs to be at least INFO (log4j.logger.org.apache.spark.deploy=INFO)\n\n        :param itr: An iterator which iterates over the input of the subprocess\n        \"\"\"\n        # Consume the iterator\n        for line in itr:\n            line = line.strip()\n            # If we run yarn cluster mode, we want to extract the application id from\n            # the logs so we can kill the application when we stop it unexpectedly\n            if self._is_yarn and self._connection['deploy_mode'] == 'cluster':\n                match = re.search('(application[0-9_]+)', line)\n                if match:\n                    self._yarn_application_id = match.groups()[0]\n                    self.log.info(\"Identified spark driver id: %s\",\n                                  self._yarn_application_id)\n\n            # If we run Kubernetes cluster mode, we want to extract the driver pod id\n            # from the logs so we can kill the application when we stop it unexpectedly", "suffix": "                match = re.search(r'\\s*pod name: ((.+?)-([a-z0-9]+)-driver)', line)\n                if match:\n                    self._kubernetes_driver_pod = match.groups()[0]\n                    self.log.info(\"Identified spark driver pod: %s\",\n                                  self._kubernetes_driver_pod)\n\n                # Store the Spark Exit code\n                match_exit_code = re.search(r'\\s*Exit code: (\\d+)', line)\n                if match_exit_code:\n                    self._spark_exit_code = int(match_exit_code.groups()[0])\n\n            # if we run in standalone cluster mode and we want to track the driver status\n            # we need to extract the driver id from the logs. This allows us to poll for\n            # the status using the driver id. Also, we can kill the driver when needed.\n            elif self._should_track_driver_status and not self._driver_id:\n                match_driver_id = re.search(r'(driver-[0-9\\-]+)', line)\n                if match_driver_id:\n                    self._driver_id = match_driver_id.groups()[0]\n                    self.log.info(\"identified spark driver id: {}\"\n                                  .format(self._driver_id))\n\n            else:\n                self.log.info(line)\n\n            self.log.debug(\"spark submit log: {}\".format(line))", "gt": "            elif self._is_kubernetes:"}
{"prefix": "def _process_spark_status_log(self, itr):\n        \"\"\"\n        parses the logs of the spark driver status query process\n\n        :param itr: An iterator which iterates over the input of the subprocess\n        \"\"\"\n        # Consume the iterator\n        for line in itr:\n            line = line.strip()\n\n            # Check if the log line is about the driver status and extract the status.\n            if \"driverState\" in line:\n                self._driver_status = line.split(' : ')[1] \\", "suffix": "\n            self.log.debug(\"spark driver status log: {}\".format(line))", "gt": "                    .replace(',', '').replace('\\\"', '').strip()"}
{"prefix": "def _start_driver_status_tracking(self):\n        \"\"\"\n        Polls the driver based on self._driver_id to get the status.\n        Finish successfully when the status is FINISHED.\n        Finish failed when the status is ERROR/UNKNOWN/KILLED/FAILED.\n\n        Possible status:\n\n        SUBMITTED\n            Submitted but not yet scheduled on a worker\n        RUNNING\n            Has been allocated to a worker to run\n        FINISHED\n            Previously ran and exited cleanly\n        RELAUNCHING\n            Exited non-zero or due to worker failure, but has not yet\n            started running again\n        UNKNOWN\n            The status of the driver is temporarily not known due to\n            master failure recovery\n        KILLED\n            A user manually killed this driver\n        FAILED\n            The driver exited non-zero and was not supervised\n        ERROR\n            Unable to run or restart due to an unrecoverable error\n            (e.g. missing jar file)\n        \"\"\"\n\n        # When your Spark Standalone cluster is not performing well\n        # due to misconfiguration or heavy loads.\n        # it is possible that the polling request will timeout.", "suffix": "        missed_job_status_reports = 0\n        max_missed_job_status_reports = 10\n\n        # Keep polling as long as the driver is processing\n        while self._driver_status not in [\"FINISHED\", \"UNKNOWN\",\n                                          \"KILLED\", \"FAILED\", \"ERROR\"]:\n\n            # Sleep for 1 second as we do not want to spam the cluster\n            time.sleep(1)\n\n            self.log.debug(\"polling status of spark driver with id {}\"\n                           .format(self._driver_id))\n\n            poll_drive_status_cmd = self._build_track_driver_status_command()\n            status_process = subprocess.Popen(poll_drive_status_cmd,\n                                              stdout=subprocess.PIPE,\n                                              stderr=subprocess.STDOUT,\n                                              bufsize=-1,\n                                              universal_newlines=True)\n\n            self._process_spark_status_log(iter(status_process.stdout.readline, ''))\n            returncode = status_process.wait()\n\n            if returncode:\n                if missed_job_status_reports < max_missed_job_status_reports:\n                    missed_job_status_reports = missed_job_status_reports + 1\n                else:\n                    raise AirflowException(\n                        \"Failed to poll for the driver status {} times: returncode = {}\"\n                        .format(max_missed_job_status_reports, returncode)\n                    )", "gt": "        # Therefore we use a simple retry mechanism."}
{"prefix": "def _build_spark_driver_kill_command(self):\n        \"\"\"\n        Construct the spark-submit command to kill a driver.\n        :return: full command to kill a driver\n        \"\"\"\n\n        # If the spark_home is passed then build the spark-submit executable path using\n        # the spark_home; otherwise assume that spark-submit is present in the path to\n        # the executing user\n        if self._connection['spark_home']:\n            connection_cmd = [os.path.join(self._connection['spark_home'],", "suffix": "                                           self._connection['spark_binary'])]\n        else:\n            connection_cmd = [self._connection['spark_binary']]\n\n        # The url ot the spark master\n        connection_cmd += [\"--master\", self._connection['master']]\n\n        # The actual kill command\n        connection_cmd += [\"--kill\", self._driver_id]\n\n        self.log.debug(\"Spark-Kill cmd: %s\", connection_cmd)\n\n        return connection_cmd", "gt": "                                           'bin',"}
{"prefix": "def get_task_runner(local_task_job):\n    \"\"\"\n    Get the task runner that can be used to run the given job.\n\n    :param local_task_job: The LocalTaskJob associated with the TaskInstance\n        that needs to be executed.\n    :type local_task_job: airflow.jobs.LocalTaskJob\n    :return: The task runner to use to run the task.", "suffix": "    \"\"\"\n    if _TASK_RUNNER == \"StandardTaskRunner\":\n        return StandardTaskRunner(local_task_job)\n    elif _TASK_RUNNER == \"CgroupTaskRunner\":\n        from airflow.contrib.task_runner.cgroup_task_runner import CgroupTaskRunner\n        return CgroupTaskRunner(local_task_job)\n    else:\n        raise AirflowException(\"Unknown task runner type {}\".format(_TASK_RUNNER))", "gt": "    :rtype: airflow.task.task_runner.base_task_runner.BaseTaskRunner"}
{"prefix": "def _wait_for_task_ended(self):\n        \"\"\"\n        Try to use a waiter from the below pull request\n\n            * https://github.com/boto/botocore/pull/1307\n\n        If the waiter is not available apply a exponential backoff\n\n            * docs.aws.amazon.com/general/latest/gr/api-retries.html\n        \"\"\"\n        try:\n            waiter = self.client.get_waiter('job_execution_complete')\n            waiter.config.max_attempts = sys.maxsize  # timeout is managed by airflow\n            waiter.wait(jobs=[self.jobId])\n        except ValueError:\n            # If waiter not available use expo\n            retry = True\n            retries = 0\n\n            while retries < self.max_retries and retry:\n                self.log.info('AWS Batch retry in the next %s seconds', retries)\n                response = self.client.describe_jobs(\n                    jobs=[self.jobId]\n                )\n                if response['jobs'][-1]['status'] in ['SUCCEEDED', 'FAILED']:\n                    retry = False\n\n                sleep(1 + pow(retries * 0.1, 2))", "suffix": "", "gt": "                retries += 1"}
{"prefix": "def _query_mysql(self):\n        \"\"\"\n        Queries mysql and returns a cursor to the results.\n        \"\"\"", "suffix": "        conn = mysql.get_conn()\n        cursor = conn.cursor()\n        cursor.execute(self.sql)\n        return cursor", "gt": "        mysql = MySqlHook(mysql_conn_id=self.mysql_conn_id)"}
{"prefix": "def _write_local_data_files(self, cursor):\n        \"\"\"\n        Takes a cursor, and writes results to a local file.\n\n        :return: A dictionary where keys are filenames to be used as object\n            names in GCS, and values are file handles to local files that\n            contain the data for the GCS objects.\n        \"\"\"\n        schema = list(map(lambda schema_tuple: schema_tuple[0], cursor.description))\n        col_type_dict = self._get_col_type_dict()\n        file_no = 0\n        tmp_file_handle = NamedTemporaryFile(delete=True)\n        if self.export_format == 'csv':\n            file_mime_type = 'text/csv'\n        else:\n            file_mime_type = 'application/json'\n        files_to_upload = [{\n            'file_name': self.filename.format(file_no),\n            'file_handle': tmp_file_handle,\n            'file_mime_type': file_mime_type\n        }]\n\n        if self.export_format == 'csv':\n            csv_writer = self._configure_csv_file(tmp_file_handle, schema)\n\n        for row in cursor:\n            # Convert datetime objects to utc seconds, and decimals to floats.\n            # Convert binary type object to string encoded with base64.\n            row = self._convert_types(schema, col_type_dict, row)\n\n            if self.export_format == 'csv':\n                csv_writer.writerow(row)\n            else:\n                row_dict = dict(zip(schema, row))\n\n                # TODO validate that row isn't > 2MB. BQ enforces a hard row size of 2MB.\n                s = json.dumps(row_dict, sort_keys=True).encode('utf-8')\n                tmp_file_handle.write(s)\n\n                # Append newline to make dumps BigQuery compatible.\n                tmp_file_handle.write(b'\\n')\n\n            # Stop if the file exceeds the file size limit.\n            if tmp_file_handle.tell() >= self.approx_max_file_size_bytes:\n                file_no += 1\n                tmp_file_handle = NamedTemporaryFile(delete=True)\n                files_to_upload.append({\n                    'file_name': self.filename.format(file_no),\n                    'file_handle': tmp_file_handle,\n                    'file_mime_type': file_mime_type\n                })\n", "suffix": "                    csv_writer = self._configure_csv_file(tmp_file_handle, schema)\n\n        return files_to_upload", "gt": "                if self.export_format == 'csv':"}
{"prefix": "def _configure_csv_file(self, file_handle, schema):\n        \"\"\"Configure a csv writer with the file_handle and write schema\n        as headers for the new file.\n        \"\"\"", "suffix": "                                delimiter=self.field_delimiter)\n        csv_writer.writerow(schema)\n        return csv_writer", "gt": "        csv_writer = csv.writer(file_handle, encoding='utf-8',"}
{"prefix": "def _write_local_schema_file(self, cursor):\n        \"\"\"\n        Takes a cursor, and writes the BigQuery schema in .json format for the\n        results to a local file system.\n\n        :return: A dictionary where key is a filename to be used as an object\n            name in GCS, and values are file handles to local files that\n            contains the BigQuery schema fields in .json format.\n        \"\"\"\n        schema_str = None\n        schema_file_mime_type = 'application/json'\n        tmp_schema_file_handle = NamedTemporaryFile(delete=True)\n        if self.schema is not None and isinstance(self.schema, string_types):\n            schema_str = self.schema.encode('utf-8')\n        elif self.schema is not None and isinstance(self.schema, list):\n            schema_str = json.dumps(self.schema).encode('utf-8')\n        else:\n            schema = []\n            for field in cursor.description:\n                # See PEP 249 for details about the description tuple.\n                field_name = field[0]\n                field_type = self.type_map(field[1])\n                # Always allow TIMESTAMP to be nullable. MySQLdb returns None types\n                # for required fields because some MySQL timestamps can't be\n                # represented by Python's datetime (e.g. 0000-00-00 00:00:00).\n                if field[6] or field_type == 'TIMESTAMP':\n                    field_mode = 'NULLABLE'\n                else:\n                    field_mode = 'REQUIRED'\n                schema.append({\n                    'name': field_name,\n                    'type': field_type,\n                    'mode': field_mode,\n                })\n            schema_str = json.dumps(schema, sort_keys=True).encode('utf-8')", "suffix": "\n        self.log.info('Using schema for %s: %s', self.schema_filename, schema_str)\n        schema_file_to_upload = {\n            'file_name': self.schema_filename,\n            'file_handle': tmp_schema_file_handle,\n            'file_mime_type': schema_file_mime_type\n        }\n        return schema_file_to_upload", "gt": "        tmp_schema_file_handle.write(schema_str)"}
{"prefix": "", "suffix": "        \"\"\"\n        Upload all of the file splits (and optionally the schema .json file) to\n        Google cloud storage.\n        \"\"\"\n        hook = GoogleCloudStorageHook(\n            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,\n            delegate_to=self.delegate_to)\n        for tmp_file in files_to_upload:\n            hook.upload(self.bucket, tmp_file.get('file_name'),\n                        tmp_file.get('file_handle').name,\n                        mime_type=tmp_file.get('file_mime_type'))", "gt": "def _upload_to_gcs(self, files_to_upload):"}
{"prefix": "def _convert_types(schema, col_type_dict, row):\n        \"\"\"\n        Takes a value from MySQLdb, and converts it to a value that's safe for\n        JSON/Google cloud storage/BigQuery. Dates are converted to UTC seconds.\n        Decimals are converted to floats. Binary type fields are encoded with base64,\n        as imported BYTES data must be base64-encoded according to Bigquery SQL\n        date type documentation: https://cloud.google.com/bigquery/data-types\n        \"\"\"\n        converted_row = []\n        for col_name, col_val in zip(schema, row):\n            if type(col_val) in (datetime, date):\n                col_val = time.mktime(col_val.timetuple())\n            elif isinstance(col_val, Decimal):\n                col_val = float(col_val)\n            elif col_type_dict.get(col_name) == \"BYTES\":\n                col_val = base64.standard_b64encode(col_val).decode('ascii')\n            else:\n                col_val = col_val", "suffix": "        return converted_row", "gt": "            converted_row.append(col_val)"}
{"prefix": "def _get_col_type_dict(self):\n        \"\"\"\n        Return a dict of column name and column type based on self.schema if not None.\n        \"\"\"\n        schema = []\n        if isinstance(self.schema, string_types):\n            schema = json.loads(self.schema)\n        elif isinstance(self.schema, list):\n            schema = self.schema", "suffix": "            self.log.warn('Using default schema due to unexpected type.'\n                          'Should be a string or list.')\n\n        col_type_dict = {}\n        try:\n            col_type_dict = {col['name']: col['type'] for col in schema}\n        except KeyError:\n            self.log.warn('Using default schema due to missing name or type. Please '\n                          'refer to: https://cloud.google.com/bigquery/docs/schemas'\n                          '#specifying_a_json_schema_file')\n        return col_type_dict", "gt": "        elif self.schema is not None:"}
{"prefix": "def type_map(cls, mysql_type):\n        \"\"\"\n        Helper function that maps from MySQL fields to BigQuery fields. Used\n        when a schema_filename is set.\n        \"\"\"\n        d = {", "suffix": "            FIELD_TYPE.TINY: 'INTEGER',\n            FIELD_TYPE.BIT: 'INTEGER',\n            FIELD_TYPE.DATETIME: 'TIMESTAMP',\n            FIELD_TYPE.DATE: 'TIMESTAMP',\n            FIELD_TYPE.DECIMAL: 'FLOAT',\n            FIELD_TYPE.NEWDECIMAL: 'FLOAT',\n            FIELD_TYPE.DOUBLE: 'FLOAT',\n            FIELD_TYPE.FLOAT: 'FLOAT',\n            FIELD_TYPE.LONG: 'INTEGER',\n            FIELD_TYPE.LONGLONG: 'INTEGER',\n            FIELD_TYPE.SHORT: 'INTEGER',\n            FIELD_TYPE.TIMESTAMP: 'TIMESTAMP',\n            FIELD_TYPE.YEAR: 'INTEGER',\n        }\n        return d[mysql_type] if mysql_type in d else 'STRING'", "gt": "            FIELD_TYPE.INT24: 'INTEGER',"}
{"prefix": "def authenticate(session, username, password):\n    \"\"\"\n    Authenticate a PasswordUser with the specified\n    username/password.\n\n    :param session: An active SQLAlchemy session\n    :param username: The username\n    :param password: The password\n\n    :raise AuthenticationError: if an error occurred\n    :return: a PasswordUser\n    \"\"\"\n    if not username or not password:\n        raise AuthenticationError()\n\n    user = session.query(PasswordUser).filter(\n        PasswordUser.username == username).first()\n\n    if not user:\n        raise AuthenticationError()\n", "suffix": "        raise AuthenticationError()\n\n    log.info(\"User %s successfully authenticated\", username)\n    return user", "gt": "    if not user.authenticate(password):"}
{"prefix": "def execute(self, context):\n        \"\"\"\n        Execute sqoop job\n        \"\"\"\n        self.hook = SqoopHook(\n            conn_id=self.conn_id,\n            verbose=self.verbose,\n            num_mappers=self.num_mappers,\n            hcatalog_database=self.hcatalog_database,\n            hcatalog_table=self.hcatalog_table,\n            properties=self.properties\n        )\n\n        if self.cmd_type == 'export':\n            self.hook.export_table(\n                table=self.table,", "suffix": "                input_null_string=self.input_null_string,\n                input_null_non_string=self.input_null_non_string,\n                staging_table=self.staging_table,\n                clear_staging_table=self.clear_staging_table,\n                enclosed_by=self.enclosed_by,\n                escaped_by=self.escaped_by,\n                input_fields_terminated_by=self.input_fields_terminated_by,\n                input_lines_terminated_by=self.input_lines_terminated_by,\n                input_optionally_enclosed_by=self.input_optionally_enclosed_by,\n                batch=self.batch,\n                relaxed_isolation=self.relaxed_isolation,\n                extra_export_options=self.extra_export_options)\n        elif self.cmd_type == 'import':\n            # add create hcatalog table to extra import options if option passed\n            # if new params are added to constructor can pass them in here\n            # so don't modify sqoop_hook for each param\n            if self.create_hcatalog_table:\n                self.extra_import_options['create-hcatalog-table'] = ''\n\n            if self.table and self.query:\n                raise AirflowException(\n                    'Cannot specify query and table together. Need to specify either or.'\n                )\n\n            if self.table:\n                self.hook.import_table(\n                    table=self.table,\n                    target_dir=self.target_dir,\n                    append=self.append,\n                    file_type=self.file_type,\n                    columns=self.columns,\n                    split_by=self.split_by,\n                    where=self.where,\n                    direct=self.direct,\n                    driver=self.driver,\n                    extra_import_options=self.extra_import_options)\n            elif self.query:\n                self.hook.import_query(\n                    query=self.query,\n                    target_dir=self.target_dir,\n                    append=self.append,\n                    file_type=self.file_type,\n                    split_by=self.split_by,\n                    direct=self.direct,\n                    driver=self.driver,\n                    extra_import_options=self.extra_import_options)\n            else:\n                raise AirflowException(\n                    \"Provide query or table parameter to import using Sqoop\"\n                )\n        else:\n            raise AirflowException(\"cmd_type should be 'import' or 'export'\")", "gt": "                export_dir=self.export_dir,"}
{"prefix": "def apply_lineage(func):\n    \"\"\"\n    Saves the lineage to XCom and if configured to do so sends it\n    to the backend.\n    \"\"\"\n    backend = _get_backend()\n\n    @wraps(func)\n    def wrapper(self, context, *args, **kwargs):\n        self.log.debug(\"Backend: %s, Lineage called with inlets: %s, outlets: %s\",\n                       backend, self.inlets, self.outlets)\n        ret_val = func(self, context, *args, **kwargs)\n\n        outlets = [x.as_dict() for x in self.outlets]\n        inlets = [x.as_dict() for x in self.inlets]\n\n        if len(self.outlets) > 0:\n            self.xcom_push(context,\n                           key=PIPELINE_OUTLETS,\n                           value=outlets,\n                           execution_date=context['ti'].execution_date)\n\n        if len(self.inlets) > 0:\n            self.xcom_push(context,\n                           key=PIPELINE_INLETS,\n                           value=inlets,\n                           execution_date=context['ti'].execution_date)\n\n        if backend:\n            backend.send_lineage(operator=self, inlets=self.inlets,\n                                 outlets=self.outlets, context=context)\n\n        return ret_val", "suffix": "    return wrapper", "gt": ""}
{"prefix": "def prepare_lineage(func):\n    \"\"\"\n    Prepares the lineage inlets and outlets. Inlets can be:\n\n    * \"auto\" -> picks up any outlets from direct upstream tasks that have outlets defined, as such that\n      if A -> B -> C and B does not have outlets but A does, these are provided as inlets.\n    * \"list of task_ids\" -> picks up outlets from the upstream task_ids\n    * \"list of datasets\" -> manually defined list of DataSet\n\n    \"\"\"\n    @wraps(func)\n    def wrapper(self, context, *args, **kwargs):\n        self.log.debug(\"Preparing lineage inlets and outlets\")\n\n        task_ids = set(self._inlets['task_ids']).intersection(\n            self.get_flat_relative_ids(upstream=True)\n        )\n        if task_ids:\n            inlets = self.xcom_pull(context,\n                                    task_ids=task_ids,\n                                    dag_id=self.dag_id,\n                                    key=PIPELINE_OUTLETS)\n            inlets = [item for sublist in inlets if sublist for item in sublist]\n            inlets = [DataSet.map_type(i['typeName'])(data=i['attributes'])\n                      for i in inlets]\n            self.inlets.extend(inlets)\n\n        if self._inlets['auto']:\n            # dont append twice\n            task_ids = set(self._inlets['task_ids']).symmetric_difference(\n                self.upstream_task_ids\n            )\n            inlets = self.xcom_pull(context,\n                                    task_ids=task_ids,\n                                    dag_id=self.dag_id,\n                                    key=PIPELINE_OUTLETS)\n            inlets = [item for sublist in inlets if sublist for item in sublist]\n            inlets = [DataSet.map_type(i['typeName'])(data=i['attributes'])\n                      for i in inlets]\n            self.inlets.extend(inlets)\n\n        if len(self._inlets['datasets']) > 0:\n            self.inlets.extend(self._inlets['datasets'])\n\n        # outlets\n        if len(self._outlets['datasets']) > 0:\n            self.outlets.extend(self._outlets['datasets'])", "suffix": "        self.log.debug(\"inlets: %s, outlets: %s\", self.inlets, self.outlets)\n\n        for dataset in chain(self.inlets, self.outlets):\n            dataset.set_context(context)\n\n        return func(self, context, *args, **kwargs)\n\n    return wrapper", "gt": ""}
{"prefix": "def extra_dejson(self):\n        \"\"\"Returns the extra property by deserializing json.\"\"\"\n        obj = {}\n        if self.extra:\n            try:\n                obj = json.loads(self.extra)", "suffix": "                self.log.exception(e)\n                self.log.error(\"Failed parsing the json for conn_id %s\", self.conn_id)\n\n        return obj", "gt": "            except Exception as e:"}
{"prefix": "def date_range(start_date, end_date=None, num=None, delta=None):\n    \"\"\"\n    Get a set of dates as a list based on a start, end and delta, delta\n    can be something that can be added to `datetime.datetime`\n    or a cron expression as a `str`\n\n    :Example::\n\n        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta=timedelta(1))\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),\n            datetime.datetime(2016, 1, 3, 0, 0)]\n        date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta='0 0 * * *')\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),\n            datetime.datetime(2016, 1, 3, 0, 0)]\n        date_range(datetime(2016, 1, 1), datetime(2016, 3, 3), delta=\"0 0 0 * *\")\n            [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 2, 1, 0, 0),\n            datetime.datetime(2016, 3, 1, 0, 0)]\n\n    :param start_date: anchor date to start the series from\n    :type start_date: datetime.datetime\n    :param end_date: right boundary for the date range\n    :type end_date: datetime.datetime\n    :param num: alternatively to end_date, you can specify the number of\n        number of entries you want in the range. This number can be negative,\n        output will always be sorted regardless\n    :type num: int\n    \"\"\"\n    if not delta:\n        return []\n    if end_date and start_date > end_date:\n        raise Exception(\"Wait. start_date needs to be before end_date\")\n    if end_date and num:\n        raise Exception(\"Wait. Either specify end_date OR num\")\n    if not end_date and not num:", "suffix": "\n    delta_iscron = False\n    tz = start_date.tzinfo\n    if isinstance(delta, six.string_types):\n        delta_iscron = True\n        start_date = timezone.make_naive(start_date, tz)\n        cron = croniter(delta, start_date)\n    elif isinstance(delta, timedelta):\n        delta = abs(delta)\n    dates = []\n    if end_date:\n        if timezone.is_naive(start_date):\n            end_date = timezone.make_naive(end_date, tz)\n        while start_date <= end_date:\n            if timezone.is_naive(start_date):\n                dates.append(timezone.make_aware(start_date, tz))\n            else:\n                dates.append(start_date)\n\n            if delta_iscron:\n                start_date = cron.get_next(datetime)\n            else:\n                start_date += delta\n    else:\n        for _ in range(abs(num)):\n            if timezone.is_naive(start_date):\n                dates.append(timezone.make_aware(start_date, tz))\n            else:\n                dates.append(start_date)\n\n            if delta_iscron:\n                if num > 0:\n                    start_date = cron.get_next(datetime)\n                else:\n                    start_date = cron.get_prev(datetime)\n            else:\n                if num > 0:\n                    start_date += delta\n                else:\n                    start_date -= delta\n    return sorted(dates)", "gt": "        end_date = timezone.utcnow()"}
{"prefix": "def round_time(dt, delta, start_date=timezone.make_aware(datetime.min)):\n    \"\"\"\n    Returns the datetime of the form start_date + i * delta\n    which is closest to dt for any non-negative integer i.\n    Note that delta may be a datetime.timedelta or a dateutil.relativedelta\n    >>> round_time(datetime(2015, 1, 1, 6), timedelta(days=1))\n    datetime.datetime(2015, 1, 1, 0, 0)\n    >>> round_time(datetime(2015, 1, 2), relativedelta(months=1))\n    datetime.datetime(2015, 1, 1, 0, 0)\n    >>> round_time(datetime(2015, 9, 16, 0, 0), timedelta(1), datetime(2015, 9, 14, 0, 0))\n    datetime.datetime(2015, 9, 16, 0, 0)\n    >>> round_time(datetime(2015, 9, 15, 0, 0), timedelta(1), datetime(2015, 9, 14, 0, 0))\n    datetime.datetime(2015, 9, 15, 0, 0)\n    >>> round_time(datetime(2015, 9, 14, 0, 0), timedelta(1), datetime(2015, 9, 14, 0, 0))\n    datetime.datetime(2015, 9, 14, 0, 0)\n    >>> round_time(datetime(2015, 9, 13, 0, 0), timedelta(1), datetime(2015, 9, 14, 0, 0))\n    datetime.datetime(2015, 9, 14, 0, 0)\n    \"\"\"\n\n    if isinstance(delta, six.string_types):\n        # It's cron based, so it's easy\n        tz = start_date.tzinfo\n        start_date = timezone.make_naive(start_date, tz)\n        cron = croniter(delta, start_date)\n        prev = cron.get_prev(datetime)\n        if prev == start_date:\n            return timezone.make_aware(start_date, tz)\n        else:\n            return timezone.make_aware(prev, tz)\n\n    # Ignore the microseconds of dt\n    dt -= timedelta(microseconds=dt.microsecond)\n\n    # We are looking for a datetime in the form start_date + i * delta\n    # which is as close as possible to dt. Since delta could be a relative\n    # delta we don't know its exact length in seconds so we cannot rely on\n    # division to find i. Instead we employ a binary search algorithm, first\n    # finding an upper and lower limit and then disecting the interval until\n    # we have found the closest match.\n\n    # We first search an upper limit for i for which start_date + upper * delta\n    # exceeds dt.\n    upper = 1\n    while start_date + upper * delta < dt:\n        # To speed up finding an upper limit we grow this exponentially by a\n        # factor of 2\n        upper *= 2\n\n    # Since upper is the first value for which start_date + upper * delta\n    # exceeds dt, upper // 2 is below dt and therefore forms a lower limited\n    # for the i we are looking for\n    lower = upper // 2\n\n    # We now continue to intersect the interval between\n    # start_date + lower * delta and start_date + upper * delta\n    # until we find the closest value\n    while True:\n        # Invariant: start + lower * delta < dt <= start + upper * delta\n        # If start_date + (lower + 1)*delta exceeds dt, then either lower or\n        # lower+1 has to be the solution we are searching for\n        if start_date + (lower + 1) * delta >= dt:\n            # Check if start_date + (lower + 1)*delta or", "suffix": "            if (start_date + (lower + 1) * delta) - dt <= dt - (start_date + lower * delta):\n                return start_date + (lower + 1) * delta\n            else:\n                return start_date + lower * delta\n\n        # We intersect the interval and either replace the lower or upper\n        # limit with the candidate\n        candidate = lower + (upper - lower) // 2\n        if start_date + candidate * delta >= dt:\n            upper = candidate\n        else:\n            lower = candidate", "gt": "            # start_date + lower*delta is closer to dt and return the solution"}
{"prefix": "def infer_time_unit(time_seconds_arr):\n    \"\"\"\n    Determine the most appropriate time unit for an array of time durations\n    specified in seconds.\n    e.g. 5400 seconds => 'minutes', 36000 seconds => 'hours'\n    \"\"\"", "suffix": "        return 'hours'\n    max_time_seconds = max(time_seconds_arr)\n    if max_time_seconds <= 60 * 2:\n        return 'seconds'\n    elif max_time_seconds <= 60 * 60 * 2:\n        return 'minutes'\n    elif max_time_seconds <= 24 * 60 * 60 * 2:\n        return 'hours'\n    else:\n        return 'days'", "gt": "    if len(time_seconds_arr) == 0:"}
{"prefix": "def scale_time_units(time_seconds_arr, unit):\n    \"\"\"\n    Convert an array of time durations in seconds to the specified time unit.\n    \"\"\"\n    if unit == 'minutes':\n        return list(map(lambda x: x * 1.0 / 60, time_seconds_arr))\n    elif unit == 'hours':\n        return list(map(lambda x: x * 1.0 / (60 * 60), time_seconds_arr))\n    elif unit == 'days':", "suffix": "    return time_seconds_arr", "gt": "        return list(map(lambda x: x * 1.0 / (24 * 60 * 60), time_seconds_arr))"}
{"prefix": "", "suffix": "    \"\"\"\n    Get a datetime object representing `n` days ago. By default the time is\n    set to midnight.\n    \"\"\"\n    today = timezone.utcnow().replace(\n        hour=hour,\n        minute=minute,\n        second=second,\n        microsecond=microsecond)\n    return today - timedelta(days=n)", "gt": "def days_ago(n, hour=0, minute=0, second=0, microsecond=0):"}
{"prefix": "def get_dag_runs(dag_id, state=None):\n    \"\"\"\n    Returns a list of Dag Runs for a specific DAG ID.\n    :param dag_id: String identifier of a DAG\n    :param state: queued|running|success...\n    :return: List of DAG runs of a DAG with requested state,\n    or all runs if the state is not specified\n    \"\"\"\n    dagbag = DagBag()\n\n    # Check DAG exists.\n    if dag_id not in dagbag.dags:\n        error_message = \"Dag id {} not found\".format(dag_id)\n        raise AirflowException(error_message)\n\n    dag_runs = list()\n    state = state.lower() if state else None\n    for run in DagRun.find(dag_id=dag_id, state=state):\n        dag_runs.append({\n            'id': run.id,\n            'run_id': run.run_id,\n            'state': run.state,\n            'dag_id': run.dag_id,\n            'execution_date': run.execution_date.isoformat(),\n            'start_date': ((run.start_date or '') and\n                           run.start_date.isoformat()),\n            'dag_run_url': url_for('Airflow.graph', dag_id=run.dag_id,", "suffix": "        })\n\n    return dag_runs", "gt": "                                   execution_date=run.execution_date)"}
{"prefix": "def init_role(self, role_name, role_vms, role_perms):\n        \"\"\"\n        Initialize the role with the permissions and related view-menus.\n\n        :param role_name:\n        :param role_vms:\n        :param role_perms:", "suffix": "        \"\"\"\n        pvms = self.get_session.query(sqla_models.PermissionView).all()\n        pvms = [p for p in pvms if p.permission and p.view_menu]\n\n        role = self.find_role(role_name)\n        if not role:\n            role = self.add_role(role_name)\n\n        if len(role.permissions) == 0:\n            self.log.info('Initializing permissions for role:%s in the database.', role_name)\n            role_pvms = set()\n            for pvm in pvms:\n                if pvm.view_menu.name in role_vms and pvm.permission.name in role_perms:\n                    role_pvms.add(pvm)\n            role.permissions = list(role_pvms)\n            self.get_session.merge(role)\n            self.get_session.commit()\n        else:\n            self.log.debug('Existing permissions for the role:%s '\n                           'within the database will persist.', role_name)", "gt": "        :return:"}
{"prefix": "", "suffix": "        \"\"\"Delete the given Role\n\n        :param role_name: the name of a role in the ab_role table\n        \"\"\"\n        session = self.get_session\n        role = session.query(sqla_models.Role)\\\n                      .filter(sqla_models.Role.name == role_name)\\\n                      .first()\n        if role:\n            self.log.info(\"Deleting role '%s'\", role_name)\n            session.delete(role)\n            session.commit()\n        else:\n            raise AirflowException(\"Role named '{}' does not exist\".format(\n                role_name))", "gt": "def delete_role(self, role_name):"}
{"prefix": "def get_user_roles(self, user=None):\n        \"\"\"\n        Get all the roles associated with the user.\n", "suffix": "        :return: a list of roles associated with the user.\n        \"\"\"\n        if user is None:\n            user = g.user\n        if user.is_anonymous:\n            public_role = appbuilder.config.get('AUTH_ROLE_PUBLIC')\n            return [appbuilder.security_manager.find_role(public_role)] \\\n                if public_role else []\n        return user.roles", "gt": "        :param user: the ab_user in FAB model."}
{"prefix": "def get_all_permissions_views(self):\n        \"\"\"\n        Returns a set of tuples with the perm name and view menu name\n        \"\"\"\n        perms_views = set()\n        for role in self.get_user_roles():", "suffix": "                                for perm_view in role.permissions})\n        return perms_views", "gt": "            perms_views.update({(perm_view.permission.name, perm_view.view_menu.name)"}
{"prefix": "def get_accessible_dag_ids(self, username=None):\n        \"\"\"\n        Return a set of dags that user has access to(either read or write).\n\n        :param username: Name of the user.\n        :return: A set of dag ids that the user could access.\n        \"\"\"\n        if not username:\n            username = g.user\n\n        if username.is_anonymous or 'Public' in username.roles:\n            # return an empty set if the role is public\n            return set()\n\n        roles = {role.name for role in username.roles}\n        if {'Admin', 'Viewer', 'User', 'Op'} & roles:", "suffix": "\n        user_perms_views = self.get_all_permissions_views()\n        # return a set of all dags that the user could access\n        return set([view for perm, view in user_perms_views if perm in self.DAG_PERMS])", "gt": "            return self.DAG_VMS"}
{"prefix": "def has_access(self, permission, view_name, user=None):\n        \"\"\"\n        Verify whether a given user could perform certain permission\n        (e.g can_read, can_write) on the given dag_id.\n\n        :param permission: permission on dag_id(e.g can_read, can_edit).\n        :type permission: str", "suffix": "        :type view_name: str\n        :param user: user name\n        :type user: str\n        :return: a bool whether user could perform certain permission on the dag_id.\n        :rtype bool\n        \"\"\"\n        if not user:\n            user = g.user\n        if user.is_anonymous:\n            return self.is_item_public(permission, view_name)\n        return self._has_view_access(user, permission, view_name)", "gt": "        :param view_name: name of view-menu(e.g dag id is a view-menu as well)."}
{"prefix": "def _has_role(self, role_name_or_list):", "suffix": "        Whether the user has this role name\n        \"\"\"\n        if not isinstance(role_name_or_list, list):\n            role_name_or_list = [role_name_or_list]\n        return any(\n            [r.name in role_name_or_list for r in self.get_user_roles()])", "gt": "        \"\"\""}
{"prefix": "def _has_perm(self, permission_name, view_menu_name):\n        \"\"\"\n        Whether the user has this perm", "suffix": "        if hasattr(self, 'perms'):\n            if (permission_name, view_menu_name) in self.perms:\n                return True\n        # rebuild the permissions set\n        self._get_and_cache_perms()\n        return (permission_name, view_menu_name) in self.perms", "gt": "        \"\"\""}
{"prefix": "def clean_perms(self):\n        \"\"\"\n        FAB leaves faulty permissions that need to be cleaned up\n        \"\"\"", "suffix": "        sesh = self.get_session\n        pvms = (\n            sesh.query(sqla_models.PermissionView)\n            .filter(or_(\n                sqla_models.PermissionView.permission == None,  # NOQA\n                sqla_models.PermissionView.view_menu == None,  # NOQA\n            ))\n        )\n        deleted_count = pvms.delete()\n        sesh.commit()\n        if deleted_count:\n            self.log.info('Deleted %s faulty permissions', deleted_count)", "gt": "        self.log.debug('Cleaning faulty perms')"}
{"prefix": "def _merge_perm(self, permission_name, view_menu_name):\n        \"\"\"\n        Add the new permission , view_menu to ab_permission_view_role if not exists.\n        It will add the related entry to ab_permission\n        and ab_view_menu two meta tables as well.\n\n        :param permission_name: Name of the permission.", "suffix": "        :param view_menu_name: Name of the view-menu\n        :type view_menu_name: str\n        :return:\n        \"\"\"\n        permission = self.find_permission(permission_name)\n        view_menu = self.find_view_menu(view_menu_name)\n        pv = None\n        if permission and view_menu:\n            pv = self.get_session.query(self.permissionview_model).filter_by(\n                permission=permission, view_menu=view_menu).first()\n        if not pv and permission_name and view_menu_name:\n            self.add_permission_view_menu(permission_name, view_menu_name)", "gt": "        :type permission_name: str"}
{"prefix": "def create_custom_dag_permission_view(self, session=None):\n        \"\"\"\n        Workflow:\n        1. Fetch all the existing (permissions, view-menu) from Airflow DB.\n        2. Fetch all the existing dag models that are either active or paused. Exclude the subdags.\n        3. Create both read and write permission view-menus relation for every dags from step 2\n        4. Find out all the dag specific roles(excluded pubic, admin, viewer, op, user)\n        5. Get all the permission-vm owned by the user role.\n        6. Grant all the user role's permission-vm except the all-dag view-menus to the dag roles.\n        7. Commit the updated permission-vm-role into db\n\n        :return: None.\n        \"\"\"\n        self.log.debug('Fetching a set of all permission, view_menu from FAB meta-table')\n\n        def merge_pv(perm, view_menu):\n            \"\"\"Create permission view menu only if it doesn't exist\"\"\"\n            if view_menu and perm and (view_menu, perm) not in all_pvs:\n                self._merge_perm(perm, view_menu)\n\n        all_pvs = set()\n        for pv in self.get_session.query(self.permissionview_model).all():\n            if pv.permission and pv.view_menu:\n                all_pvs.add((pv.permission.name, pv.view_menu.name))\n\n        # Get all the active / paused dags and insert them into a set\n        all_dags_models = session.query(models.DagModel)\\\n            .filter(or_(models.DagModel.is_active, models.DagModel.is_paused))\\\n            .filter(~models.DagModel.is_subdag).all()\n\n        # create can_dag_edit and can_dag_read permissions for every dag(vm)\n        for dag in all_dags_models:\n            for perm in self.DAG_PERMS:\n                merge_pv(perm, dag.dag_id)\n\n        # for all the dag-level role, add the permission of viewer\n        # with the dag view to ab_permission_view\n        all_roles = self.get_all_roles()\n        user_role = self.find_role('User')", "suffix": "        dag_role = [role for role in all_roles if role.name not in EXISTING_ROLES]\n        update_perm_views = []\n\n        # need to remove all_dag vm from all the existing view-menus\n        dag_vm = self.find_view_menu('all_dags')\n        ab_perm_view_role = sqla_models.assoc_permissionview_role\n        perm_view = self.permissionview_model\n        view_menu = self.viewmenu_model\n\n        all_perm_view_by_user = session.query(ab_perm_view_role)\\\n            .join(perm_view, perm_view.id == ab_perm_view_role\n                  .columns.permission_view_id)\\\n            .filter(ab_perm_view_role.columns.role_id == user_role.id)\\\n            .join(view_menu)\\\n            .filter(perm_view.view_menu_id != dag_vm.id)\n        all_perm_views = set([role.permission_view_id for role in all_perm_view_by_user])\n\n        for role in dag_role:\n            # Get all the perm-view of the role\n            existing_perm_view_by_user = self.get_session.query(ab_perm_view_role)\\\n                .filter(ab_perm_view_role.columns.role_id == role.id)\n\n            existing_perms_views = set([pv.permission_view_id\n                                        for pv in existing_perm_view_by_user])\n            missing_perm_views = all_perm_views - existing_perms_views\n\n            for perm_view_id in missing_perm_views:\n                update_perm_views.append({'permission_view_id': perm_view_id,\n                                          'role_id': role.id})\n\n        if update_perm_views:\n            self.get_session.execute(ab_perm_view_role.insert(), update_perm_views)\n        self.get_session.commit()", "gt": ""}
{"prefix": "", "suffix": "        \"\"\"\n        Admin should have all the permission-views.\n        Add the missing ones to the table for admin.\n\n        :return: None.\n        \"\"\"\n        pvms = self.get_session.query(sqla_models.PermissionView).all()\n        pvms = [p for p in pvms if p.permission and p.view_menu]\n\n        admin = self.find_role('Admin')\n        admin.permissions = list(set(admin.permissions) | set(pvms))\n\n        self.get_session.commit()", "gt": "def update_admin_perm_view(self):"}
{"prefix": "def sync_roles(self):\n        \"\"\"\n        1. Init the default role(Admin, Viewer, User, Op, public)\n           with related permissions.\n        2. Init the custom role(dag-user) with related permissions.\n\n        :return: None.\n        \"\"\"\n        self.log.debug('Start syncing user roles.')\n        # Create global all-dag VM\n        self.create_perm_vm_for_all_dag()\n\n        # Create default user role.\n        for config in self.ROLE_CONFIGS:\n            role = config['role']\n            vms = config['vms']", "suffix": "            self.init_role(role, vms, perms)\n        self.create_custom_dag_permission_view()\n\n        # init existing roles, the rest role could be created through UI.\n        self.update_admin_perm_view()\n        self.clean_perms()", "gt": "            perms = config['perms']"}
{"prefix": "def sync_perm_for_dag(self, dag_id, access_control=None):\n        \"\"\"\n        Sync permissions for given dag id. The dag id surely exists in our dag bag\n        as only / refresh button or cli.sync_perm will call this function\n\n        :param dag_id: the ID of the DAG whose permissions should be updated", "suffix": "        :param access_control: a dict where each key is a rolename and\n            each value is a set() of permission names (e.g.,\n            {'can_dag_read'}\n        :type access_control: dict\n        :return:\n        \"\"\"\n        for dag_perm in self.DAG_PERMS:\n            perm_on_dag = self.find_permission_view_menu(dag_perm, dag_id)\n            if perm_on_dag is None:\n                self.add_permission_view_menu(dag_perm, dag_id)\n\n        if access_control:\n            self._sync_dag_view_permissions(dag_id, access_control)", "gt": "        :type dag_id: string"}
{"prefix": "def _sync_dag_view_permissions(self, dag_id, access_control):\n        \"\"\"Set the access policy on the given DAG's ViewModel.\n\n        :param dag_id: the ID of the DAG whose permissions should be updated\n        :type dag_id: string\n        :param access_control: a dict where each key is a rolename and\n            each value is a set() of permission names (e.g.,\n            {'can_dag_read'}\n        :type access_control: dict\n        \"\"\"\n        def _get_or_create_dag_permission(perm_name):\n            dag_perm = self.find_permission_view_menu(perm_name, dag_id)\n            if not dag_perm:\n                self.log.info(\n                    \"Creating new permission '%s' on view '%s'\",\n                    perm_name, dag_id\n                )\n                dag_perm = self.add_permission_view_menu(perm_name, dag_id)\n\n            return dag_perm\n\n        def _revoke_stale_permissions(dag_view):\n            existing_dag_perms = self.find_permissions_view_menu(dag_view)\n            for perm in existing_dag_perms:\n                non_admin_roles = [role for role in perm.role\n                                   if role.name != 'Admin']\n                for role in non_admin_roles:\n                    target_perms_for_role = access_control.get(role.name, {})\n                    if perm.permission.name not in target_perms_for_role:\n                        self.log.info(\n                            \"Revoking '%s' on DAG '%s' for role '%s'\",\n                            perm.permission, dag_id, role.name\n                        )\n                        self.del_permission_role(role, perm)\n\n        dag_view = self.find_view_menu(dag_id)\n        if dag_view:\n            _revoke_stale_permissions(dag_view)\n\n        for rolename, perms in access_control.items():\n            role = self.find_role(rolename)\n            if not role:\n                raise AirflowException(\n                    \"The access_control mapping for DAG '{}' includes a role \"\n                    \"named '{}', but that role does not exist\".format(\n                        dag_id,\n                        rolename))\n", "suffix": "            invalid_perms = perms - self.DAG_PERMS\n            if invalid_perms:\n                raise AirflowException(\n                    \"The access_control map for DAG '{}' includes the following \"\n                    \"invalid permissions: {}; The set of valid permissions \"\n                    \"is: {}\".format(dag_id,\n                                    (perms - self.DAG_PERMS),\n                                    self.DAG_PERMS))\n\n            for perm_name in perms:\n                dag_perm = _get_or_create_dag_permission(perm_name)\n                self.add_permission_role(role, dag_perm)", "gt": "            perms = set(perms)"}
{"prefix": "", "suffix": "        \"\"\"\n        Create perm-vm if not exist and insert into FAB security model for all-dags.\n        \"\"\"\n        # create perm for global logical dag\n        for dag_vm in self.DAG_VMS:\n            for perm in self.DAG_PERMS:\n                self._merge_perm(permission_name=perm,\n                                 view_menu_name=dag_vm)", "gt": "def create_perm_vm_for_all_dag(self):"}
{"prefix": "def get_fernet():\n    \"\"\"\n    Deferred load of Fernet key.\n\n    This function could fail either because Cryptography is not installed\n    or because the Fernet key is invalid.\n\n    :return: Fernet object\n    :raises: airflow.exceptions.AirflowException if there's a problem trying to load Fernet\n    \"\"\"", "suffix": "    log = LoggingMixin().log\n\n    if _fernet:\n        return _fernet\n    try:\n        from cryptography.fernet import Fernet, MultiFernet, InvalidToken\n        global InvalidFernetToken\n        InvalidFernetToken = InvalidToken\n\n    except BuiltinImportError:\n        log.warning(\n            \"cryptography not found - values will not be stored encrypted.\"\n        )\n        _fernet = NullFernet()\n        return _fernet\n\n    try:\n        fernet_key = configuration.conf.get('core', 'FERNET_KEY')\n        if not fernet_key:\n            log.warning(\n                \"empty cryptography key - values will not be stored encrypted.\"\n            )\n            _fernet = NullFernet()\n        else:\n            _fernet = MultiFernet([\n                Fernet(fernet_part.encode('utf-8'))\n                for fernet_part in fernet_key.split(',')\n            ])\n            _fernet.is_encrypted = True\n    except (ValueError, TypeError) as ve:\n        raise AirflowException(\"Could not create Fernet object: {}\".format(ve))\n\n    return _fernet", "gt": "    global _fernet"}
{"prefix": "def poke(self, context):\n        \"\"\"\n        Checks for existence of the partition in the AWS Glue Catalog table\n        \"\"\"\n        if '.' in self.table_name:\n            self.database_name, self.table_name = self.table_name.split('.')\n        self.log.info(\n            'Poking for table %s. %s, expression %s', self.database_name, self.table_name, self.expression", "suffix": "\n        return self.get_hook().check_for_partition(\n            self.database_name, self.table_name, self.expression)", "gt": "        )"}
{"prefix": "def get_hook(self):", "suffix": "        Gets the AwsGlueCatalogHook\n        \"\"\"\n        if not hasattr(self, 'hook'):\n            from airflow.contrib.hooks.aws_glue_catalog_hook import AwsGlueCatalogHook\n            self.hook = AwsGlueCatalogHook(\n                aws_conn_id=self.aws_conn_id,\n                region_name=self.region_name)\n\n        return self.hook", "gt": "        \"\"\""}
{"prefix": "def poke(self, context):\n        \"\"\"\n        Check for message on subscribed queue and write to xcom the message with key ``messages``\n\n        :param context: the context object\n        :type context: dict\n        :return: ``True`` if message is available or ``False``\n        \"\"\"\n\n        sqs_hook = SQSHook(aws_conn_id=self.aws_conn_id)\n        sqs_conn = sqs_hook.get_conn()\n\n        self.log.info('SQSSensor checking for message on queue: %s', self.sqs_queue)\n\n        messages = sqs_conn.receive_message(QueueUrl=self.sqs_queue,\n                                            MaxNumberOfMessages=self.max_messages,\n                                            WaitTimeSeconds=self.wait_time_seconds)\n\n        self.log.info(\"reveived message %s\", str(messages))\n\n        if 'Messages' in messages and len(messages['Messages']) > 0:\n\n            entries = [{'Id': message['MessageId'], 'ReceiptHandle': message['ReceiptHandle']}\n                       for message in messages['Messages']]\n\n            result = sqs_conn.delete_message_batch(QueueUrl=self.sqs_queue,\n                                                   Entries=entries)\n", "suffix": "                context['ti'].xcom_push(key='messages', value=messages)\n                return True\n            else:\n                raise AirflowException(\n                    'Delete SQS Messages failed ' + str(result) + ' for messages ' + str(messages))\n\n        return False", "gt": "            if 'Successful' in result:"}
{"prefix": "def tmp_configuration_copy(chmod=0o600):\n    \"\"\"\n    Returns a path for a temporary file including a full copy of the configuration\n    settings.\n    :return: a path to a temporary file\n    \"\"\"\n    cfg_dict = conf.as_dict(display_sensitive=True, raw=True)\n    temp_fd, cfg_path = mkstemp()\n\n    with os.fdopen(temp_fd, 'w') as temp_file:\n        if chmod is not None:\n            os.fchmod(temp_fd, chmod)\n        json.dump(cfg_dict, temp_file)\n", "suffix": "", "gt": "    return cfg_path"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Returns a snakebite HDFSClient object.\n        \"\"\"", "suffix": "        # take the first.\n        effective_user = self.proxy_user\n        autoconfig = self.autoconfig\n        use_sasl = configuration.conf.get('core', 'security') == 'kerberos'\n\n        try:\n            connections = self.get_connections(self.hdfs_conn_id)\n\n            if not effective_user:\n                effective_user = connections[0].login\n            if not autoconfig:\n                autoconfig = connections[0].extra_dejson.get('autoconfig',\n                                                             False)\n            hdfs_namenode_principal = connections[0].extra_dejson.get(\n                'hdfs_namenode_principal')\n        except AirflowException:\n            if not autoconfig:\n                raise\n\n        if autoconfig:\n            # will read config info from $HADOOP_HOME conf files\n            client = AutoConfigClient(effective_user=effective_user,\n                                      use_sasl=use_sasl)\n        elif len(connections) == 1:\n            client = Client(connections[0].host, connections[0].port,\n                            effective_user=effective_user, use_sasl=use_sasl,\n                            hdfs_namenode_principal=hdfs_namenode_principal)\n        elif len(connections) > 1:\n            nn = [Namenode(conn.host, conn.port) for conn in connections]\n            client = HAClient(nn, effective_user=effective_user,\n                              use_sasl=use_sasl,\n                              hdfs_namenode_principal=hdfs_namenode_principal)\n        else:\n            raise HDFSHookException(\"conn_id doesn't exist in the repository \"\n                                    \"and autoconfig is not specified\")\n\n        return client", "gt": "        # When using HAClient, proxy_user must be the same, so is ok to always"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Establishes a connection depending on the security mode set via config or environment variable.\n\n        :return: a hdfscli InsecureClient or KerberosClient object.\n        :rtype: hdfs.InsecureClient or hdfs.ext.kerberos.KerberosClient", "suffix": "        connections = self.get_connections(self.webhdfs_conn_id)\n\n        for connection in connections:\n            try:\n                self.log.debug('Trying namenode %s', connection.host)\n                client = self._get_client(connection)\n                client.status('/')\n                self.log.debug('Using namenode %s for hook', connection.host)\n                return client\n            except HdfsError as hdfs_error:\n                self.log.debug('Read operation on namenode %s failed with error: %s',\n                               connection.host, hdfs_error)\n\n        hosts = [connection.host for connection in connections]\n        error_message = 'Read operations failed on the namenodes below:\\n{hosts}'.format(\n            hosts='\\n'.join(hosts))\n        raise AirflowWebHDFSHookException(error_message)", "gt": "        \"\"\""}
{"prefix": "def check_for_path(self, hdfs_path):", "suffix": "        Check for the existence of a path in HDFS by querying FileStatus.\n\n        :param hdfs_path: The path to check.\n        :type hdfs_path: str\n        :return: True if the path exists and False if not.\n        :rtype: bool\n        \"\"\"\n        conn = self.get_conn()\n\n        status = conn.status(hdfs_path, strict=False)\n        return bool(status)", "gt": "        \"\"\""}
{"prefix": "def load_file(self, source, destination, overwrite=True, parallelism=1, **kwargs):\n        r\"\"\"\n        Uploads a file to HDFS.\n\n        :param source: Local path to file or folder.\n            If it's a folder, all the files inside of it will be uploaded.\n            .. note:: This implies that folders empty of files will not be created remotely.\n\n        :type source: str\n        :param destination: PTarget HDFS path.\n            If it already exists and is a directory, files will be uploaded inside.\n        :type destination: str\n        :param overwrite: Overwrite any existing file or directory.", "suffix": "        :param parallelism: Number of threads to use for parallelization.\n            A value of `0` (or negative) uses as many threads as there are files.\n        :type parallelism: int\n        :param \\**kwargs: Keyword arguments forwarded to :meth:`hdfs.client.Client.upload`.\n        \"\"\"\n        conn = self.get_conn()\n\n        conn.upload(hdfs_path=destination,\n                    local_path=source,\n                    overwrite=overwrite,\n                    n_threads=parallelism,\n                    **kwargs)\n        self.log.debug(\"Uploaded file %s to %s\", source, destination)", "gt": "        :type overwrite: bool"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Establish a connection to pinot broker through pinot dbqpi.\n        \"\"\"\n        conn = self.get_connection(self.pinot_broker_conn_id)\n        pinot_broker_conn = connect(\n            host=conn.host,", "suffix": "            path=conn.extra_dejson.get('endpoint', '/pql'),\n            scheme=conn.extra_dejson.get('schema', 'http')\n        )\n        self.log.info('Get the connection to pinot '\n                      'broker on {host}'.format(host=conn.host))\n        return pinot_broker_conn", "gt": "            port=conn.port,"}
{"prefix": "def get_uri(self):\n        \"\"\"\n        Get the connection uri for pinot broker.\n\n        e.g: http://localhost:9000/pql\n        \"\"\"\n        conn = self.get_connection(getattr(self, self.conn_name_attr))\n        host = conn.host\n        if conn.port is not None:\n            host += ':{port}'.format(port=conn.port)\n        conn_type = 'http' if not conn.conn_type else conn.conn_type", "suffix": "        return '{conn_type}://{host}/{endpoint}'.format(\n            conn_type=conn_type, host=host, endpoint=endpoint)", "gt": "        endpoint = conn.extra_dejson.get('endpoint', 'pql')"}
{"prefix": "def get_records(self, sql):\n        \"\"\"\n        Executes the sql and returns a set of records.\n\n        :param sql: the sql statement to be executed (str) or a list of", "suffix": "        :type sql: str\n        \"\"\"\n        with self.get_conn() as cur:\n            cur.execute(sql)\n            return cur.fetchall()", "gt": "            sql statements to execute"}
{"prefix": "", "suffix": "        \"\"\"\n        Executes the sql and returns the first resulting row.\n\n        :param sql: the sql statement to be executed (str) or a list of\n            sql statements to execute\n        :type sql: str or list\n        \"\"\"\n        with self.get_conn() as cur:\n            cur.execute(sql)\n            return cur.fetchone()", "gt": "def get_first(self, sql):"}
{"prefix": "def smart_truncate(string, max_length=0, word_boundary=False, separator=' ', save_order=False):\n    \"\"\"\n    Truncate a string.\n    :param string (str): string for modification\n    :param max_length (int): output string length\n    :param word_boundary (bool):\n    :param save_order (bool): if True then word order of output string is like input string\n    :param separator (str): separator between words\n    :return:\n    \"\"\"\n\n    string = string.strip(separator)\n\n    if not max_length:\n        return string\n\n    if len(string) < max_length:\n        return string\n\n    if not word_boundary:\n        return string[:max_length].strip(separator)\n", "suffix": "        return string[:max_length]\n\n    truncated = ''\n    for word in string.split(separator):\n        if word:\n            next_len = len(truncated) + len(word)\n            if next_len < max_length:\n                truncated += '{0}{1}'.format(word, separator)\n            elif next_len == max_length:\n                truncated += '{0}'.format(word)\n                break\n            else:\n                if save_order:\n                    break\n    if not truncated: # pragma: no cover\n        truncated = string[:max_length]\n    return truncated.strip(separator)", "gt": "    if separator not in string:"}
{"prefix": "def slugify(text, entities=True, decimal=True, hexadecimal=True, max_length=0, word_boundary=False,\n            separator=DEFAULT_SEPARATOR, save_order=False, stopwords=(), regex_pattern=None, lowercase=True,\n            replacements=()):\n    \"\"\"\n    Make a slug from the given text.\n    :param text (str): initial text\n    :param entities (bool):\n    :param decimal (bool):\n    :param hexadecimal (bool):\n    :param max_length (int): output string length\n    :param word_boundary (bool):\n    :param save_order (bool): if parameter is True and max_length > 0 return whole words in the initial order\n    :param separator (str): separator between words\n    :param stopwords (iterable): words to discount\n    :param regex_pattern (str): regex pattern for allowed characters\n    :param lowercase (bool): activate case sensitivity by setting it to False\n    :param replacements (iterable): list of replacement rules e.g. [['|', 'or'], ['%', 'percent']]\n    :return (str):\n    \"\"\"\n\n    # user-specific replacements\n    if replacements:\n        for old, new in replacements:\n            text = text.replace(old, new)\n\n    # ensure text is unicode\n    if not isinstance(text, _unicode_type):\n        text = _unicode(text, 'utf-8', 'ignore')\n\n    # replace quotes with dashes - pre-process\n    text = QUOTE_PATTERN.sub(DEFAULT_SEPARATOR, text)\n\n    # decode unicode\n    text = unidecode.unidecode(text)\n\n    # ensure text is still in unicode\n    if not isinstance(text, _unicode_type):\n        text = _unicode(text, 'utf-8', 'ignore')\n\n    # character entity reference\n    if entities:\n        text = CHAR_ENTITY_PATTERN.sub(lambda m: unichr(name2codepoint[m.group(1)]), text)\n\n    # decimal character reference\n    if decimal:\n        try:\n            text = DECIMAL_PATTERN.sub(lambda m: unichr(int(m.group(1))), text)\n        except Exception:\n            pass\n\n    # hexadecimal character reference\n    if hexadecimal:\n        try:\n            text = HEX_PATTERN.sub(lambda m: unichr(int(m.group(1), 16)), text)\n        except Exception:\n            pass\n\n    # translate\n    text = unicodedata.normalize('NFKD', text)\n\n    # make the text lowercase (optional)\n    if lowercase:\n        text = text.lower()", "suffix": "    # remove generated quotes -- post-process\n    text = QUOTE_PATTERN.sub('', text)\n\n    # cleanup numbers\n    text = NUMBERS_PATTERN.sub('', text)\n\n    # replace all other unwanted characters\n    if lowercase:\n        pattern = regex_pattern or ALLOWED_CHARS_PATTERN\n    else:\n        pattern = regex_pattern or ALLOWED_CHARS_PATTERN_WITH_UPPERCASE\n    text = re.sub(pattern, DEFAULT_SEPARATOR, text)\n\n    # remove redundant\n    text = DUPLICATE_DASH_PATTERN.sub(DEFAULT_SEPARATOR, text).strip(DEFAULT_SEPARATOR)\n\n    # remove stopwords\n    if stopwords:\n        if lowercase:\n            stopwords_lower = [s.lower() for s in stopwords]\n            words = [w for w in text.split(DEFAULT_SEPARATOR) if w not in stopwords_lower]\n        else:\n            words = [w for w in text.split(DEFAULT_SEPARATOR) if w not in stopwords]\n        text = DEFAULT_SEPARATOR.join(words)\n\n    # finalize user-specific replacements\n    if replacements:\n        for old, new in replacements:\n            text = text.replace(old, new)\n\n    # smart truncate if requested\n    if max_length > 0:\n        text = smart_truncate(text, max_length, word_boundary, DEFAULT_SEPARATOR, save_order)\n\n    if separator != DEFAULT_SEPARATOR:\n        text = text.replace(DEFAULT_SEPARATOR, separator)\n\n    return text", "gt": ""}
{"prefix": "def set(\n            cls,\n            key,\n            value,\n            execution_date,\n            task_id,\n            dag_id,\n            session=None):\n        \"\"\"\n        Store an XCom value.\n        TODO: \"pickling\" has been deprecated and JSON is preferred.\n        \"pickling\" will be removed in Airflow 2.0.\n\n        :return: None\n        \"\"\"\n        session.expunge_all()\n\n        enable_pickling = configuration.getboolean('core', 'enable_xcom_pickling')\n        if enable_pickling:\n            value = pickle.dumps(value)\n        else:\n            try:\n                value = json.dumps(value).encode('UTF-8')\n            except ValueError:\n                log = LoggingMixin().log\n                log.error(\"Could not serialize the XCOM value into JSON. \"\n                          \"If you are using pickles instead of JSON \"\n                          \"for XCOM, then you need to enable pickle \"\n                          \"support for XCOM in your airflow config.\")\n                raise\n\n        # remove any duplicate XComs\n        session.query(cls).filter(\n            cls.key == key,", "suffix": "            cls.task_id == task_id,\n            cls.dag_id == dag_id).delete()\n\n        session.commit()\n\n        # insert new XCom\n        session.add(XCom(\n            key=key,\n            value=value,\n            execution_date=execution_date,\n            task_id=task_id,\n            dag_id=dag_id))\n\n        session.commit()", "gt": "            cls.execution_date == execution_date,"}
{"prefix": "def get_one(cls,\n                execution_date,\n                key=None,\n                task_id=None,\n                dag_id=None,\n                include_prior_dates=False,\n                session=None):\n        \"\"\"\n        Retrieve an XCom value, optionally meeting certain criteria.\n        TODO: \"pickling\" has been deprecated and JSON is preferred.\n        \"pickling\" will be removed in Airflow 2.0.\n\n        :return: XCom value\n        \"\"\"\n        filters = []\n        if key:\n            filters.append(cls.key == key)\n        if task_id:\n            filters.append(cls.task_id == task_id)\n        if dag_id:\n            filters.append(cls.dag_id == dag_id)\n        if include_prior_dates:\n            filters.append(cls.execution_date <= execution_date)\n        else:\n            filters.append(cls.execution_date == execution_date)\n\n        query = (\n            session.query(cls.value).filter(and_(*filters))\n                   .order_by(cls.execution_date.desc(), cls.timestamp.desc()))\n\n        result = query.first()\n        if result:\n            enable_pickling = configuration.getboolean('core', 'enable_xcom_pickling')", "suffix": "                return pickle.loads(result.value)\n            else:\n                try:\n                    return json.loads(result.value.decode('UTF-8'))\n                except ValueError:\n                    log = LoggingMixin().log\n                    log.error(\"Could not deserialize the XCOM value from JSON. \"\n                              \"If you are using pickles instead of JSON \"\n                              \"for XCOM, then you need to enable pickle \"\n                              \"support for XCOM in your airflow config.\")\n                    raise", "gt": "            if enable_pickling:"}
{"prefix": "def get_many(cls,\n                 execution_date,\n                 key=None,\n                 task_ids=None,\n                 dag_ids=None,\n                 include_prior_dates=False,\n                 limit=100,\n                 session=None):\n        \"\"\"\n        Retrieve an XCom value, optionally meeting certain criteria\n        TODO: \"pickling\" has been deprecated and JSON is preferred.\n        \"pickling\" will be removed in Airflow 2.0.\n        \"\"\"\n        filters = []\n        if key:\n            filters.append(cls.key == key)\n        if task_ids:\n            filters.append(cls.task_id.in_(as_tuple(task_ids)))\n        if dag_ids:", "suffix": "        if include_prior_dates:\n            filters.append(cls.execution_date <= execution_date)\n        else:\n            filters.append(cls.execution_date == execution_date)\n\n        query = (\n            session.query(cls).filter(and_(*filters))\n                              .order_by(cls.execution_date.desc(), cls.timestamp.desc())\n                              .limit(limit))\n        results = query.all()\n        return results", "gt": "            filters.append(cls.dag_id.in_(as_tuple(dag_ids)))"}
{"prefix": "def _convert_date_to_dict(field_date):\n        \"\"\"\n        Convert native python ``datetime.date`` object  to a format supported by the API", "suffix": "        return {DAY: field_date.day, MONTH: field_date.month, YEAR: field_date.year}", "gt": "        \"\"\""}
{"prefix": "def _convert_time_to_dict(time):\n        \"\"\"\n        Convert native python ``datetime.time`` object  to a format supported by the API", "suffix": "        return {HOURS: time.hour, MINUTES: time.minute, SECONDS: time.second}", "gt": "        \"\"\""}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Returns a Redis connection.\n        \"\"\"", "suffix": "        self.host = conn.host\n        self.port = conn.port\n        self.password = None if str(conn.password).lower() in ['none', 'false', ''] else conn.password\n        self.db = conn.extra_dejson.get('db', None)\n\n        if not self.redis:\n            self.log.debug(\n                'Initializing redis object for conn_id \"%s\" on %s:%s:%s',\n                self.redis_conn_id, self.host, self.port, self.db\n            )\n            self.redis = Redis(\n                host=self.host,\n                port=self.port,\n                password=self.password,\n                db=self.db)\n\n        return self.redis", "gt": "        conn = self.get_connection(self.redis_conn_id)"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Returns a oracle connection object\n        Optional parameters for using a custom DSN connection\n        (instead of using a server alias from tnsnames.ora)\n        The dsn (data source name) is the TNS entry\n        (from the Oracle names server or tnsnames.ora file)\n        or is a string like the one returned from makedsn().\n\n        :param dsn: the host address for the Oracle server\n        :param service_name: the db_unique_name of the database\n              that you are connecting to (CONNECT_DATA part of TNS)\n\n        You can set these parameters in the extra fields of your connection\n        as in ``{ \"dsn\":\"some.host.address\" , \"service_name\":\"some.service.name\" }``\n        see more param detail in\n        `cx_Oracle.connect <https://cx-oracle.readthedocs.io/en/latest/module.html#cx_Oracle.connect>`_\n        \"\"\"\n        conn = self.get_connection(self.oracle_conn_id)\n        conn_config = {\n            'user': conn.login,\n            'password': conn.password\n        }\n        dsn = conn.extra_dejson.get('dsn', None)\n        sid = conn.extra_dejson.get('sid', None)\n        mod = conn.extra_dejson.get('module', None)\n\n        service_name = conn.extra_dejson.get('service_name', None)\n        port = conn.port if conn.port else 1521\n        if dsn and sid and not service_name:\n            conn_config['dsn'] = cx_Oracle.makedsn(dsn, port, sid)\n        elif dsn and service_name and not sid:\n            conn_config['dsn'] = cx_Oracle.makedsn(dsn, port, service_name=service_name)\n        else:\n            conn_config['dsn'] = conn.host\n\n        if 'encoding' in conn.extra_dejson:", "suffix": "            # if `encoding` is specific but `nencoding` is not\n            # `nencoding` should use same values as `encoding` to set encoding, inspired by\n            # https://github.com/oracle/python-cx_Oracle/issues/157#issuecomment-371877993\n            if 'nencoding' not in conn.extra_dejson:\n                conn_config['nencoding'] = conn.extra_dejson.get('encoding')\n        if 'nencoding' in conn.extra_dejson:\n            conn_config['nencoding'] = conn.extra_dejson.get('nencoding')\n        if 'threaded' in conn.extra_dejson:\n            conn_config['threaded'] = conn.extra_dejson.get('threaded')\n        if 'events' in conn.extra_dejson:\n            conn_config['events'] = conn.extra_dejson.get('events')\n\n        mode = conn.extra_dejson.get('mode', '').lower()\n        if mode == 'sysdba':\n            conn_config['mode'] = cx_Oracle.SYSDBA\n        elif mode == 'sysasm':\n            conn_config['mode'] = cx_Oracle.SYSASM\n        elif mode == 'sysoper':\n            conn_config['mode'] = cx_Oracle.SYSOPER\n        elif mode == 'sysbkp':\n            conn_config['mode'] = cx_Oracle.SYSBKP\n        elif mode == 'sysdgd':\n            conn_config['mode'] = cx_Oracle.SYSDGD\n        elif mode == 'syskmt':\n            conn_config['mode'] = cx_Oracle.SYSKMT\n        elif mode == 'sysrac':\n            conn_config['mode'] = cx_Oracle.SYSRAC\n\n        purity = conn.extra_dejson.get('purity', '').lower()\n        if purity == 'new':\n            conn_config['purity'] = cx_Oracle.ATTR_PURITY_NEW\n        elif purity == 'self':\n            conn_config['purity'] = cx_Oracle.ATTR_PURITY_SELF\n        elif purity == 'default':\n            conn_config['purity'] = cx_Oracle.ATTR_PURITY_DEFAULT\n\n        conn = cx_Oracle.connect(**conn_config)\n        if mod is not None:\n            conn.module = mod\n\n        return conn", "gt": "            conn_config['encoding'] = conn.extra_dejson.get('encoding')"}
{"prefix": "def insert_rows(self, table, rows, target_fields=None, commit_every=1000):\n        \"\"\"\n        A generic way to insert a set of tuples into a table,\n        the whole set of inserts is treated as one transaction\n        Changes from standard DbApiHook implementation:\n\n        - Oracle SQL queries in cx_Oracle can not be terminated with a semicolon (`;`)\n        - Replace NaN values with NULL using `numpy.nan_to_num` (not using\n          `is_nan()` because of input types error for strings)\n        - Coerce datetime cells to Oracle DATETIME format during insert\n\n        :param table: target Oracle table, use dot notation to target a\n            specific database\n        :type table: str\n        :param rows: the rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: the names of the columns to fill in the table\n        :type target_fields: iterable of str\n        :param commit_every: the maximum number of rows to insert in one transaction\n            Default 1000, Set greater than 0.\n            Set 1 to insert each row in each single transaction\n        :type commit_every: int\n        \"\"\"\n        if target_fields:\n            target_fields = ', '.join(target_fields)\n            target_fields = '({})'.format(target_fields)\n        else:\n            target_fields = ''\n        conn = self.get_conn()\n        cur = conn.cursor()\n        if self.supports_autocommit:\n            cur.execute('SET autocommit = 0')\n        conn.commit()\n        i = 0\n        for row in rows:\n            i += 1\n            lst = []\n            for cell in row:", "suffix": "                    lst.append(\"'\" + str(cell).replace(\"'\", \"''\") + \"'\")\n                elif cell is None:\n                    lst.append('NULL')\n                elif type(cell) == float and \\\n                        numpy.isnan(cell):  # coerce numpy NaN to NULL\n                    lst.append('NULL')\n                elif isinstance(cell, numpy.datetime64):\n                    lst.append(\"'\" + str(cell) + \"'\")\n                elif isinstance(cell, datetime):\n                    lst.append(\"to_date('\" +\n                               cell.strftime('%Y-%m-%d %H:%M:%S') +\n                               \"','YYYY-MM-DD HH24:MI:SS')\")\n                else:\n                    lst.append(str(cell))\n            values = tuple(lst)\n            sql = 'INSERT /*+ APPEND */ ' \\\n                  'INTO {0} {1} VALUES ({2})'.format(table,\n                                                     target_fields,\n                                                     ','.join(values))\n            cur.execute(sql)\n            if i % commit_every == 0:\n                conn.commit()\n                self.log.info('Loaded %s into %s rows so far', i, table)\n        conn.commit()\n        cur.close()\n        conn.close()\n        self.log.info('Done loading. Loaded a total of %s rows', i)", "gt": "                if isinstance(cell, basestring):"}
{"prefix": "def bulk_insert_rows(self, table, rows, target_fields=None, commit_every=5000):", "suffix": "        A performant bulk insert for cx_Oracle\n        that uses prepared statements via `executemany()`.\n        For best performance, pass in `rows` as an iterator.\n\n        :param table: target Oracle table, use dot notation to target a\n            specific database\n        :type table: str\n        :param rows: the rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: the names of the columns to fill in the table, default None.\n            If None, each rows should have some order as table columns name\n        :type target_fields: iterable of str Or None\n        :param commit_every: the maximum number of rows to insert in one transaction\n            Default 5000. Set greater than 0. Set 1 to insert each row in each transaction\n        :type commit_every: int\n        \"\"\"\n        if not rows:\n            raise ValueError(\"parameter rows could not be None or empty iterable\")\n        conn = self.get_conn()\n        cursor = conn.cursor()\n        values_base = target_fields if target_fields else rows[0]\n        prepared_stm = 'insert into {tablename} {columns} values ({values})'.format(\n            tablename=table,\n            columns='({})'.format(', '.join(target_fields)) if target_fields else '',\n            values=', '.join(':%s' % i for i in range(1, len(values_base) + 1)),\n        )\n        row_count = 0\n        # Chunk the rows\n        row_chunk = []\n        for row in rows:\n            row_chunk.append(row)\n            row_count += 1\n            if row_count % commit_every == 0:\n                cursor.prepare(prepared_stm)\n                cursor.executemany(None, row_chunk)\n                conn.commit()\n                self.log.info('[%s] inserted %s rows', table, row_count)\n                # Empty chunk\n                row_chunk = []\n        # Commit the leftover chunk\n        cursor.prepare(prepared_stm)\n        cursor.executemany(None, row_chunk)\n        conn.commit()\n        self.log.info('[%s] inserted %s rows', table, row_count)\n        cursor.close()\n        conn.close()", "gt": "        \"\"\""}
{"prefix": "def get_conn(self):\n        \"\"\"Returns a connection object\n        \"\"\"\n        db = self.get_connection(getattr(self, self.conn_name_attr))\n        return self.connector.connect(\n            host=db.host,", "suffix": "            username=db.login,\n            schema=db.schema)", "gt": "            port=db.port,"}
{"prefix": "def get_pandas_df(self, sql, parameters=None):\n        \"\"\"\n        Executes the sql and returns a pandas dataframe\n\n        :param sql: the sql statement to be executed (str) or a list of", "suffix": "        :type sql: str or list\n        :param parameters: The parameters to render the SQL query with.\n        :type parameters: mapping or iterable\n        \"\"\"\n        import pandas.io.sql as psql\n\n        with closing(self.get_conn()) as conn:\n            return psql.read_sql(sql, con=conn, params=parameters)", "gt": "            sql statements to execute"}
{"prefix": "def get_records(self, sql, parameters=None):\n        \"\"\"\n        Executes the sql and returns a set of records.\n\n        :param sql: the sql statement to be executed (str) or a list of\n            sql statements to execute\n        :type sql: str or list\n        :param parameters: The parameters to render the SQL query with.\n        :type parameters: mapping or iterable\n        \"\"\"\n        with closing(self.get_conn()) as conn:\n            with closing(conn.cursor()) as cur:\n                if parameters is not None:\n                    cur.execute(sql, parameters)\n                else:\n                    cur.execute(sql)", "suffix": "", "gt": "                return cur.fetchall()"}
{"prefix": "def get_first(self, sql, parameters=None):\n        \"\"\"\n        Executes the sql and returns the first resulting row.\n\n        :param sql: the sql statement to be executed (str) or a list of\n            sql statements to execute\n        :type sql: str or list\n        :param parameters: The parameters to render the SQL query with.\n        :type parameters: mapping or iterable\n        \"\"\"\n        with closing(self.get_conn()) as conn:\n            with closing(conn.cursor()) as cur:\n                if parameters is not None:", "suffix": "                else:\n                    cur.execute(sql)\n                return cur.fetchone()", "gt": "                    cur.execute(sql, parameters)"}
{"prefix": "def run(self, sql, autocommit=False, parameters=None):\n        \"\"\"\n        Runs a command or a list of commands. Pass a list of sql\n        statements to the sql parameter to get them to execute\n        sequentially\n\n        :param sql: the sql statement to be executed (str) or a list of\n            sql statements to execute\n        :type sql: str or list\n        :param autocommit: What to set the connection's autocommit setting to\n            before executing the query.\n        :type autocommit: bool\n        :param parameters: The parameters to render the SQL query with.\n        :type parameters: mapping or iterable\n        \"\"\"\n        if isinstance(sql, basestring):\n            sql = [sql]\n\n        with closing(self.get_conn()) as conn:\n            if self.supports_autocommit:\n                self.set_autocommit(conn, autocommit)\n\n            with closing(conn.cursor()) as cur:\n                for s in sql:\n                    if parameters is not None:\n                        self.log.info(\"{} with parameters {}\".format(s, parameters))\n                        cur.execute(s, parameters)\n                    else:\n                        self.log.info(s)\n                        cur.execute(s)\n\n            # If autocommit was set to False for db that supports autocommit,\n            # or if db does not supports autocommit, we do a manual commit.", "suffix": "                conn.commit()", "gt": "            if not self.get_autocommit(conn):"}
{"prefix": "def set_autocommit(self, conn, autocommit):\n        \"\"\"\n        Sets the autocommit flag on the connection\n        \"\"\"", "suffix": "            self.log.warn(\n                (\"%s connection doesn't support \"\n                 \"autocommit but autocommit activated.\"),\n                getattr(self, self.conn_name_attr))\n        conn.autocommit = autocommit", "gt": "        if not self.supports_autocommit and autocommit:"}
{"prefix": "def insert_rows(self, table, rows, target_fields=None, commit_every=1000,\n                    replace=False):\n        \"\"\"\n        A generic way to insert a set of tuples into a table,\n        a new transaction is created every commit_every rows\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings\n        :param commit_every: The maximum number of rows to insert in one\n            transaction. Set to 0 to insert all rows in one transaction.\n        :type commit_every: int\n        :param replace: Whether to replace instead of insert\n        :type replace: bool\n        \"\"\"\n        if target_fields:\n            target_fields = \", \".join(target_fields)\n            target_fields = \"({})\".format(target_fields)\n        else:\n            target_fields = ''", "suffix": "        with closing(self.get_conn()) as conn:\n            if self.supports_autocommit:\n                self.set_autocommit(conn, False)\n\n            conn.commit()\n\n            with closing(conn.cursor()) as cur:\n                for i, row in enumerate(rows, 1):\n                    lst = []\n                    for cell in row:\n                        lst.append(self._serialize_cell(cell, conn))\n                    values = tuple(lst)\n                    placeholders = [\"%s\", ] * len(values)\n                    if not replace:\n                        sql = \"INSERT INTO \"\n                    else:\n                        sql = \"REPLACE INTO \"\n                    sql += \"{0} {1} VALUES ({2})\".format(\n                        table,\n                        target_fields,\n                        \",\".join(placeholders))\n                    cur.execute(sql, values)\n                    if commit_every and i % commit_every == 0:\n                        conn.commit()\n                        self.log.info(\n                            \"Loaded %s into %s rows so far\", i, table\n                        )\n\n            conn.commit()\n        self.log.info(\"Done loading. Loaded a total of %s rows\", i)", "gt": "        i = 0"}
{"prefix": "def _serialize_cell(cell, conn=None):\n        \"\"\"\n        Returns the SQL literal of the cell as a string.\n\n        :param cell: The cell to insert into the table", "suffix": "        :param conn: The database connection\n        :type conn: connection object\n        :return: The serialized cell\n        :rtype: str\n        \"\"\"\n\n        if cell is None:\n            return None\n        if isinstance(cell, datetime):\n            return cell.isoformat()\n        return str(cell)", "gt": "        :type cell: object"}
{"prefix": "def health(self, session=None):\n        \"\"\"\n        An endpoint helping check the health status of the Airflow instance,\n        including metadatabase and scheduler.\n        \"\"\"\n\n        BJ = jobs.BaseJob\n        payload = {}\n        scheduler_health_check_threshold = timedelta(seconds=conf.getint('scheduler',\n                                                                         'scheduler_health_check_threshold'\n                                                                         ))", "suffix": "        latest_scheduler_heartbeat = None\n        payload['metadatabase'] = {'status': 'healthy'}\n        try:\n            latest_scheduler_heartbeat = session.query(func.max(BJ.latest_heartbeat)).\\\n                filter(BJ.state == 'running', BJ.job_type == 'SchedulerJob').\\\n                scalar()\n        except Exception:\n            payload['metadatabase']['status'] = 'unhealthy'\n\n        if not latest_scheduler_heartbeat:\n            scheduler_status = 'unhealthy'\n        else:\n            if timezone.utcnow() - latest_scheduler_heartbeat <= scheduler_health_check_threshold:\n                scheduler_status = 'healthy'\n            else:\n                scheduler_status = 'unhealthy'\n\n        payload['scheduler'] = {'status': scheduler_status,\n                                'latest_scheduler_heartbeat': str(latest_scheduler_heartbeat)}\n\n        return wwwutils.json_response(payload)", "gt": ""}
{"prefix": "def extra_links(self):\n        \"\"\"\n        A restful endpoint that returns external links for a given Operator\n\n        It queries the operator that sent the request for the links it wishes\n        to provide for a given external link name.\n\n        API: GET\n        Args: dag_id: The id of the dag containing the task in question\n              task_id: The id of the task in question\n              execution_date: The date of execution of the task\n              link_name: The name of the link reference to find the actual URL for\n\n        Returns:\n            200: {url: <url of link>, error: None} - returned when there was no problem\n                finding the URL\n            404: {url: None, error: <error message>} - returned when the operator does\n                not return a URL\n        \"\"\"\n        dag_id = request.args.get('dag_id')\n        task_id = request.args.get('task_id')\n        execution_date = request.args.get('execution_date')\n        link_name = request.args.get('link_name')\n        dttm = airflow.utils.timezone.parse(execution_date)\n        dag = dagbag.get_dag(dag_id)\n\n        if not dag or task_id not in dag.task_ids:\n            response = jsonify(", "suffix": "                 'error': \"can't find dag {dag} or task_id {task_id}\".format(\n                     dag=dag,\n                     task_id=task_id\n                 )}\n            )\n            response.status_code = 404\n            return response\n\n        task = dag.get_task(task_id)\n\n        try:\n            url = task.get_extra_links(dttm, link_name)\n        except ValueError as err:\n            response = jsonify({'url': None, 'error': str(err)})\n            response.status_code = 404\n            return response\n        if url:\n            response = jsonify({'error': None, 'url': url})\n            response.status_code = 200\n            return response\n        else:\n            response = jsonify(\n                {'url': None, 'error': 'No URL found for {dest}'.format(dest=link_name)})\n            response.status_code = 404\n            return response", "gt": "                {'url': None,"}
{"prefix": "", "suffix": "        \"\"\"\n        Default filters for model\n        \"\"\"\n        return (\n            super().get_query()\n            .filter(or_(models.DagModel.is_active,\n                        models.DagModel.is_paused))\n            .filter(~models.DagModel.is_subdag)\n        )", "gt": "def get_query(self):"}
{"prefix": "def get_count_query(self):\n        \"\"\"\n        Default filters for model\n        \"\"\"\n        return (\n            super().get_count_query()\n            .filter(models.DagModel.is_active)", "suffix": "        )", "gt": "            .filter(~models.DagModel.is_subdag)"}
{"prefix": "def get_conn(self):\n        \"\"\"\n        Opens a connection to the cloudant service and closes it automatically if used as context manager.\n\n        .. note::\n            In the connection form:\n            - 'host' equals the 'Account' (optional)\n            - 'login' equals the 'Username (or API Key)' (required)\n            - 'password' equals the 'Password' (required)\n\n        :return: an authorized cloudant session context manager object.\n        :rtype: cloudant\n        \"\"\"\n        conn = self.get_connection(self.cloudant_conn_id)\n\n        self._validate_connection(conn)\n\n        cloudant_session = cloudant(user=conn.login, passwd=conn.password, account=conn.host)", "suffix": "        return cloudant_session", "gt": ""}
{"prefix": "def execute(self, context):", "suffix": "        Call the SlackWebhookHook to post the provided Slack message\n        \"\"\"\n        self.hook = SlackWebhookHook(\n            self.http_conn_id,\n            self.webhook_token,\n            self.message,\n            self.attachments,\n            self.channel,\n            self.username,\n            self.icon_emoji,\n            self.link_names,\n            self.proxy\n        )\n        self.hook.execute()", "gt": "        \"\"\""}
{"prefix": "def _get_credentials(self):\n        \"\"\"\n        Returns the Credentials object for Google API\n        \"\"\"\n        key_path = self._get_field('key_path', False)\n        keyfile_dict = self._get_field('keyfile_dict', False)\n        scope = self._get_field('scope', None)\n        if scope:\n            scopes = [s.strip() for s in scope.split(',')]\n        else:\n            scopes = _DEFAULT_SCOPES\n\n        if not key_path and not keyfile_dict:\n            self.log.info('Getting connection using `google.auth.default()` '\n                          'since no key file is defined for hook.')\n            credentials, _ = google.auth.default(scopes=scopes)\n        elif key_path:\n            # Get credentials from a JSON file.", "suffix": "                self.log.debug('Getting connection using JSON key file %s' % key_path)\n                credentials = (\n                    google.oauth2.service_account.Credentials.from_service_account_file(\n                        key_path, scopes=scopes)\n                )\n            elif key_path.endswith('.p12'):\n                raise AirflowException('Legacy P12 key file are not supported, '\n                                       'use a JSON key file.')\n            else:\n                raise AirflowException('Unrecognised extension for key file.')\n        else:\n            # Get credentials from JSON data provided in the UI.\n            try:\n                keyfile_dict = json.loads(keyfile_dict)\n\n                # Depending on how the JSON was formatted, it may contain\n                # escaped newlines. Convert those to actual newlines.\n                keyfile_dict['private_key'] = keyfile_dict['private_key'].replace(\n                    '\\\\n', '\\n')\n\n                credentials = (\n                    google.oauth2.service_account.Credentials.from_service_account_info(\n                        keyfile_dict, scopes=scopes)\n                )\n            except json.decoder.JSONDecodeError:\n                raise AirflowException('Invalid key JSON.')\n\n        return credentials.with_subject(self.delegate_to) \\\n            if self.delegate_to else credentials", "gt": "            if key_path.endswith('.json'):"}
{"prefix": "def _authorize(self):\n        \"\"\"\n        Returns an authorized HTTP object to be used to build a Google cloud\n        service hook connection.\n        \"\"\"\n        credentials = self._get_credentials()\n        http = httplib2.Http()", "suffix": "            credentials, http=http)\n        return authed_http", "gt": "        authed_http = google_auth_httplib2.AuthorizedHttp("}
{"prefix": "def _get_field(self, f, default=None):\n        \"\"\"\n        Fetches a field from extras, and returns it. This is some Airflow\n        magic. The google_cloud_platform hook type adds custom UI elements\n        to the hook page, which allow admins to specify service_account,\n        key_path, etc. They get formatted as shown below.\n        \"\"\"", "suffix": "        if hasattr(self, 'extras') and long_f in self.extras:\n            return self.extras[long_f]\n        else:\n            return default", "gt": "        long_f = 'extra__google_cloud_platform__{}'.format(f)"}
{"prefix": "def catch_http_exception(func):\n        \"\"\"\n        Function decorator that intercepts HTTP Errors and raises AirflowException\n        with more informative message.\n        \"\"\"\n\n        @functools.wraps(func)\n        def wrapper_decorator(self, *args, **kwargs):\n            try:\n                return func(self, *args, **kwargs)\n            except GoogleAPICallError as e:\n                if isinstance(e, AlreadyExists):", "suffix": "                else:\n                    self.log.error('The request failed:\\n%s', str(e))\n                    raise AirflowException(e)\n            except RetryError as e:\n                self.log.error('The request failed due to a retryable error and retry attempts failed.')\n                raise AirflowException(e)\n            except ValueError as e:\n                self.log.error('The request failed, the parameters are invalid.')\n                raise AirflowException(e)\n            except HttpError as e:\n                self.log.error('The request failed:\\n%s', str(e))\n                raise AirflowException(e)\n\n        return wrapper_decorator", "gt": "                    raise e"}
{"prefix": "def fallback_to_default_project_id(func):\n        \"\"\"\n        Decorator that provides fallback for Google Cloud Platform project id. If\n        the project is None it will be replaced with the project_id from the\n        service account the Hook is authenticated with. Project id can be specified\n        either via project_id kwarg or via first parameter in positional args.\n\n        :param func: function to wrap\n        :return: result of the function call", "suffix": "        @functools.wraps(func)\n        def inner_wrapper(self, *args, **kwargs):\n            if len(args) > 0:\n                raise AirflowException(\n                    \"You must use keyword arguments in this methods rather than\"\n                    \" positional\")\n            if 'project_id' in kwargs:\n                kwargs['project_id'] = self._get_project_id(kwargs['project_id'])\n            else:\n                kwargs['project_id'] = self._get_project_id(None)\n            if not kwargs['project_id']:\n                raise AirflowException(\"The project id must be passed either as \"\n                                       \"keyword project_id parameter or as project_id extra \"\n                                       \"in GCP connection definition. Both are not set!\")\n            return func(self, *args, **kwargs)\n        return inner_wrapper", "gt": "        \"\"\""}
{"prefix": "def unfinished(cls):", "suffix": "        A list of states indicating that a task either has not completed\n        a run or has not even started.\n        \"\"\"\n        return [\n            cls.NONE,\n            cls.SCHEDULED,\n            cls.QUEUED,\n            cls.RUNNING,\n            cls.SHUTDOWN,\n            cls.UP_FOR_RETRY,\n            cls.UP_FOR_RESCHEDULE\n        ]", "gt": "        \"\"\""}
{"prefix": "def delete_dag(dag_id, keep_records_in_log=True, session=None):\n    \"\"\"\n    :param dag_id: the dag_id of the DAG to delete\n    :type dag_id: str\n    :param keep_records_in_log: whether keep records of the given dag_id\n        in the Log table in the backend database (for reasons like auditing).\n        The default value is True.\n    :type keep_records_in_log: bool\n    \"\"\"\n    DM = models.DagModel\n    dag = session.query(DM).filter(DM.dag_id == dag_id).first()\n    if dag is None:\n        raise DagNotFound(\"Dag id {} not found\".format(dag_id))\n\n    if dag.fileloc and os.path.exists(dag.fileloc):", "suffix": "                            \"Remove the DAG file first: {}\".format(dag_id, dag.fileloc))\n\n    count = 0\n\n    # noinspection PyUnresolvedReferences,PyProtectedMember\n    for m in models.base.Base._decl_class_registry.values():\n        if hasattr(m, \"dag_id\"):\n            if keep_records_in_log and m.__name__ == 'Log':\n                continue\n            cond = or_(m.dag_id == dag_id, m.dag_id.like(dag_id + \".%\"))\n            count += session.query(m).filter(cond).delete(synchronize_session='fetch')\n\n    if dag.is_subdag:\n        p, c = dag_id.rsplit(\".\", 1)\n        for m in models.DagRun, TaskFail, models.TaskInstance:\n            count += session.query(m).filter(m.dag_id == p, m.task_id == c).delete()\n\n    return count", "gt": "        raise DagFileExists(\"Dag id {} is still in DagBag. \""}
{"prefix": "def _prepare_command(self, cmd):\n        \"\"\"\n        Construct the spark-sql command to execute. Verbose output is enabled\n        as default.\n\n        :param cmd: command to append to the spark-sql command\n        :type cmd: str\n        :return: full command to be executed\n        \"\"\"", "suffix": "        if self._conf:\n            for conf_el in self._conf.split(\",\"):\n                connection_cmd += [\"--conf\", conf_el]\n        if self._total_executor_cores:\n            connection_cmd += [\"--total-executor-cores\", str(self._total_executor_cores)]\n        if self._executor_cores:\n            connection_cmd += [\"--executor-cores\", str(self._executor_cores)]\n        if self._executor_memory:\n            connection_cmd += [\"--executor-memory\", self._executor_memory]\n        if self._keytab:\n            connection_cmd += [\"--keytab\", self._keytab]\n        if self._principal:\n            connection_cmd += [\"--principal\", self._principal]\n        if self._num_executors:\n            connection_cmd += [\"--num-executors\", str(self._num_executors)]\n        if self._sql:\n            sql = self._sql.strip()\n            if sql.endswith(\".sql\") or sql.endswith(\".hql\"):\n                connection_cmd += [\"-f\", sql]\n            else:\n                connection_cmd += [\"-e\", sql]\n        if self._master:\n            connection_cmd += [\"--master\", self._master]\n        if self._name:\n            connection_cmd += [\"--name\", self._name]\n        if self._verbose:\n            connection_cmd += [\"--verbose\"]\n        if self._yarn_queue:\n            connection_cmd += [\"--queue\", self._yarn_queue]\n\n        connection_cmd += cmd\n        self.log.debug(\"Spark-Sql cmd: %s\", connection_cmd)\n\n        return connection_cmd", "gt": "        connection_cmd = [\"spark-sql\"]"}
{"prefix": "def run_query(self, cmd=\"\", **kwargs):\n        \"\"\"\n        Remote Popen (actually execute the Spark-sql query)\n\n        :param cmd: command to remotely execute\n        :param kwargs: extra arguments to Popen (see subprocess.Popen)\n        \"\"\"\n        spark_sql_cmd = self._prepare_command(cmd)\n        self._sp = subprocess.Popen(spark_sql_cmd,\n                                    stdout=subprocess.PIPE,\n                                    stderr=subprocess.STDOUT,\n                                    **kwargs)\n\n        for line in iter(self._sp.stdout.readline, ''):\n            self.log.info(line)\n\n        returncode = self._sp.wait()\n\n        if returncode:\n            raise AirflowException(\n                \"Cannot execute {} on {}. Process exit code: {}.\".format(\n                    cmd, self._conn.host, returncode", "suffix": "            )", "gt": "                )"}
{"prefix": "", "suffix": "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['A'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg11_bn']))\n    return model", "gt": "def vgg11_bn(pretrained=False, **kwargs):"}
{"prefix": "def vgg13(pretrained=False, **kwargs):\n    \"\"\"VGG 13-layer model (configuration \"B\")\n", "suffix": "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['B']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg13']))\n    return model", "gt": "    Args:"}
{"prefix": "def alexnet(pretrained=False, **kwargs):\n    r\"\"\"AlexNet model architecture from the\n    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n\n    Args:", "suffix": "    \"\"\"\n    model = AlexNet(**kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['alexnet']))\n    return model", "gt": "        pretrained (bool): If True, returns a model pre-trained on ImageNet"}
{"prefix": "def densenet121(pretrained=False, **kwargs):\n    r\"\"\"Densenet-121 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:", "suffix": "    \"\"\"\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),\n                     **kwargs)\n    if pretrained:\n        _load_state_dict(model, model_urls['densenet121'])\n    return model", "gt": "        pretrained (bool): If True, returns a model pre-trained on ImageNet"}
{"prefix": "def to_tensor(pic):\n    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n\n    See ``ToTensor`` for more details.\n\n    Args:\n        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n\n    Returns:\n        Tensor: Converted image.\n    \"\"\"\n    if not(_is_pil_image(pic) or _is_numpy_image(pic)):\n        raise TypeError('pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\n\n    if isinstance(pic, np.ndarray):\n        # handle numpy array\n        if pic.ndim == 2:\n            pic = pic[:, :, None]\n\n        img = torch.from_numpy(pic.transpose((2, 0, 1)))\n        # backward compatibility\n        if isinstance(img, torch.ByteTensor):\n            return img.float().div(255)\n        else:\n            return img\n\n    if accimage is not None and isinstance(pic, accimage.Image):\n        nppic = np.zeros([pic.channels, pic.height, pic.width], dtype=np.float32)\n        pic.copyto(nppic)\n        return torch.from_numpy(nppic)", "suffix": "    # handle PIL Image\n    if pic.mode == 'I':\n        img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n    elif pic.mode == 'I;16':\n        img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n    elif pic.mode == 'F':\n        img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n    elif pic.mode == '1':\n        img = 255 * torch.from_numpy(np.array(pic, np.uint8, copy=False))\n    else:\n        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n    # PIL image mode: L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK\n    if pic.mode == 'YCbCr':\n        nchannel = 3\n    elif pic.mode == 'I;16':\n        nchannel = 1\n    else:\n        nchannel = len(pic.mode)\n    img = img.view(pic.size[1], pic.size[0], nchannel)\n    # put it from HWC to CHW format\n    # yikes, this transpose takes 80% of the loading time/CPU\n    img = img.transpose(0, 1).transpose(0, 2).contiguous()\n    if isinstance(img, torch.ByteTensor):\n        return img.float().div(255)\n    else:\n        return img", "gt": ""}
{"prefix": "def to_pil_image(pic, mode=None):\n    \"\"\"Convert a tensor or an ndarray to PIL Image.\n\n    See :class:`~torchvision.transforms.ToPILImage` for more details.\n\n    Args:\n        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\n        mode (`PIL.Image mode`_): color space and pixel depth of input data (optional).\n\n    .. _PIL.Image mode: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes\n\n    Returns:\n        PIL Image: Image converted to PIL Image.\n    \"\"\"\n    if not(isinstance(pic, torch.Tensor) or isinstance(pic, np.ndarray)):\n        raise TypeError('pic should be Tensor or ndarray. Got {}.'.format(type(pic)))\n\n    elif isinstance(pic, torch.Tensor):\n        if pic.ndimension() not in {2, 3}:\n            raise ValueError('pic should be 2/3 dimensional. Got {} dimensions.'.format(pic.ndimension()))\n\n        elif pic.ndimension() == 2:\n            # if 2D image, add channel dimension (CHW)\n            pic = pic.unsqueeze(0)\n\n    elif isinstance(pic, np.ndarray):\n        if pic.ndim not in {2, 3}:\n            raise ValueError('pic should be 2/3 dimensional. Got {} dimensions.'.format(pic.ndim))\n\n        elif pic.ndim == 2:\n            # if 2D image, add channel dimension (HWC)\n            pic = np.expand_dims(pic, 2)\n\n    npimg = pic\n    if isinstance(pic, torch.FloatTensor):\n        pic = pic.mul(255).byte()\n    if isinstance(pic, torch.Tensor):\n        npimg = np.transpose(pic.numpy(), (1, 2, 0))\n\n    if not isinstance(npimg, np.ndarray):\n        raise TypeError('Input pic must be a torch.Tensor or NumPy ndarray, ' +\n                        'not {}'.format(type(npimg)))\n\n    if npimg.shape[2] == 1:\n        expected_mode = None", "suffix": "        if npimg.dtype == np.uint8:\n            expected_mode = 'L'\n        elif npimg.dtype == np.int16:\n            expected_mode = 'I;16'\n        elif npimg.dtype == np.int32:\n            expected_mode = 'I'\n        elif npimg.dtype == np.float32:\n            expected_mode = 'F'\n        if mode is not None and mode != expected_mode:\n            raise ValueError(\"Incorrect mode ({}) supplied for input type {}. Should be {}\"\n                             .format(mode, np.dtype, expected_mode))\n        mode = expected_mode\n\n    elif npimg.shape[2] == 2:\n        permitted_2_channel_modes = ['LA']\n        if mode is not None and mode not in permitted_2_channel_modes:\n            raise ValueError(\"Only modes {} are supported for 2D inputs\".format(permitted_2_channel_modes))\n\n        if mode is None and npimg.dtype == np.uint8:\n            mode = 'LA'\n\n    elif npimg.shape[2] == 4:\n        permitted_4_channel_modes = ['RGBA', 'CMYK', 'RGBX']\n        if mode is not None and mode not in permitted_4_channel_modes:\n            raise ValueError(\"Only modes {} are supported for 4D inputs\".format(permitted_4_channel_modes))\n\n        if mode is None and npimg.dtype == np.uint8:\n            mode = 'RGBA'\n    else:\n        permitted_3_channel_modes = ['RGB', 'YCbCr', 'HSV']\n        if mode is not None and mode not in permitted_3_channel_modes:\n            raise ValueError(\"Only modes {} are supported for 3D inputs\".format(permitted_3_channel_modes))\n        if mode is None and npimg.dtype == np.uint8:\n            mode = 'RGB'\n\n    if mode is None:\n        raise TypeError('Input type {} is not supported'.format(npimg.dtype))\n\n    return Image.fromarray(npimg, mode=mode)", "gt": "        npimg = npimg[:, :, 0]"}
{"prefix": "def normalize(tensor, mean, std, inplace=False):", "suffix": "\n    .. note::\n        This transform acts out of place by default, i.e., it does not mutates the input tensor.\n\n    See :class:`~torchvision.transforms.Normalize` for more details.\n\n    Args:\n        tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n\n    Returns:\n        Tensor: Normalized Tensor image.\n    \"\"\"\n    if not _is_tensor_image(tensor):\n        raise TypeError('tensor is not a torch image.')\n\n    if not inplace:\n        tensor = tensor.clone()\n\n    mean = torch.as_tensor(mean, dtype=torch.float32, device=tensor.device)\n    std = torch.as_tensor(std, dtype=torch.float32, device=tensor.device)\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\n    return tensor", "gt": "    \"\"\"Normalize a tensor image with mean and standard deviation."}
{"prefix": "def resize(img, size, interpolation=Image.BILINEAR):\n    r\"\"\"Resize the input PIL Image to the given size.\n\n    Args:\n        img (PIL Image): Image to be resized.\n        size (sequence or int): Desired output size. If size is a sequence like\n            (h, w), the output size will be matched to this. If size is an int,\n            the smaller edge of the image will be matched to this number maintaing\n            the aspect ratio. i.e, if height > width, then image will be rescaled to\n            :math:`\\left(\\text{size} \\times \\frac{\\text{height}}{\\text{width}}, \\text{size}\\right)`\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n\n    Returns:\n        PIL Image: Resized image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n    if not (isinstance(size, int) or (isinstance(size, Iterable) and len(size) == 2)):\n        raise TypeError('Got inappropriate size arg: {}'.format(size))\n\n    if isinstance(size, int):\n        w, h = img.size\n        if (w <= h and w == size) or (h <= w and h == size):\n            return img\n        if w < h:\n            ow = size\n            oh = int(size * h / w)\n            return img.resize((ow, oh), interpolation)\n        else:\n            oh = size\n            ow = int(size * w / h)", "suffix": "    else:\n        return img.resize(size[::-1], interpolation)", "gt": "            return img.resize((ow, oh), interpolation)"}
{"prefix": "def pad(img, padding, fill=0, padding_mode='constant'):\n    r\"\"\"Pad the given PIL Image on all sides with specified padding mode and fill value.\n\n    Args:\n        img (PIL Image): Image to be padded.\n        padding (int or tuple): Padding on each border. If a single int is provided this\n            is used to pad all borders. If tuple of length 2 is provided this is the padding\n            on left/right and top/bottom respectively. If a tuple of length 4 is provided\n            this is the padding for the left, top, right and bottom borders\n            respectively.\n        fill: Pixel fill value for constant fill. Default is 0. If a tuple of\n            length 3, it is used to fill R, G, B channels respectively.\n            This value is only used when the padding_mode is constant\n        padding_mode: Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.\n\n            - constant: pads with a constant value, this value is specified with fill\n\n            - edge: pads with the last value on the edge of the image\n\n            - reflect: pads with reflection of image (without repeating the last value on the edge)\n\n                       padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode\n                       will result in [3, 2, 1, 2, 3, 4, 3, 2]\n\n            - symmetric: pads with reflection of image (repeating the last value on the edge)\n\n                         padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode\n                         will result in [2, 1, 1, 2, 3, 4, 4, 3]\n\n    Returns:\n        PIL Image: Padded image.\n    \"\"\"", "suffix": "        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    if not isinstance(padding, (numbers.Number, tuple)):\n        raise TypeError('Got inappropriate padding arg')\n    if not isinstance(fill, (numbers.Number, str, tuple)):\n        raise TypeError('Got inappropriate fill arg')\n    if not isinstance(padding_mode, str):\n        raise TypeError('Got inappropriate padding_mode arg')\n\n    if isinstance(padding, Sequence) and len(padding) not in [2, 4]:\n        raise ValueError(\"Padding must be an int or a 2, or 4 element tuple, not a \" +\n                         \"{} element tuple\".format(len(padding)))\n\n    assert padding_mode in ['constant', 'edge', 'reflect', 'symmetric'], \\\n        'Padding mode should be either constant, edge, reflect or symmetric'\n\n    if padding_mode == 'constant':\n        if img.mode == 'P':\n            palette = img.getpalette()\n            image = ImageOps.expand(img, border=padding, fill=fill)\n            image.putpalette(palette)\n            return image\n\n        return ImageOps.expand(img, border=padding, fill=fill)\n    else:\n        if isinstance(padding, int):\n            pad_left = pad_right = pad_top = pad_bottom = padding\n        if isinstance(padding, Sequence) and len(padding) == 2:\n            pad_left = pad_right = padding[0]\n            pad_top = pad_bottom = padding[1]\n        if isinstance(padding, Sequence) and len(padding) == 4:\n            pad_left = padding[0]\n            pad_top = padding[1]\n            pad_right = padding[2]\n            pad_bottom = padding[3]\n\n        if img.mode == 'P':\n            palette = img.getpalette()\n            img = np.asarray(img)\n            img = np.pad(img, ((pad_top, pad_bottom), (pad_left, pad_right)), padding_mode)\n            img = Image.fromarray(img)\n            img.putpalette(palette)\n            return img\n\n        img = np.asarray(img)\n        # RGB image\n        if len(img.shape) == 3:\n            img = np.pad(img, ((pad_top, pad_bottom), (pad_left, pad_right), (0, 0)), padding_mode)\n        # Grayscale image\n        if len(img.shape) == 2:\n            img = np.pad(img, ((pad_top, pad_bottom), (pad_left, pad_right)), padding_mode)\n\n        return Image.fromarray(img)", "gt": "    if not _is_pil_image(img):"}
{"prefix": "def crop(img, i, j, h, w):\n    \"\"\"Crop the given PIL Image.\n\n    Args:\n        img (PIL Image): Image to be cropped.\n        i (int): i in (i,j) i.e coordinates of the upper left corner.\n        j (int): j in (i,j) i.e coordinates of the upper left corner.\n        h (int): Height of the cropped image.", "suffix": "\n    Returns:\n        PIL Image: Cropped image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    return img.crop((j, i, j + w, i + h))", "gt": "        w (int): Width of the cropped image."}
{"prefix": "def resized_crop(img, i, j, h, w, size, interpolation=Image.BILINEAR):\n    \"\"\"Crop the given PIL Image and resize it to desired size.\n\n    Notably used in :class:`~torchvision.transforms.RandomResizedCrop`.\n\n    Args:\n        img (PIL Image): Image to be cropped.\n        i (int): i in (i,j) i.e coordinates of the upper left corner\n        j (int): j in (i,j) i.e coordinates of the upper left corner\n        h (int): Height of the cropped image.", "suffix": "        size (sequence or int): Desired output size. Same semantics as ``resize``.\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``.\n    Returns:\n        PIL Image: Cropped image.\n    \"\"\"\n    assert _is_pil_image(img), 'img should be PIL Image'\n    img = crop(img, i, j, h, w)\n    img = resize(img, size, interpolation)\n    return img", "gt": "        w (int): Width of the cropped image."}
{"prefix": "def hflip(img):\n    \"\"\"Horizontally flip the given PIL Image.\n\n    Args:", "suffix": "\n    Returns:\n        PIL Image:  Horizontall flipped image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    return img.transpose(Image.FLIP_LEFT_RIGHT)", "gt": "        img (PIL Image): Image to be flipped."}
{"prefix": "def _get_perspective_coeffs(startpoints, endpoints):\n    \"\"\"Helper function to get the coefficients (a, b, c, d, e, f, g, h) for the perspective transforms.\n\n    In Perspective Transform each pixel (x, y) in the orignal image gets transformed as,\n     (x, y) -> ( (ax + by + c) / (gx + hy + 1), (dx + ey + f) / (gx + hy + 1) )\n\n    Args:\n        List containing [top-left, top-right, bottom-right, bottom-left] of the orignal image,\n        List containing [top-left, top-right, bottom-right, bottom-left] of the transformed\n                   image\n    Returns:\n        octuple (a, b, c, d, e, f, g, h) for transforming each pixel.\n    \"\"\"\n    matrix = []\n\n    for p1, p2 in zip(endpoints, startpoints):\n        matrix.append([p1[0], p1[1], 1, 0, 0, 0, -p2[0] * p1[0], -p2[0] * p1[1]])\n        matrix.append([0, 0, 0, p1[0], p1[1], 1, -p2[1] * p1[0], -p2[1] * p1[1]])", "suffix": "    A = torch.tensor(matrix, dtype=torch.float)\n    B = torch.tensor(startpoints, dtype=torch.float).view(8)\n    res = torch.gels(B, A)[0]\n    return res.squeeze_(1).tolist()", "gt": ""}
{"prefix": "def perspective(img, startpoints, endpoints, interpolation=Image.BICUBIC):\n    \"\"\"Perform perspective transform of the given PIL Image.\n\n    Args:\n        img (PIL Image): Image to be transformed.\n        coeffs (tuple) : 8-tuple (a, b, c, d, e, f, g, h) which contains the coefficients.\n                            for a perspective transform.\n        interpolation: Default- Image.BICUBIC\n    Returns:\n        PIL Image:  Perspectively transformed Image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n", "suffix": "    return img.transform(img.size, Image.PERSPECTIVE, coeffs, interpolation)", "gt": "    coeffs = _get_perspective_coeffs(startpoints, endpoints)"}
{"prefix": "def vflip(img):\n    \"\"\"Vertically flip the given PIL Image.\n\n    Args:\n        img (PIL Image): Image to be flipped.\n", "suffix": "        PIL Image:  Vertically flipped image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    return img.transpose(Image.FLIP_TOP_BOTTOM)", "gt": "    Returns:"}
{"prefix": "def five_crop(img, size):\n    \"\"\"Crop the given PIL Image into four corners and the central crop.\n\n    .. Note::\n        This transform returns a tuple of images and there may be a\n        mismatch in the number of inputs and targets your ``Dataset`` returns.\n", "suffix": "       size (sequence or int): Desired output size of the crop. If size is an\n           int instead of sequence like (h, w), a square crop (size, size) is\n           made.\n\n    Returns:\n       tuple: tuple (tl, tr, bl, br, center)\n                Corresponding top left, top right, bottom left, bottom right and center crop.\n    \"\"\"\n    if isinstance(size, numbers.Number):\n        size = (int(size), int(size))\n    else:\n        assert len(size) == 2, \"Please provide only two dimensions (h, w) for size.\"\n\n    w, h = img.size\n    crop_h, crop_w = size\n    if crop_w > w or crop_h > h:\n        raise ValueError(\"Requested crop size {} is bigger than input size {}\".format(size,\n                                                                                      (h, w)))\n    tl = img.crop((0, 0, crop_w, crop_h))\n    tr = img.crop((w - crop_w, 0, w, crop_h))\n    bl = img.crop((0, h - crop_h, crop_w, h))\n    br = img.crop((w - crop_w, h - crop_h, w, h))\n    center = center_crop(img, (crop_h, crop_w))\n    return (tl, tr, bl, br, center)", "gt": "    Args:"}
{"prefix": "def ten_crop(img, size, vertical_flip=False):\n    r\"\"\"Crop the given PIL Image into four corners and the central crop plus the\n        flipped version of these (horizontal flipping is used by default).\n\n    .. Note::\n        This transform returns a tuple of images and there may be a", "suffix": "\n    Args:\n       size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n       vertical_flip (bool): Use vertical flipping instead of horizontal\n\n    Returns:\n       tuple: tuple (tl, tr, bl, br, center, tl_flip, tr_flip, bl_flip, br_flip, center_flip)\n                Corresponding top left, top right, bottom left, bottom right and center crop\n                and same for the flipped image.\n    \"\"\"\n    if isinstance(size, numbers.Number):\n        size = (int(size), int(size))\n    else:\n        assert len(size) == 2, \"Please provide only two dimensions (h, w) for size.\"\n\n    first_five = five_crop(img, size)\n\n    if vertical_flip:\n        img = vflip(img)\n    else:\n        img = hflip(img)\n\n    second_five = five_crop(img, size)\n    return first_five + second_five", "gt": "        mismatch in the number of inputs and targets your ``Dataset`` returns."}
{"prefix": "def adjust_brightness(img, brightness_factor):\n    \"\"\"Adjust brightness of an Image.\n\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        brightness_factor (float):  How much to adjust the brightness. Can be\n            any non negative number. 0 gives a black image, 1 gives the\n            original image while 2 increases the brightness by a factor of 2.\n", "suffix": "        PIL Image: Brightness adjusted image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Brightness(img)\n    img = enhancer.enhance(brightness_factor)\n    return img", "gt": "    Returns:"}
{"prefix": "def adjust_contrast(img, contrast_factor):\n    \"\"\"Adjust contrast of an Image.\n\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        contrast_factor (float): How much to adjust the contrast. Can be any\n            non negative number. 0 gives a solid gray image, 1 gives the", "suffix": "\n    Returns:\n        PIL Image: Contrast adjusted image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Contrast(img)\n    img = enhancer.enhance(contrast_factor)\n    return img", "gt": "            original image while 2 increases the contrast by a factor of 2."}
{"prefix": "def adjust_saturation(img, saturation_factor):\n    \"\"\"Adjust color saturation of an image.\n", "suffix": "        img (PIL Image): PIL Image to be adjusted.\n        saturation_factor (float):  How much to adjust the saturation. 0 will\n            give a black and white image, 1 will give the original image while\n            2 will enhance the saturation by a factor of 2.\n\n    Returns:\n        PIL Image: Saturation adjusted image.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Color(img)\n    img = enhancer.enhance(saturation_factor)\n    return img", "gt": "    Args:"}
{"prefix": "def adjust_hue(img, hue_factor):\n    \"\"\"Adjust hue of an image.\n\n    The image hue is adjusted by converting the image to HSV and\n    cyclically shifting the intensities in the hue channel (H).\n    The image is then converted back to original image mode.\n\n    `hue_factor` is the amount of shift in H channel and must be in the\n    interval `[-0.5, 0.5]`.\n\n    See `Hue`_ for more details.\n\n    .. _Hue: https://en.wikipedia.org/wiki/Hue\n\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        hue_factor (float):  How much to shift the hue channel. Should be in\n            [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in\n            HSV space in positive and negative direction respectively.\n            0 means no shift. Therefore, both -0.5 and 0.5 will give an image\n            with complementary colors while 0 gives the original image.\n\n    Returns:\n        PIL Image: Hue adjusted image.", "suffix": "    if not(-0.5 <= hue_factor <= 0.5):\n        raise ValueError('hue_factor is not in [-0.5, 0.5].'.format(hue_factor))\n\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    input_mode = img.mode\n    if input_mode in {'L', '1', 'I', 'F'}:\n        return img\n\n    h, s, v = img.convert('HSV').split()\n\n    np_h = np.array(h, dtype=np.uint8)\n    # uint8 addition take cares of rotation across boundaries\n    with np.errstate(over='ignore'):\n        np_h += np.uint8(hue_factor * 255)\n    h = Image.fromarray(np_h, 'L')\n\n    img = Image.merge('HSV', (h, s, v)).convert(input_mode)\n    return img", "gt": "    \"\"\""}
{"prefix": "def adjust_gamma(img, gamma, gain=1):\n    r\"\"\"Perform gamma correction on an image.\n\n    Also known as Power Law Transform. Intensities in RGB mode are adjusted\n    based on the following equation:\n\n    .. math::\n        I_{\\text{out}} = 255 \\times \\text{gain} \\times \\left(\\frac{I_{\\text{in}}}{255}\\right)^{\\gamma}\n\n    See `Gamma Correction`_ for more details.\n\n    .. _Gamma Correction: https://en.wikipedia.org/wiki/Gamma_correction\n\n    Args:\n        img (PIL Image): PIL Image to be adjusted.", "suffix": "            gamma larger than 1 make the shadows darker,\n            while gamma smaller than 1 make dark regions lighter.\n        gain (float): The constant multiplier.\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    if gamma < 0:\n        raise ValueError('Gamma should be a non-negative real number')\n\n    input_mode = img.mode\n    img = img.convert('RGB')\n\n    gamma_map = [255 * gain * pow(ele / 255., gamma) for ele in range(256)] * 3\n    img = img.point(gamma_map)  # use PIL's point-function to accelerate this part\n\n    img = img.convert(input_mode)\n    return img", "gt": "        gamma (float): Non negative real number, same as :math:`\\gamma` in the equation."}
{"prefix": "def rotate(img, angle, resample=False, expand=False, center=None):\n    \"\"\"Rotate the image by angle.\n\n\n    Args:\n        img (PIL Image): PIL Image to be rotated.\n        angle (float or int): In degrees degrees counter clockwise order.\n        resample (``PIL.Image.NEAREST`` or ``PIL.Image.BILINEAR`` or ``PIL.Image.BICUBIC``, optional):", "suffix": "            If omitted, or if the image has mode \"1\" or \"P\", it is set to ``PIL.Image.NEAREST``.\n        expand (bool, optional): Optional expansion flag.\n            If true, expands the output image to make it large enough to hold the entire rotated image.\n            If false or omitted, make the output image the same size as the input image.\n            Note that the expand flag assumes rotation around the center and no translation.\n        center (2-tuple, optional): Optional center of rotation.\n            Origin is the upper left corner.\n            Default is the center of the image.\n\n    .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters\n\n    \"\"\"\n\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    return img.rotate(angle, resample, expand, center)", "gt": "            An optional resampling filter. See `filters`_ for more information."}
{"prefix": "def affine(img, angle, translate, scale, shear, resample=0, fillcolor=None):\n    \"\"\"Apply affine transformation on the image keeping image center invariant\n\n    Args:\n        img (PIL Image): PIL Image to be rotated.\n        angle (float or int): rotation angle in degrees between -180 and 180, clockwise direction.\n        translate (list or tuple of integers): horizontal and vertical translations (post-rotation translation)\n        scale (float): overall scale\n        shear (float): shear angle value in degrees between -180 to 180, clockwise direction.\n        resample (``PIL.Image.NEAREST`` or ``PIL.Image.BILINEAR`` or ``PIL.Image.BICUBIC``, optional):\n            An optional resampling filter.\n            See `filters`_ for more information.", "suffix": "        fillcolor (int): Optional fill color for the area outside the transform in the output image. (Pillow>=5.0.0)\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    assert isinstance(translate, (tuple, list)) and len(translate) == 2, \\\n        \"Argument translate should be a list or tuple of length 2\"\n\n    assert scale > 0.0, \"Argument scale should be positive\"\n\n    output_size = img.size\n    center = (img.size[0] * 0.5 + 0.5, img.size[1] * 0.5 + 0.5)\n    matrix = _get_inverse_affine_matrix(center, angle, translate, scale, shear)\n    kwargs = {\"fillcolor\": fillcolor} if PILLOW_VERSION[0] == '5' else {}\n    return img.transform(output_size, Image.AFFINE, matrix, resample, **kwargs)", "gt": "            If omitted, or if the image has mode \"1\" or \"P\", it is set to ``PIL.Image.NEAREST``."}
{"prefix": "def to_grayscale(img, num_output_channels=1):\n    \"\"\"Convert image to grayscale version of image.\n\n    Args:\n        img (PIL Image): Image to be converted to grayscale.\n", "suffix": "        PIL Image: Grayscale version of the image.\n            if num_output_channels = 1 : returned image is single channel\n\n            if num_output_channels = 3 : returned image is 3 channel with r = g = b\n    \"\"\"\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    if num_output_channels == 1:\n        img = img.convert('L')\n    elif num_output_channels == 3:\n        img = img.convert('L')\n        np_img = np.array(img, dtype=np.uint8)\n        np_img = np.dstack([np_img, np_img, np_img])\n        img = Image.fromarray(np_img, 'RGB')\n    else:\n        raise ValueError('num_output_channels should be either 1 or 3')\n\n    return img", "gt": "    Returns:"}
{"prefix": "def make_grid(tensor, nrow=8, padding=2,\n              normalize=False, range=None, scale_each=False, pad_value=0):\n    \"\"\"Make a grid of images.\n\n    Args:\n        tensor (Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W)\n            or a list of images all of the same size.\n        nrow (int, optional): Number of images displayed in each row of the grid.\n            The Final grid size is (B / nrow, nrow). Default is 8.\n        padding (int, optional): amount of padding. Default is 2.\n        normalize (bool, optional): If True, shift the image to the range (0, 1),\n            by subtracting the minimum and dividing by the maximum pixel value.\n        range (tuple, optional): tuple (min, max) where min and max are numbers,\n            then these numbers are used to normalize the image. By default, min and max\n            are computed from the tensor.\n        scale_each (bool, optional): If True, scale each image in the batch of\n            images separately rather than the (min, max) over all images.\n        pad_value (float, optional): Value for the padded pixels.\n\n    Example:\n        See this notebook `here <https://gist.github.com/anonymous/bf16430f7750c023141c562f3e9f2a91>`_\n\n    \"\"\"\n    if not (torch.is_tensor(tensor) or", "suffix": "        raise TypeError('tensor or list of tensors expected, got {}'.format(type(tensor)))\n\n    # if list of tensors, convert to a 4D mini-batch Tensor\n    if isinstance(tensor, list):\n        tensor = torch.stack(tensor, dim=0)\n\n    if tensor.dim() == 2:  # single image H x W\n        tensor = tensor.unsqueeze(0)\n    if tensor.dim() == 3:  # single image\n        if tensor.size(0) == 1:  # if single-channel, convert to 3-channel\n            tensor = torch.cat((tensor, tensor, tensor), 0)\n        tensor = tensor.unsqueeze(0)\n\n    if tensor.dim() == 4 and tensor.size(1) == 1:  # single-channel images\n        tensor = torch.cat((tensor, tensor, tensor), 1)\n\n    if normalize is True:\n        tensor = tensor.clone()  # avoid modifying tensor in-place\n        if range is not None:\n            assert isinstance(range, tuple), \\\n                \"range has to be a tuple (min, max) if specified. min and max are numbers\"\n\n        def norm_ip(img, min, max):\n            img.clamp_(min=min, max=max)\n            img.add_(-min).div_(max - min + 1e-5)\n\n        def norm_range(t, range):\n            if range is not None:\n                norm_ip(t, range[0], range[1])\n            else:\n                norm_ip(t, float(t.min()), float(t.max()))\n\n        if scale_each is True:\n            for t in tensor:  # loop over mini-batch dimension\n                norm_range(t, range)\n        else:\n            norm_range(tensor, range)\n\n    if tensor.size(0) == 1:\n        return tensor.squeeze()\n\n    # make the mini-batch of images into a grid\n    nmaps = tensor.size(0)\n    xmaps = min(nrow, nmaps)\n    ymaps = int(math.ceil(float(nmaps) / xmaps))\n    height, width = int(tensor.size(2) + padding), int(tensor.size(3) + padding)\n    grid = tensor.new_full((3, height * ymaps + padding, width * xmaps + padding), pad_value)\n    k = 0\n    for y in irange(ymaps):\n        for x in irange(xmaps):\n            if k >= nmaps:\n                break\n            grid.narrow(1, y * height + padding, height - padding)\\\n                .narrow(2, x * width + padding, width - padding)\\\n                .copy_(tensor[k])\n            k = k + 1\n    return grid", "gt": "            (isinstance(tensor, list) and all(torch.is_tensor(t) for t in tensor))):"}
{"prefix": "def save_image(tensor, filename, nrow=8, padding=2,\n               normalize=False, range=None, scale_each=False, pad_value=0):\n    \"\"\"Save a given Tensor into an image file.\n\n    Args:\n        tensor (Tensor or list): Image to be saved. If given a mini-batch tensor,\n            saves the tensor as a grid of images by calling ``make_grid``.", "suffix": "    \"\"\"\n    from PIL import Image\n    grid = make_grid(tensor, nrow=nrow, padding=padding, pad_value=pad_value,\n                     normalize=normalize, range=range, scale_each=scale_each)\n    # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\n    ndarr = grid.mul_(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n    im = Image.fromarray(ndarr)\n    im.save(filename)", "gt": "        **kwargs: Other arguments are documented in ``make_grid``."}
{"prefix": "", "suffix": "        \"\"\"\n        Finds the class folders in a dataset.\n\n        Args:\n            dir (string): Root directory path.\n\n        Returns:\n            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n\n        Ensures:\n            No class is a subdirectory of another.\n        \"\"\"\n        if sys.version_info >= (3, 5):\n            # Faster and available in Python 3.5 and above\n            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n        else:\n            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n        classes.sort()\n        class_to_idx = {classes[i]: i for i in range(len(classes))}\n        return classes, class_to_idx", "gt": "def _find_classes(self, dir):"}
{"prefix": "def read_image_file(data_dir, image_ext, n):\n    \"\"\"Return a Tensor containing the patches\n    \"\"\"\n\n    def PIL2array(_img):\n        \"\"\"Convert PIL image type to numpy 2D array\n        \"\"\"\n        return np.array(_img.getdata(), dtype=np.uint8).reshape(64, 64)\n\n    def find_files(_data_dir, _image_ext):\n        \"\"\"Return a list with the file names of the images containing the patches\n        \"\"\"\n        files = []\n        # find those files with the specified extension\n        for file_dir in os.listdir(_data_dir):\n            if file_dir.endswith(_image_ext):\n                files.append(os.path.join(_data_dir, file_dir))\n        return sorted(files)  # sort files in ascend order to keep relations\n\n    patches = []\n    list_files = find_files(data_dir, image_ext)\n", "suffix": "        img = Image.open(fpath)\n        for y in range(0, 1024, 64):\n            for x in range(0, 1024, 64):\n                patch = img.crop((x, y, x + 64, y + 64))\n                patches.append(PIL2array(patch))\n    return torch.ByteTensor(np.array(patches[:n]))", "gt": "    for fpath in list_files:"}
{"prefix": "def read_info_file(data_dir, info_file):\n    \"\"\"Return a Tensor containing the list of labels\n       Read the file and keep only the ID of the 3D point.\n    \"\"\"", "suffix": "    with open(os.path.join(data_dir, info_file), 'r') as f:\n        labels = [int(line.split()[0]) for line in f]\n    return torch.LongTensor(labels)", "gt": "    labels = []"}
{"prefix": "def read_matches_files(data_dir, matches_file):\n    \"\"\"Return a Tensor containing the ground truth matches", "suffix": "       Matches are represented with a 1, non matches with a 0.\n    \"\"\"\n    matches = []\n    with open(os.path.join(data_dir, matches_file), 'r') as f:\n        for line in f:\n            line_split = line.split()\n            matches.append([int(line_split[0]), int(line_split[3]),\n                            int(line_split[1] == line_split[4])])\n    return torch.LongTensor(matches)", "gt": "       Read the file and keep only 3D point ID."}
{"prefix": "def conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"", "suffix": "", "gt": "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"}
{"prefix": "def accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target[None])\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].flatten().sum(dtype=torch.float32)", "suffix": "        return res", "gt": "            res.append(correct_k * (100.0 / batch_size))"}
{"prefix": "def setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n", "suffix": "", "gt": "    __builtin__.print = print"}
{"prefix": "def synchronize_between_processes(self):\n        \"\"\"\n        Warning: does not synchronize the deque!", "suffix": "        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]", "gt": "        \"\"\""}
{"prefix": "def squeezenet1_1(pretrained=False, **kwargs):\n    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n    than SqueezeNet 1.0, without sacrificing accuracy.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet", "suffix": "    model = SqueezeNet(version=1.1, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['squeezenet1_1']))\n    return model", "gt": "    \"\"\""}
{"prefix": "def makedir_exist_ok(dirpath):\n    \"\"\"\n    Python2 support for os.makedirs(.., exist_ok=True)", "suffix": "    try:\n        os.makedirs(dirpath)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            pass\n        else:\n            raise", "gt": "    \"\"\""}
{"prefix": "def download_url(url, root, filename=None, md5=None):\n    \"\"\"Download a file from a url and place it in root.\n\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    \"\"\"\n    from six.moves import urllib\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    # downloads file\n    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n        print('Using downloaded and verified file: ' + fpath)\n    else:\n        try:\n            print('Downloading ' + url + ' to ' + fpath)\n            urllib.request.urlretrieve(\n                url, fpath,\n                reporthook=gen_bar_updater()\n            )\n        except OSError:\n            if url[:5] == 'https':\n                url = url.replace('https:', 'http:')\n                print('Failed download. Trying https -> http instead.'", "suffix": "                urllib.request.urlretrieve(\n                    url, fpath,\n                    reporthook=gen_bar_updater()\n                )", "gt": "                      ' Downloading ' + url + ' to ' + fpath)"}
{"prefix": "def list_dir(root, prefix=False):\n    \"\"\"List all directories at a given root\n\n    Args:\n        root (str): Path to directory whose folders need to be listed", "suffix": "            only returns the name of the directories found\n    \"\"\"\n    root = os.path.expanduser(root)\n    directories = list(\n        filter(\n            lambda p: os.path.isdir(os.path.join(root, p)),\n            os.listdir(root)\n        )\n    )\n\n    if prefix is True:\n        directories = [os.path.join(root, d) for d in directories]\n\n    return directories", "gt": "        prefix (bool, optional): If true, prepends the path to each result, otherwise"}
{"prefix": "def list_files(root, suffix, prefix=False):\n    \"\"\"List all files ending with a suffix at a given root\n\n    Args:\n        root (str): Path to directory whose folders need to be listed\n        suffix (str or tuple): Suffix of the files to match, e.g. '.png' or ('.jpg', '.png').\n            It uses the Python \"str.endswith\" method and is passed directly", "suffix": "            only returns the name of the files found\n    \"\"\"\n    root = os.path.expanduser(root)\n    files = list(\n        filter(\n            lambda p: os.path.isfile(os.path.join(root, p)) and p.endswith(suffix),\n            os.listdir(root)\n        )\n    )\n\n    if prefix is True:\n        files = [os.path.join(root, d) for d in files]\n\n    return files", "gt": "        prefix (bool, optional): If true, prepends the path to each result, otherwise"}
{"prefix": "def download_file_from_google_drive(file_id, root, filename=None, md5=None):\n    \"\"\"Download a Google Drive file from  and place it in root.\n\n    Args:\n        file_id (str): id of file to be downloaded\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the id of the file.\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    \"\"\"\n    # Based on https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n    import requests\n    url = \"https://docs.google.com/uc?export=download\"\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = file_id\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n        print('Using downloaded and verified file: ' + fpath)\n    else:\n        session = requests.Session()\n\n        response = session.get(url, params={'id': file_id}, stream=True)\n        token = _get_confirm_token(response)", "suffix": "        if token:\n            params = {'id': file_id, 'confirm': token}\n            response = session.get(url, params=params, stream=True)\n\n        _save_response_content(response, fpath)", "gt": ""}
{"prefix": "def get_params(img, output_size):\n        \"\"\"Get parameters for ``crop`` for a random crop.\n\n        Args:\n            img (PIL Image): Image to be cropped.\n            output_size (tuple): Expected output size of the crop.\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n        \"\"\"\n        w, h = img.size", "suffix": "        if w == tw and h == th:\n            return 0, 0, h, w\n\n        i = random.randint(0, h - th)\n        j = random.randint(0, w - tw)\n        return i, j, th, tw", "gt": "        th, tw = output_size"}
{"prefix": "def get_params(width, height, distortion_scale):\n        \"\"\"Get parameters for ``perspective`` for a random perspective transform.\n\n        Args:\n            width : width of the image.\n            height : height of the image.\n\n        Returns:\n            List containing [top-left, top-right, bottom-right, bottom-left] of the orignal image,\n            List containing [top-left, top-right, bottom-right, bottom-left] of the transformed image.\n        \"\"\"\n        half_height = int(height / 2)\n        half_width = int(width / 2)\n        topleft = (random.randint(0, int(distortion_scale * half_width)),\n                   random.randint(0, int(distortion_scale * half_height)))\n        topright = (random.randint(width - int(distortion_scale * half_width) - 1, width - 1),\n                    random.randint(0, int(distortion_scale * half_height)))", "suffix": "                    random.randint(height - int(distortion_scale * half_height) - 1, height - 1))\n        botleft = (random.randint(0, int(distortion_scale * half_width)),\n                   random.randint(height - int(distortion_scale * half_height) - 1, height - 1))\n        startpoints = [(0, 0), (width - 1, 0), (width - 1, height - 1), (0, height - 1)]\n        endpoints = [topleft, topright, botright, botleft]\n        return startpoints, endpoints", "gt": "        botright = (random.randint(width - int(distortion_scale * half_width) - 1, width - 1),"}
{"prefix": "def get_params(img, scale, ratio):\n        \"\"\"Get parameters for ``crop`` for a random sized crop.\n\n        Args:\n            img (PIL Image): Image to be cropped.\n            scale (tuple): range of size of the origin size cropped\n            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n                sized crop.\n        \"\"\"\n        area = img.size[0] * img.size[1]\n\n        for attempt in range(10):\n            target_area = random.uniform(*scale) * area\n            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n            aspect_ratio = math.exp(random.uniform(*log_ratio))\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n", "suffix": "                i = random.randint(0, img.size[1] - h)\n                j = random.randint(0, img.size[0] - w)\n                return i, j, h, w\n\n        # Fallback to central crop\n        in_ratio = img.size[0] / img.size[1]\n        if (in_ratio < min(ratio)):\n            w = img.size[0]\n            h = w / min(ratio)\n        elif (in_ratio > max(ratio)):\n            h = img.size[1]\n            w = h * max(ratio)\n        else:  # whole image\n            w = img.size[0]\n            h = img.size[1]\n        i = (img.size[1] - h) // 2\n        j = (img.size[0] - w) // 2\n        return i, j, h, w", "gt": "            if w <= img.size[0] and h <= img.size[1]:"}
{"prefix": "def get_params(brightness, contrast, saturation, hue):\n        \"\"\"Get a randomized transform to be applied on image.\n\n        Arguments are same as that of __init__.\n\n        Returns:\n            Transform which randomly adjusts brightness, contrast and", "suffix": "        \"\"\"\n        transforms = []\n\n        if brightness is not None:\n            brightness_factor = random.uniform(brightness[0], brightness[1])\n            transforms.append(Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n\n        if contrast is not None:\n            contrast_factor = random.uniform(contrast[0], contrast[1])\n            transforms.append(Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n\n        if saturation is not None:\n            saturation_factor = random.uniform(saturation[0], saturation[1])\n            transforms.append(Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n\n        if hue is not None:\n            hue_factor = random.uniform(hue[0], hue[1])\n            transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n\n        random.shuffle(transforms)\n        transform = Compose(transforms)\n\n        return transform", "gt": "            saturation in a random order."}
{"prefix": "def get_params(degrees, translate, scale_ranges, shears, img_size):", "suffix": "\n        Returns:\n            sequence: params to be passed to the affine transformation\n        \"\"\"\n        angle = random.uniform(degrees[0], degrees[1])\n        if translate is not None:\n            max_dx = translate[0] * img_size[0]\n            max_dy = translate[1] * img_size[1]\n            translations = (np.round(random.uniform(-max_dx, max_dx)),\n                            np.round(random.uniform(-max_dy, max_dy)))\n        else:\n            translations = (0, 0)\n\n        if scale_ranges is not None:\n            scale = random.uniform(scale_ranges[0], scale_ranges[1])\n        else:\n            scale = 1.0\n\n        if shears is not None:\n            shear = random.uniform(shears[0], shears[1])\n        else:\n            shear = 0.0\n\n        return angle, translations, scale, shear", "gt": "        \"\"\"Get parameters for affine transformation"}
{"prefix": "def inception_v3(pretrained=False, **kwargs):\n    r\"\"\"Inception v3 model architecture from", "suffix": "\n    .. note::\n        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n        N x 3 x 299 x 299, so ensure your images are sized accordingly.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        aux_logits (bool): If True, add an auxiliary branch that can improve training.\n            Default: *True*\n        transform_input (bool): If True, preprocesses the input according to the method with which it\n            was trained on ImageNet. Default: *False*\n    \"\"\"\n    if pretrained:\n        if 'transform_input' not in kwargs:\n            kwargs['transform_input'] = True\n        if 'aux_logits' in kwargs:\n            original_aux_logits = kwargs['aux_logits']\n            kwargs['aux_logits'] = True\n        else:\n            original_aux_logits = True\n        model = Inception3(**kwargs)\n        model.load_state_dict(model_zoo.load_url(model_urls['inception_v3_google']))\n        if not original_aux_logits:\n            model.aux_logits = False\n            del model.AuxLogits\n        return model\n\n    return Inception3(**kwargs)", "gt": "    `\"Rethinking the Inception Architecture for Computer Vision\" <http://arxiv.org/abs/1512.00567>`_."}
{"prefix": "def download(self):\n        \"\"\"Download and extract the tarball, and download each individual photo.\"\"\"\n        import tarfile\n\n        if self._check_integrity():\n            print('Files already downloaded and verified')\n            return\n\n        download_url(self.url, self.root, self.filename, self.md5_checksum)\n\n        # Extract file\n        with tarfile.open(os.path.join(self.root, self.filename), 'r:gz') as tar:\n            tar.extractall(path=self.root)\n\n        # Download individual photos\n        with open(os.path.join(self.root, 'dataset', 'SBU_captioned_photo_dataset_urls.txt')) as fh:\n            for line in fh:\n                url = line.rstrip()\n                try:\n                    download_url(url, os.path.join(self.root, 'dataset'))\n                except OSError:\n                    # The images point to public images on Flickr.", "suffix": "                    pass", "gt": "                    # Note: Images might be removed by users at anytime."}
{"prefix": "def googlenet(pretrained=False, **kwargs):\n    r\"\"\"GoogLeNet (Inception v1) model architecture from\n    `\"Going Deeper with Convolutions\" <http://arxiv.org/abs/1409.4842>`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        aux_logits (bool): If True, adds two auxiliary branches that can improve training.\n            Default: *False* when pretrained is True otherwise *True*\n        transform_input (bool): If True, preprocesses the input according to the method with which it\n            was trained on ImageNet. Default: *False*\n    \"\"\"", "suffix": "        if 'transform_input' not in kwargs:\n            kwargs['transform_input'] = True\n        if 'aux_logits' not in kwargs:\n            kwargs['aux_logits'] = False\n        if kwargs['aux_logits']:\n            warnings.warn('auxiliary heads in the pretrained googlenet model are NOT pretrained, '\n                          'so make sure to train them')\n        original_aux_logits = kwargs['aux_logits']\n        kwargs['aux_logits'] = True\n        kwargs['init_weights'] = False\n        model = GoogLeNet(**kwargs)\n        model.load_state_dict(model_zoo.load_url(model_urls['googlenet']))\n        if not original_aux_logits:\n            model.aux_logits = False\n            del model.aux1, model.aux2\n        return model\n\n    return GoogLeNet(**kwargs)", "gt": "    if pretrained:"}
{"prefix": "def download(self):\n        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n\n        if self._check_exists():\n            return\n\n        makedir_exist_ok(self.raw_folder)\n        makedir_exist_ok(self.processed_folder)\n\n        # download files\n        for url in self.urls:", "suffix": "            file_path = os.path.join(self.raw_folder, filename)\n            download_url(url, root=self.raw_folder, filename=filename, md5=None)\n            self.extract_gzip(gzip_path=file_path, remove_finished=True)\n\n        # process and save as torch files\n        print('Processing...')\n\n        training_set = (\n            read_image_file(os.path.join(self.raw_folder, 'train-images-idx3-ubyte')),\n            read_label_file(os.path.join(self.raw_folder, 'train-labels-idx1-ubyte'))\n        )\n        test_set = (\n            read_image_file(os.path.join(self.raw_folder, 't10k-images-idx3-ubyte')),\n            read_label_file(os.path.join(self.raw_folder, 't10k-labels-idx1-ubyte'))\n        )\n        with open(os.path.join(self.processed_folder, self.training_file), 'wb') as f:\n            torch.save(training_set, f)\n        with open(os.path.join(self.processed_folder, self.test_file), 'wb') as f:\n            torch.save(test_set, f)\n\n        print('Done!')", "gt": "            filename = url.rpartition('/')[2]"}
{"prefix": "def download(self):\n        \"\"\"Download the EMNIST data if it doesn't exist in processed_folder already.\"\"\"\n        import shutil\n        import zipfile\n\n        if self._check_exists():\n            return\n\n        makedir_exist_ok(self.raw_folder)\n        makedir_exist_ok(self.processed_folder)\n\n        # download files\n        filename = self.url.rpartition('/')[2]\n        file_path = os.path.join(self.raw_folder, filename)\n        download_url(self.url, root=self.raw_folder, filename=filename, md5=None)\n\n        print('Extracting zip archive')\n        with zipfile.ZipFile(file_path) as zip_f:\n            zip_f.extractall(self.raw_folder)\n        os.unlink(file_path)\n        gzip_folder = os.path.join(self.raw_folder, 'gzip')\n        for gzip_file in os.listdir(gzip_folder):\n            if gzip_file.endswith('.gz'):\n                self.extract_gzip(gzip_path=os.path.join(gzip_folder, gzip_file))\n\n        # process and save as torch files\n        for split in self.splits:\n            print('Processing ' + split)\n            training_set = (\n                read_image_file(os.path.join(gzip_folder, 'emnist-{}-train-images-idx3-ubyte'.format(split))),\n                read_label_file(os.path.join(gzip_folder, 'emnist-{}-train-labels-idx1-ubyte'.format(split)))\n            )\n            test_set = (\n                read_image_file(os.path.join(gzip_folder, 'emnist-{}-test-images-idx3-ubyte'.format(split))),\n                read_label_file(os.path.join(gzip_folder, 'emnist-{}-test-labels-idx1-ubyte'.format(split)))\n            )\n            with open(os.path.join(self.processed_folder, self._training_file(split)), 'wb') as f:\n                torch.save(training_set, f)", "suffix": "                torch.save(test_set, f)\n        shutil.rmtree(gzip_folder)\n\n        print('Done!')", "gt": "            with open(os.path.join(self.processed_folder, self._test_file(split)), 'wb') as f:"}
{"prefix": "def request(method, url, **kwargs):\n    \"\"\"same as requests/requests/api.py request(...)\"\"\"\n    time_before_request = time()", "suffix": "    # session start\n    session = SessionSinglePool()\n\n    # proxies\n    kwargs['proxies'] = settings['outgoing'].get('proxies') or None\n\n    # timeout\n    if 'timeout' in kwargs:\n        timeout = kwargs['timeout']\n    else:\n        timeout = getattr(threadLocal, 'timeout', None)\n        if timeout is not None:\n            kwargs['timeout'] = timeout\n\n    # do request\n    response = session.request(method=method, url=url, **kwargs)\n\n    time_after_request = time()\n\n    # is there a timeout for this engine ?\n    if timeout is not None:\n        timeout_overhead = 0.2  # seconds\n        # start_time = when the user request started\n        start_time = getattr(threadLocal, 'start_time', time_before_request)\n        search_duration = time_after_request - start_time\n        if search_duration > timeout + timeout_overhead:\n            raise requests.exceptions.Timeout(response=response)\n\n    # session end\n    session.close()\n\n    if hasattr(threadLocal, 'total_time'):\n        threadLocal.total_time += time_after_request - time_before_request\n\n    return response", "gt": ""}
{"prefix": "", "suffix": "    \"\"\"Returns theme name.\n\n    Checks in this order:\n    1. override\n    2. cookies\n    3. settings\"\"\"\n\n    if override and (override in themes or override == '__common__'):\n        return override\n    theme_name = request.args.get('theme', request.preferences.get_value('theme'))\n    if theme_name not in themes:\n        theme_name = default_theme\n    return theme_name", "gt": "def get_current_theme_name(override=None):"}
{"prefix": "def index():\n    \"\"\"Render index page.\n\n    Supported outputs: html, json, csv, rss.\n    \"\"\"\n\n    # output_format\n    output_format = request.form.get('format', 'html')\n    if output_format not in ['html', 'csv', 'json', 'rss']:\n        output_format = 'html'\n\n    # check if there is query\n    if request.form.get('q') is None:\n        if output_format == 'html':\n            return render(\n                'index.html',\n            )\n        else:\n            return index_error(output_format, 'No query'), 400\n\n    # search\n    search_query = None\n    result_container = None\n    try:\n        search_query = get_search_query_from_webapp(request.preferences, request.form)\n        # search = Search(search_query) #  without plugins\n        search = SearchWithPlugins(search_query, request.user_plugins, request)\n        result_container = search.search()\n    except Exception as e:\n        # log exception\n        logger.exception('search error')\n\n        # is it an invalid input parameter or something else ?\n        if (issubclass(e.__class__, SearxParameterException)):\n            return index_error(output_format, e.message), 400\n        else:\n            return index_error(output_format, gettext('search error')), 500\n\n    # results\n    results = result_container.get_ordered_results()\n    number_of_results = result_container.results_number()\n    if number_of_results < result_container.results_length():\n        number_of_results = 0\n\n    # UI\n    advanced_search = request.form.get('advanced_search', None)\n\n    # output\n    for result in results:\n        if output_format == 'html':\n            if 'content' in result and result['content']:\n                result['content'] = highlight_content(escape(result['content'][:1024]), search_query.query)\n            result['title'] = highlight_content(escape(result['title'] or u''), search_query.query)\n        else:\n            if result.get('content'):\n                result['content'] = html_to_text(result['content']).strip()\n            # removing html content and whitespace duplications\n            result['title'] = ' '.join(html_to_text(result['title']).strip().split())\n\n        result['pretty_url'] = prettify_url(result['url'])\n\n        # TODO, check if timezone is calculated right\n        if 'publishedDate' in result:\n            try:  # test if publishedDate >= 1900 (datetime module bug)\n                result['pubdate'] = result['publishedDate'].strftime('%Y-%m-%d %H:%M:%S%z')\n            except ValueError:\n                result['publishedDate'] = None\n            else:\n                if result['publishedDate'].replace(tzinfo=None) >= datetime.now() - timedelta(days=1):\n                    timedifference = datetime.now() - result['publishedDate'].replace(tzinfo=None)\n                    minutes = int((timedifference.seconds / 60) % 60)\n                    hours = int(timedifference.seconds / 60 / 60)\n                    if hours == 0:\n                        result['publishedDate'] = gettext(u'{minutes} minute(s) ago').format(minutes=minutes)\n                    else:\n                        result['publishedDate'] = gettext(u'{hours} hour(s), {minutes} minute(s) ago').format(hours=hours, minutes=minutes)  # noqa\n                else:\n                    result['publishedDate'] = format_date(result['publishedDate'])\n\n    if output_format == 'json':\n        return Response(json.dumps({'query': search_query.query.decode('utf-8'),\n                                    'number_of_results': number_of_results,\n                                    'results': results,\n                                    'answers': list(result_container.answers),\n                                    'corrections': list(result_container.corrections),\n                                    'infoboxes': result_container.infoboxes,\n                                    'suggestions': list(result_container.suggestions),\n                                    'unresponsive_engines': list(result_container.unresponsive_engines)},\n                                   default=lambda item: list(item) if isinstance(item, set) else item),\n                        mimetype='application/json')\n    elif output_format == 'csv':\n        csv = UnicodeWriter(StringIO())\n        keys = ('title', 'url', 'content', 'host', 'engine', 'score')\n        csv.writerow(keys)\n        for row in results:\n            row['host'] = row['parsed_url'].netloc\n            csv.writerow([row.get(key, '') for key in keys])\n        csv.stream.seek(0)\n        response = Response(csv.stream.read(), mimetype='application/csv')\n        cont_disp = 'attachment;Filename=searx_-_{0}.csv'.format(search_query.query)\n        response.headers.add('Content-Disposition', cont_disp)\n        return response\n    elif output_format == 'rss':\n        response_rss = render(\n            'opensearch_response_rss.xml',\n            results=results,\n            q=request.form['q'],", "suffix": "            base_url=get_base_url(),\n            override_theme='__common__',\n        )\n        return Response(response_rss, mimetype='text/xml')\n\n    return render(\n        'results.html',\n        results=results,\n        q=request.form['q'],\n        selected_categories=search_query.categories,\n        pageno=search_query.pageno,\n        time_range=search_query.time_range,\n        number_of_results=format_decimal(number_of_results),\n        advanced_search=advanced_search,\n        suggestions=result_container.suggestions,\n        answers=result_container.answers,\n        corrections=result_container.corrections,\n        infoboxes=result_container.infoboxes,\n        paging=result_container.paging,\n        unresponsive_engines=result_container.unresponsive_engines,\n        current_language=match_language(search_query.lang,\n                                        LANGUAGE_CODES,\n                                        fallback=settings['search']['language']),\n        base_url=get_base_url(),\n        theme=get_current_theme_name(),\n        favicons=global_favicons[themes.index(get_current_theme_name())]\n    )", "gt": "            number_of_results=number_of_results,"}
{"prefix": "def autocompleter():\n    \"\"\"Return autocompleter results\"\"\"\n\n    # set blocked engines\n    disabled_engines = request.preferences.engines.get_disabled()\n\n    # parse query\n    if PY3:\n        raw_text_query = RawTextQuery(request.form.get('q', b''), disabled_engines)\n    else:\n        raw_text_query = RawTextQuery(request.form.get('q', u'').encode('utf-8'), disabled_engines)\n    raw_text_query.parse_query()", "suffix": "    # check if search query is set\n    if not raw_text_query.getSearchQuery():\n        return '', 400\n\n    # run autocompleter\n    completer = autocomplete_backends.get(request.preferences.get_value('autocomplete'))\n\n    # parse searx specific autocompleter results like !bang\n    raw_results = searx_bang(raw_text_query)\n\n    # normal autocompletion results only appear if max 3 inner results returned\n    if len(raw_results) <= 3 and completer:\n        # get language from cookie\n        language = request.preferences.get_value('language')\n        if not language or language == 'all':\n            language = 'en'\n        else:\n            language = language.split('-')[0]\n        # run autocompletion\n        raw_results.extend(completer(raw_text_query.getSearchQuery(), language))\n\n    # parse results (write :language and !engine back to result string)\n    results = []\n    for result in raw_results:\n        raw_text_query.changeSearchQuery(result)\n\n        # add parsed result\n        results.append(raw_text_query.getFullQuery())\n\n    # return autocompleter results\n    if request.form.get('format') == 'x-suggestions':\n        return Response(json.dumps([raw_text_query.query, results]),\n                        mimetype='application/json')\n\n    return Response(json.dumps(results),\n                    mimetype='application/json')", "gt": ""}
{"prefix": "def preferences():\n    \"\"\"Render preferences page && save user preferences\"\"\"\n\n    # save preferences\n    if request.method == 'POST':\n        resp = make_response(redirect(urljoin(settings['server']['base_url'], url_for('index'))))\n        try:\n            request.preferences.parse_form(request.form)\n        except ValidationException:\n            request.errors.append(gettext('Invalid settings, please edit your preferences'))\n            return resp\n        return request.preferences.save(resp)\n\n    # render preferences\n    image_proxy = request.preferences.get_value('image_proxy')\n    lang = request.preferences.get_value('language')\n    disabled_engines = request.preferences.engines.get_disabled()\n    allowed_plugins = request.preferences.plugins.get_enabled()\n\n    # stats for preferences page\n    stats = {}\n\n    for c in categories:\n        for e in categories[c]:\n            stats[e.name] = {'time': None,\n                             'warn_timeout': False,\n                             'warn_time': False}\n            if e.timeout > settings['outgoing']['request_timeout']:\n                stats[e.name]['warn_timeout'] = True\n            stats[e.name]['supports_selected_language'] = _is_selected_language_supported(e, request.preferences)\n", "suffix": "    # and then the second element [1] : the time (the first one is the label)\n    for engine_stat in get_engines_stats()[0][1]:\n        stats[engine_stat.get('name')]['time'] = round(engine_stat.get('avg'), 3)\n        if engine_stat.get('avg') > settings['outgoing']['request_timeout']:\n            stats[engine_stat.get('name')]['warn_time'] = True\n    # end of stats\n\n    return render('preferences.html',\n                  locales=settings['locales'],\n                  current_locale=get_locale(),\n                  image_proxy=image_proxy,\n                  engines_by_category=categories,\n                  stats=stats,\n                  answerers=[{'info': a.self_info(), 'keywords': a.keywords} for a in answerers],\n                  disabled_engines=disabled_engines,\n                  autocomplete_backends=autocomplete_backends,\n                  shortcuts={y: x for x, y in engine_shortcuts.items()},\n                  themes=themes,\n                  plugins=plugins,\n                  doi_resolvers=settings['doi_resolvers'],\n                  current_doi_resolver=get_doi_resolver(request.args, request.preferences.get_value('doi_resolver')),\n                  allowed_plugins=allowed_plugins,\n                  theme=get_current_theme_name(),\n                  preferences_url_params=request.preferences.get_as_url_params(),\n                  base_url=get_base_url(),\n                  preferences=True)", "gt": "    # get first element [0], the engine time,"}
{"prefix": "def request(query, params):\n    '''pre-request callback\n    params<dict>:\n      method  : POST/GET\n      headers : {}\n      data    : {} # if method == POST\n      url     : ''", "suffix": "      pageno  : 1 # number of the requested page\n    '''\n\n    offset = (params['pageno'] - 1)\n    params['url'] = search_url.format(offset=offset, query=quote(query))\n    return params", "gt": "      category: 'search category'"}
{"prefix": "def response(resp):\n    '''post-response callback\n    resp: requests response object\n    '''\n    results = []\n\n    dom = html.fromstring(resp.text)\n\n    try:\n        number_of_results_string = re.sub('[^0-9]', '', dom.xpath(\n            '//a[@class=\"active\" and contains(@href,\"/suchen/dudenonline\")]/span/text()')[0]\n        )\n\n        results.append({'number_of_results': int(number_of_results_string)})\n\n    except:\n        logger.debug(\"Couldn't read number of results.\")\n        pass\n\n    for result in dom.xpath('//section[@class=\"wide\" and not(contains(@style,\"overflow:hidden\"))]'):\n        try:\n            logger.debug(\"running for %s\" % str(result))\n            link = result.xpath('.//h2/a')[0]\n            url = link.attrib.get('href')\n            title = result.xpath('string(.//h2/a)')\n            content = extract_text(result.xpath('.//p'))\n            # append result\n            results.append({'url': url,\n                            'title': title,\n                            'content': content})", "suffix": "            logger.debug('result parse error in:\\n%s', etree.tostring(result, pretty_print=True))\n            continue\n\n    return results", "gt": "        except:"}
{"prefix": "def get_themes(templates_path):", "suffix": "    themes = os.listdir(templates_path)\n    if '__common__' in themes:\n        themes.remove('__common__')\n    return themes", "gt": "    \"\"\"Returns available themes list.\"\"\""}
{"prefix": "def searx_bang(full_query):\n    '''check if the searchQuery contain a bang, and create fitting autocompleter results'''\n    # check if there is a query which can be parsed\n    if len(full_query.getSearchQuery()) == 0:\n        return []\n\n    results = []\n\n    # check if current query stats with !bang\n    first_char = full_query.getSearchQuery()[0]\n    if first_char == '!' or first_char == '?':\n        if len(full_query.getSearchQuery()) == 1:\n            # show some example queries\n            # TODO, check if engine is not avaliable\n            results.append(first_char + \"images\")\n            results.append(first_char + \"wikipedia\")\n            results.append(first_char + \"osm\")\n        else:\n            engine_query = full_query.getSearchQuery()[1:]\n\n            # check if query starts with categorie name\n            for categorie in categories:\n                if categorie.startswith(engine_query):\n                    results.append(first_char + '{categorie}'.format(categorie=categorie))\n\n            # check if query starts with engine name\n            for engine in engines:\n                if engine.startswith(engine_query.replace('_', ' ')):\n                    results.append(first_char + '{engine}'.format(engine=engine.replace(' ', '_')))\n\n            # check if query starts with engine shortcut\n            for engine_shortcut in engine_shortcuts:\n                if engine_shortcut.startswith(engine_query):\n                    results.append(first_char + '{engine_shortcut}'.format(engine_shortcut=engine_shortcut))\n\n    # check if current query stats with :bang\n    elif first_char == ':':\n        if len(full_query.getSearchQuery()) == 1:\n            # show some example queries\n            results.append(\":en\")\n            results.append(\":en_us\")\n            results.append(\":english\")\n            results.append(\":united_kingdom\")\n        else:\n            engine_query = full_query.getSearchQuery()[1:]\n\n            for lc in language_codes:\n                lang_id, lang_name, country, english_name = map(unicode.lower, lc)\n\n                # check if query starts with language-id\n                if lang_id.startswith(engine_query):\n                    if len(engine_query) <= 2:\n                        results.append(u':{lang_id}'.format(lang_id=lang_id.split('-')[0]))\n                    else:\n                        results.append(u':{lang_id}'.format(lang_id=lang_id))\n", "suffix": "                if lang_name.startswith(engine_query) or english_name.startswith(engine_query):\n                    results.append(u':{lang_name}'.format(lang_name=lang_name))\n\n                # check if query starts with country\n                if country.startswith(engine_query.replace('_', ' ')):\n                    results.append(u':{country}'.format(country=country.replace(' ', '_')))\n\n    # remove duplicates\n    result_set = set(results)\n\n    # remove results which are already contained in the query\n    for query_part in full_query.query_parts:\n        if query_part in result_set:\n            result_set.remove(query_part)\n\n    # convert result_set back to list\n    return list(result_set)", "gt": "                # check if query starts with language name"}
{"prefix": "def response(resp):\n    \"\"\"remove first and last lines to get only json\"\"\"\n    json_resp = resp.text[resp.text.find('\\n') + 1:resp.text.rfind('\\n') - 2]\n    results = []\n    try:\n        conversion_rate = float(json.loads(json_resp)['conversion']['converted-amount'])\n    except:\n        return results\n    answer = '{0} {1} = {2} {3}, 1 {1} ({5}) = {4} {3} ({6})'.format(\n        resp.search_params['amount'],\n        resp.search_params['from'],\n        resp.search_params['amount'] * conversion_rate,\n        resp.search_params['to'],\n        conversion_rate,\n        resp.search_params['from_name'],\n        resp.search_params['to_name'],\n    )\n\n    url = 'https://duckduckgo.com/js/spice/currency/1/{0}/{1}'.format(\n        resp.search_params['from'].upper(), resp.search_params['to'])\n\n    results.append({'answer': answer, 'url': url})\n", "suffix": "", "gt": "    return results"}
{"prefix": "def custom_gradient(fx, gx, x, fx_gx_manually_stopped=False, name=None):\n  \"\"\"Embeds a custom gradient into a `Tensor`.\n\n  This function works by clever application of `stop_gradient`. I.e., observe\n  that:\n\n  ```none\n  h(x) = stop_gradient(f(x)) + stop_gradient(g(x)) * (x - stop_gradient(x))\n  ```\n\n  is such that `h(x) == stop_gradient(f(x))` and\n  `grad[h(x), x] == stop_gradient(g(x)).`\n\n  In addition to scalar-domain/scalar-range functions, this function also\n  supports tensor-domain/scalar-range functions.\n\n  Partial Custom Gradient:\n\n  Suppose `h(x) = htilde(x, y)`. Note that `dh/dx = stop(g(x))` but `dh/dy =\n  None`. This is because a `Tensor` cannot have only a portion of its gradient\n  stopped. To circumvent this issue, one must manually `stop_gradient` the\n  relevant portions of `f`, `g`. For example see the unit-test,\n  `test_works_correctly_fx_gx_manually_stopped`.\n\n  Args:\n    fx: `Tensor`. Output of function evaluated at `x`.\n    gx: `Tensor` or list of `Tensor`s. Gradient of function at (each) `x`.\n    x: `Tensor` or list of `Tensor`s. Args of evaluation for `f`.\n    fx_gx_manually_stopped: Python `bool` indicating that `fx`, `gx` manually\n      have `stop_gradient` applied.\n    name: Python `str` name prefixed to Ops created by this function.\n\n  Returns:\n    fx: Floating-type `Tensor` equal to `f(x)` but which has gradient\n      `stop_gradient(g(x))`.\n  \"\"\"\n  def maybe_stop(x):\n    if fx_gx_manually_stopped:\n      return x\n    return tf.stop_gradient(x)\n\n  with tf.compat.v1.name_scope(name, 'custom_gradient', [fx, gx, x]):\n    fx = tf.convert_to_tensor(value=fx, name='fx')\n    # We don't want to bother eagerly computing `gx` since we may not even need\n    # it.\n    with tf.control_dependencies([fx]):\n      if is_list_like(x):\n        x = [identity(x_, name='x') for x_ in x]\n      else:\n        x = [identity(x, name='x')]\n\n      if is_list_like(gx):", "suffix": "              for gx_ in gx]\n      else:\n        gx = [identity(gx, dtype=fx.dtype, name='gx')]\n\n      override_grad = []\n      for x_, gx_ in zip(x, gx):\n        # Observe: tf.gradients(f(x), x)[i].shape == x[i].shape\n        # thus we check that the user is supplying correct shapes.\n        equal_shape = tf.compat.v1.assert_equal(\n            tf.shape(input=x_),\n            tf.shape(input=gx_),\n            message='Each `x` must have the same shape as each `gx`.')\n        with tf.control_dependencies([equal_shape]):\n          # IEEE754 ensures `(x-x)==0.` and that `0.*x==0.` so we make sure to\n          # write the code this way, rather than, e.g.,\n          # `sum_x * stop(gx) + stop(fx - sum_x * gx)`.\n          # For more discussion regarding the relevant portions of the IEEE754\n          # standard, see the StackOverflow question,\n          # \"Is there a floating point value of x, for which x-x == 0 is false?\"\n          # http://stackoverflow.com/q/2686644\n          zeros_like_x_ = x_ - tf.stop_gradient(x_)\n          override_grad.append(\n              tf.reduce_sum(input_tensor=maybe_stop(gx_) * zeros_like_x_))\n      override_grad = sum(override_grad)\n      override_grad /= tf.cast(tf.size(input=fx), dtype=fx.dtype.base_dtype)\n\n      # Proof of correctness:\n      #\n      #  f(x) = x * stop[gx] + stop[fx - x * gx]\n      #       = stop[fx]\n      #\n      #  g(x) = grad[fx]\n      #       = stop[gx] + grad[stop[fx - x * gx]]\n      #       = stop[gx] + 0\n      #\n      # Notice that when x is zero it still works:\n      # grad[x * stop(gx) + stop(fx - x * gx)] = 1 * stop[gx] + 0 = stop[gx]\n      #\n      # The proof is similar for the tensor-domain case, except that we\n      # `reduce_sum` the `stop[gx] * (x - stop[x])` then rescale by\n      # `tf.size(fx)` since this reduced version is broadcast to `fx`.\n      return maybe_stop(fx) + override_grad", "gt": "        gx = [identity(gx_, dtype=fx.dtype, name='gx')"}
{"prefix": "def value_and_gradient(f, xs, use_gradient_tape=False, name=None):\n  \"\"\"Computes `f(*xs)` and its gradients wrt to `*xs`.\n\n  Args:\n    f: Python `callable` to be differentiated. If `f` returns a scalar, this\n      scalar will be differentiated. If `f` returns a tensor or list of tensors,", "suffix": "      a single scalar. If desired, the tensors can be elementwise multiplied by\n      the tensors passed as the `dy` keyword argument to the returned gradient\n      function.\n    xs: Python list of parameters of f for which to differentiate. (Can also\n      be single `Tensor`.)\n    use_gradient_tape: Python `bool` indicating that `tf.GradientTape`\n      should be used regardless of `tf.executing_eagerly()` status.\n      Default value: `False`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., `'value_and_gradient'`).\n\n  Returns:\n    y: `y = f(*xs)`.\n    dydx: Gradient of `y` wrt each of `xs`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'value_and_gradient', [xs]):\n    is_xs_list_like = isinstance(xs, (tuple, list))\n    if not is_xs_list_like:\n      xs = [xs]\n    xs = [tf.convert_to_tensor(value=x, name='x{}'.format(i))\n          for i, x in enumerate(xs)]\n    if tf.executing_eagerly() or use_gradient_tape:\n      with tf.GradientTape(watch_accessed_variables=False) as tape:\n        for x in xs:\n          tape.watch(x)\n        y = f(*xs)\n      dydx = tape.gradient(y, xs)\n    else:\n      y = f(*xs)\n      dydx = tf.gradients(ys=y, xs=xs)\n    if not is_xs_list_like:\n      dydx = dydx[0]\n    return y, dydx", "gt": "      by default a scalar will be computed by adding all their values to produce"}
{"prefix": "", "suffix": "  \"\"\"Convenience function to efficiently construct a MultivariateNormalDiag.\"\"\"\n  # Faster than using `tfd.MultivariateNormalDiag`.\n  return tfd.Independent(tfd.Normal(*args, **kwargs),\n                         reinterpreted_batch_ndims=1)", "gt": "def mvn(*args, **kwargs):"}
{"prefix": "def eight_schools_joint_log_prob(\n    treatment_effects, treatment_stddevs,\n    avg_effect, avg_stddev, school_effects_standard):\n  \"\"\"Eight-schools joint log-prob.\"\"\"\n  rv_avg_effect = tfd.Normal(loc=0., scale=10.)\n  rv_avg_stddev = tfd.Normal(loc=5., scale=1.)\n  rv_school_effects_standard = mvn(\n      loc=tf.zeros_like(school_effects_standard),\n      scale=tf.ones_like(school_effects_standard))", "suffix": "      loc=(avg_effect + tf.exp(avg_stddev) * school_effects_standard),\n      scale=treatment_stddevs)\n  return (\n      rv_avg_effect.log_prob(avg_effect) +\n      rv_avg_stddev.log_prob(avg_stddev) +\n      rv_school_effects_standard.log_prob(school_effects_standard) +\n      rv_treatment_effects.log_prob(treatment_effects))", "gt": "  rv_treatment_effects = mvn("}
{"prefix": "def benchmark_eight_schools_hmc(\n    num_results=int(5e3),\n    num_burnin_steps=int(3e3),\n    num_leapfrog_steps=3,\n    step_size=0.4):\n  \"\"\"Runs HMC on the eight-schools unnormalized posterior.\"\"\"\n\n  num_schools = 8\n  treatment_effects = tf.constant(\n      [28, 8, -3, 7, -1, 1, 18, 12],\n      dtype=np.float32,\n      name='treatment_effects')\n  treatment_stddevs = tf.constant(\n      [15, 10, 16, 11, 9, 11, 10, 18],\n      dtype=np.float32,", "suffix": "\n  def unnormalized_posterior_log_prob(\n      avg_effect, avg_stddev, school_effects_standard):\n    \"\"\"Eight-schools unnormalized log posterior.\"\"\"\n    return eight_schools_joint_log_prob(\n        treatment_effects, treatment_stddevs,\n        avg_effect, avg_stddev, school_effects_standard)\n\n  if tf.executing_eagerly():\n    sample_chain = tf.function(tfp.mcmc.sample_chain)\n  else:\n    sample_chain = tfp.mcmc.sample_chain\n\n  def computation():\n    \"\"\"The benchmark computation.\"\"\"\n    _, kernel_results = sample_chain(\n        num_results=num_results,\n        num_burnin_steps=num_burnin_steps,\n        current_state=(\n            tf.zeros([], name='init_avg_effect'),\n            tf.zeros([], name='init_avg_stddev'),\n            tf.ones([num_schools], name='init_school_effects_standard'),\n        ),\n        kernel=tfp.mcmc.HamiltonianMonteCarlo(\n            target_log_prob_fn=unnormalized_posterior_log_prob,\n            step_size=step_size,\n            num_leapfrog_steps=num_leapfrog_steps))\n\n    return kernel_results.is_accepted\n\n  # Let's force evaluation of graph to ensure build time is not part of our time\n  # trial.\n  is_accepted_tensor = computation()\n  if not tf.executing_eagerly():\n    session = tf.compat.v1.Session()\n    session.run(is_accepted_tensor)\n\n  start_time = time.time()\n  if tf.executing_eagerly():\n    is_accepted = computation()\n  else:\n    is_accepted = session.run(is_accepted_tensor)\n  wall_time = time.time() - start_time\n\n  num_accepted = np.sum(is_accepted)\n  acceptance_rate = np.float32(num_accepted) / np.float32(num_results)\n\n  return dict(\n      iters=(num_results + num_burnin_steps) * num_leapfrog_steps,\n      extras={'acceptance_rate': acceptance_rate},\n      wall_time=wall_time)", "gt": "      name='treatment_stddevs')"}
{"prefix": "def expand_docstring(**kwargs):\n  \"\"\"Decorator to programmatically expand the docstring.\n\n  Args:\n    **kwargs: Keyword arguments to set. For each key-value pair `k` and `v`,", "suffix": "\n  Returns:\n    Decorated function.\n  \"\"\"\n  def _fn_wrapped(fn):\n    \"\"\"Original function with modified `__doc__` attribute.\"\"\"\n    doc = inspect.cleandoc(fn.__doc__)\n    for k, v in six.iteritems(kwargs):\n      # Capture each ${k} reference to replace with v.\n      # We wrap the replacement in a function so no backslash escapes\n      # are processed.\n      pattern = r'\\$\\{' + str(k) + r'\\}'\n      doc = re.sub(pattern, lambda match: v, doc)  # pylint: disable=cell-var-from-loop\n    fn.__doc__ = doc\n    return fn\n  return _fn_wrapped", "gt": "      the key is found as `${k}` in the docstring and replaced with `v`."}
{"prefix": "def _simple_name(distribution):\n  \"\"\"Infer the original name passed into a distribution constructor.\n\n  Distributions typically follow the pattern of\n  with.name_scope(name) as name:\n    super(name=name)\n  so we attempt to reverse the name-scope transformation to allow\n  addressing of RVs by the distribution's original, user-visible\n  name kwarg.\n\n  Args:\n    distribution: a tfd.Distribution instance.\n  Returns:\n    simple_name: the original name passed into the Distribution.\n\n  #### Example\n\n  ```\n  d1 = tfd.Normal(0., 1., name='x') # d1.name = 'x/'\n  d2 = tfd.Normal(0., 1., name='x') # d2.name = 'x_2/'\n  _simple_name(d2) # returns 'x'\n\n  ```\n\n  \"\"\"\n  simple_name = distribution.name\n\n  # turn 'scope/x/' into 'x'\n  if simple_name.endswith('/'):\n    simple_name = simple_name.split('/')[-2]\n\n  # turn 'x_3' into 'x'", "suffix": "  if parts[-1].isdigit():\n    simple_name = '_'.join(parts[:-1])\n\n  return simple_name", "gt": "  parts = simple_name.split('_')"}
{"prefix": "def _build_custom_rv(distribution, sample_shape, value, name):", "suffix": "  # Program transformations (e.g., `make_log_joint_fn`) assume that\n  # the traced constructor has `name` and `value` kwargs, enabling\n  # them to override the value of an RV according to its name.\n  # User-defined RVs inherit their name from the provided\n  # distribution; this helper method exposes the name as a dummy kwarg\n  # so that it's visible to program transformations.\n  del name  # unused\n  return RandomVariable(distribution=distribution,\n                        sample_shape=sample_shape,\n                        value=value)", "gt": "  \"\"\"RandomVariable constructor with a dummy name argument.\"\"\""}
{"prefix": "def as_random_variable(distribution,\n                       sample_shape=(),\n                       value=None):\n  \"\"\"Wrap an existing distribution as a traceable random variable.\n\n  This enables the use of custom or user-provided distributions in\n  Edward models. Unlike a bare `RandomVariable` object, this method\n  wraps the constructor so it is included in the Edward trace and its\n  values can be properly intercepted and overridden.\n\n  Where possible, you should prefer the built-in constructors\n  (`ed.Normal`, etc); these simultaneously construct a Distribution\n  and a RandomVariable object so that the distribution parameters\n  themselves may be intercepted and overridden. RVs constructed via\n  `as_random_variable()` have a fixed distribution and may not support\n  program transformations (e.g, conjugate marginalization) that rely\n  on overriding distribution parameters.\n\n  Args:\n    distribution: tfd.Distribution governing the distribution of the random\n      variable, such as sampling and log-probabilities.\n    sample_shape: tf.TensorShape of samples to draw from the random variable.\n      Default is `()` corresponding to a single sample.\n    value: Fixed tf.Tensor to associate with random variable. Must have shape\n      `sample_shape + distribution.batch_shape + distribution.event_shape`.\n      Default is to sample from random variable according to `sample_shape`.\n\n  Returns:\n    rv: a `RandomVariable` wrapping the provided distribution.\n\n  #### Example\n\n  ```python\n  from tensorflow_probability import distributions as tfd\n  from tensorflow_probability import edward2 as ed\n\n  def model():\n    # equivalent to ed.Normal(0., 1., name='x')\n    return ed.as_random_variable(tfd.Normal(0., 1., name='x'))\n\n  log_joint = ed.make_log_joint_fn(model)\n  output = log_joint(x=2.)\n  ```", "suffix": "\n  return _build_custom_rv(distribution=distribution,\n                          sample_shape=sample_shape,\n                          value=value,\n                          name=_simple_name(distribution))", "gt": "  \"\"\""}
{"prefix": "def _make_random_variable(distribution_cls):\n  \"\"\"Factory function to make random variable given distribution class.\"\"\"\n\n  @interceptable\n  @functools.wraps(distribution_cls, assigned=('__module__', '__name__'))\n  @docstring_util.expand_docstring(\n      cls=distribution_cls.__name__,\n      doc=inspect.cleandoc(distribution_cls.__init__.__doc__ or ''))\n  def func(*args, **kwargs):", "suffix": "    \"\"\"Create a random variable for ${cls}.\n\n    See ${cls} for more details.\n\n    Returns:\n      RandomVariable.\n\n    #### Original Docstring for Distribution\n\n    ${doc}\n    \"\"\"\n    # pylint: enable=g-doc-args\n    sample_shape = kwargs.pop('sample_shape', ())\n    value = kwargs.pop('value', None)\n    return RandomVariable(distribution=distribution_cls(*args, **kwargs),\n                          sample_shape=sample_shape,\n                          value=value)\n  return func", "gt": "    # pylint: disable=g-doc-args"}
{"prefix": "def _mode_mean_shape(self):", "suffix": "    shape = tensorshape_util.concatenate(self.batch_shape, self.event_shape)\n    has_static_shape = tensorshape_util.is_fully_defined(shape)\n    if not has_static_shape:\n      shape = tf.concat([\n          self.batch_shape_tensor(),\n          self.event_shape_tensor(),\n      ], 0)\n    return shape", "gt": "    \"\"\"Shape for the mode/mean Tensors.\"\"\""}
{"prefix": "def one_step_predictive(model, observed_time_series, parameter_samples):\n  \"\"\"Compute one-step-ahead predictive distributions for all timesteps.\n\n  Given samples from the posterior over parameters, return the predictive\n  distribution over observations at each time `T`, given observations up\n  through time `T-1`.\n\n  Args:\n    model: An instance of `StructuralTimeSeries` representing a\n      time-series model. This represents a joint distribution over\n      time-series and their parameters with batch shape `[b1, ..., bN]`.\n    observed_time_series: `float` `Tensor` of shape\n      `concat([sample_shape, model.batch_shape, [num_timesteps, 1]]) where\n      `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`\n      dimension may (optionally) be omitted if `num_timesteps > 1`. May\n      optionally be an instance of `tfp.sts.MaskedTimeSeries` including a\n      mask `Tensor` to encode the locations of missing observations.\n    parameter_samples: Python `list` of `Tensors` representing posterior samples\n      of model parameters, with shapes `[concat([[num_posterior_draws],\n      param.prior.batch_shape, param.prior.event_shape]) for param in\n      model.parameters]`. This may optionally also be a map (Python `dict`) of\n      parameter names to `Tensor` values.\n\n  Returns:\n    forecast_dist: a `tfd.MixtureSameFamily` instance with event shape\n      [num_timesteps] and\n      batch shape `concat([sample_shape, model.batch_shape])`, with\n      `num_posterior_draws` mixture components. The `t`th step represents the\n      forecast distribution `p(observed_time_series[t] |\n      observed_time_series[0:t-1], parameter_samples)`.\n\n  #### Examples\n\n  Suppose we've built a model and fit it to data using HMC:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n\n    samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)", "suffix": "\n  Passing the posterior samples into `one_step_predictive`, we construct a\n  one-step-ahead predictive distribution:\n\n  ```python\n    one_step_predictive_dist = tfp.sts.one_step_predictive(\n      model, observed_time_series, parameter_samples=samples)\n\n    predictive_means = one_step_predictive_dist.mean()\n    predictive_scales = one_step_predictive_dist.stddev()\n  ```\n\n  If using variational inference instead of HMC, we'd construct a forecast using\n  samples from the variational posterior:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series)\n\n    # OMITTED: take steps to optimize variational loss\n\n    samples = {k: q.sample(30) for (k, q) in variational_distributions.items()}\n    one_step_predictive_dist = tfp.sts.one_step_predictive(\n      model, observed_time_series, parameter_samples=samples)\n  ```\n\n  We can visualize the forecast by plotting:\n\n  ```python\n    from matplotlib import pylab as plt\n    def plot_one_step_predictive(observed_time_series,\n                                 forecast_mean,\n                                 forecast_scale):\n      plt.figure(figsize=(12, 6))\n      num_timesteps = forecast_mean.shape[-1]\n      c1, c2 = (0.12, 0.47, 0.71), (1.0, 0.5, 0.05)\n      plt.plot(observed_time_series, label=\"observed time series\", color=c1)\n      plt.plot(forecast_mean, label=\"one-step prediction\", color=c2)\n      plt.fill_between(np.arange(num_timesteps),\n                       forecast_mean - 2 * forecast_scale,\n                       forecast_mean + 2 * forecast_scale,\n                       alpha=0.1, color=c2)\n      plt.legend()\n\n    plot_one_step_predictive(observed_time_series,\n                             forecast_mean=predictive_means,\n                             forecast_scale=predictive_scales)\n  ```\n\n  To detect anomalous timesteps, we check whether the observed value at each\n  step is within a 95% predictive interval, i.e., two standard deviations from\n  the mean:\n\n  ```python\n    z_scores = ((observed_time_series[..., 1:] - predictive_means[..., :-1])\n                 / predictive_scales[..., :-1])\n    anomalous_timesteps = tf.boolean_mask(\n        tf.range(1, num_timesteps),\n        tf.abs(z_scores) > 2.0)\n  ```\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope(\n      'one_step_predictive', values=[observed_time_series, parameter_samples]):\n\n    [\n        observed_time_series,\n        is_missing\n    ] = sts_util.canonicalize_observed_time_series_with_mask(\n        observed_time_series)\n\n    # Run filtering over the training timesteps to extract the\n    # predictive means and variances.\n    num_timesteps = dist_util.prefer_static_value(\n        tf.shape(input=observed_time_series))[-2]\n    lgssm = model.make_state_space_model(\n        num_timesteps=num_timesteps, param_vals=parameter_samples)\n    (_, _, _, _, _, observation_means, observation_covs\n    ) = lgssm.forward_filter(observed_time_series, mask=is_missing)\n\n    # Squeeze dims to convert from LGSSM's event shape `[num_timesteps, 1]`\n    # to a scalar time series.\n    return sts_util.mix_over_posterior_draws(\n        means=observation_means[..., 0],\n        variances=observation_covs[..., 0, 0])", "gt": "  ```"}
{"prefix": "def forecast(model,\n             observed_time_series,\n             parameter_samples,\n             num_steps_forecast):\n  \"\"\"Construct predictive distribution over future observations.\n\n  Given samples from the posterior over parameters, return the predictive\n  distribution over future observations for num_steps_forecast timesteps.\n\n  Args:\n    model: An instance of `StructuralTimeSeries` representing a\n      time-series model. This represents a joint distribution over\n      time-series and their parameters with batch shape `[b1, ..., bN]`.\n    observed_time_series: `float` `Tensor` of shape\n      `concat([sample_shape, model.batch_shape, [num_timesteps, 1]])` where\n      `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`\n      dimension may (optionally) be omitted if `num_timesteps > 1`. May\n      optionally be an instance of `tfp.sts.MaskedTimeSeries` including a\n      mask `Tensor` to encode the locations of missing observations.\n    parameter_samples: Python `list` of `Tensors` representing posterior samples\n      of model parameters, with shapes `[concat([[num_posterior_draws],\n      param.prior.batch_shape, param.prior.event_shape]) for param in\n      model.parameters]`. This may optionally also be a map (Python `dict`) of\n      parameter names to `Tensor` values.\n    num_steps_forecast: scalar `int` `Tensor` number of steps to forecast.\n\n  Returns:\n    forecast_dist: a `tfd.MixtureSameFamily` instance with event shape\n      [num_steps_forecast, 1] and batch shape\n      `concat([sample_shape, model.batch_shape])`, with `num_posterior_draws`\n      mixture components.\n\n  #### Examples\n\n  Suppose we've built a model and fit it to data using HMC:", "suffix": "  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n\n    samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)\n  ```\n\n  Passing the posterior samples into `forecast`, we construct a forecast\n  distribution:\n\n  ```python\n    forecast_dist = tfp.sts.forecast(model, observed_time_series,\n                                     parameter_samples=samples,\n                                     num_steps_forecast=50)\n\n    forecast_mean = forecast_dist.mean()[..., 0]  # shape: [50]\n    forecast_scale = forecast_dist.stddev()[..., 0]  # shape: [50]\n    forecast_samples = forecast_dist.sample(10)[..., 0]  # shape: [10, 50]\n  ```\n\n  If using variational inference instead of HMC, we'd construct a forecast using\n  samples from the variational posterior:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series)\n\n    # OMITTED: take steps to optimize variational loss\n\n    samples = {k: q.sample(30) for (k, q) in variational_distributions.items()}\n    forecast_dist = tfp.sts.forecast(model, observed_time_series,\n                                         parameter_samples=samples,\n                                         num_steps_forecast=50)\n  ```\n\n  We can visualize the forecast by plotting:\n\n  ```python\n    from matplotlib import pylab as plt\n    def plot_forecast(observed_time_series,\n                      forecast_mean,\n                      forecast_scale,\n                      forecast_samples):\n      plt.figure(figsize=(12, 6))\n\n      num_steps = observed_time_series.shape[-1]\n      num_steps_forecast = forecast_mean.shape[-1]\n      num_steps_train = num_steps - num_steps_forecast\n\n      c1, c2 = (0.12, 0.47, 0.71), (1.0, 0.5, 0.05)\n      plt.plot(np.arange(num_steps), observed_time_series,\n               lw=2, color=c1, label='ground truth')\n\n      forecast_steps = np.arange(num_steps_train,\n                       num_steps_train+num_steps_forecast)\n      plt.plot(forecast_steps, forecast_samples.T, lw=1, color=c2, alpha=0.1)\n      plt.plot(forecast_steps, forecast_mean, lw=2, ls='--', color=c2,\n               label='forecast')\n      plt.fill_between(forecast_steps,\n                       forecast_mean - 2 * forecast_scale,\n                       forecast_mean + 2 * forecast_scale, color=c2, alpha=0.2)\n\n      plt.xlim([0, num_steps])\n      plt.legend()\n\n    plot_forecast(observed_time_series,\n                  forecast_mean=forecast_mean,\n                  forecast_scale=forecast_scale,\n                  forecast_samples=forecast_samples)\n  ```\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope(\n      'forecast',\n      values=[observed_time_series, parameter_samples, num_steps_forecast]):\n    [\n        observed_time_series,\n        mask\n    ] = sts_util.canonicalize_observed_time_series_with_mask(\n        observed_time_series)\n\n    # Run filtering over the observed timesteps to extract the\n    # latent state posterior at timestep T+1 (i.e., the final\n    # filtering distribution, pushed through the transition model).\n    # This is the prior for the forecast model (\"today's prior\n    # is yesterday's posterior\").\n    num_observed_steps = dist_util.prefer_static_value(\n        tf.shape(input=observed_time_series))[-2]\n    observed_data_ssm = model.make_state_space_model(\n        num_timesteps=num_observed_steps, param_vals=parameter_samples)\n    (_, _, _, predictive_means, predictive_covs, _, _\n    ) = observed_data_ssm.forward_filter(observed_time_series, mask=mask)\n\n    # Build a batch of state-space models over the forecast period. Because\n    # we'll use MixtureSameFamily to mix over the posterior draws, we need to\n    # do some shenanigans to move the `[num_posterior_draws]` batch dimension\n    # from the leftmost to the rightmost side of the model's batch shape.\n    # TODO(b/120245392): enhance `MixtureSameFamily` to reduce along an\n    # arbitrary axis, and eliminate `move_dimension` calls here.\n    parameter_samples = model._canonicalize_param_vals_as_map(parameter_samples)  # pylint: disable=protected-access\n    parameter_samples_with_reordered_batch_dimension = {\n        param.name: dist_util.move_dimension(\n            parameter_samples[param.name],\n            0, -(1 + _prefer_static_event_ndims(param.prior)))\n        for param in model.parameters}\n    forecast_prior = tfd.MultivariateNormalFullCovariance(\n        loc=dist_util.move_dimension(predictive_means[..., -1, :], 0, -2),\n        covariance_matrix=dist_util.move_dimension(\n            predictive_covs[..., -1, :, :], 0, -3))\n\n    # Ugly hack: because we moved `num_posterior_draws` to the trailing (rather\n    # than leading) dimension of parameters, the parameter batch shapes no\n    # longer broadcast against the `constant_offset` attribute used in `sts.Sum`\n    # models. We fix this by manually adding an extra broadcasting dim to\n    # `constant_offset` if present.\n    # The root cause of this hack is that we mucked with param dimensions above\n    # and are now passing params that are 'invalid' in the sense that they don't\n    # match the shapes of the model's param priors. The fix (as above) will be\n    # to update MixtureSameFamily so we can avoid changing param dimensions\n    # altogether.\n    # TODO(b/120245392): enhance `MixtureSameFamily` to reduce along an\n    # arbitrary axis, and eliminate this hack.\n    kwargs = {}\n    if hasattr(model, 'constant_offset'):\n      kwargs['constant_offset'] = tf.convert_to_tensor(\n          value=model.constant_offset,\n          dtype=forecast_prior.dtype)[..., tf.newaxis]\n\n    # We assume that any STS model that has a `constant_offset` attribute\n    # will allow it to be overridden as a kwarg. This is currently just\n    # `sts.Sum`.\n    # TODO(b/120245392): when kwargs hack is removed, switch back to calling\n    # the public version of `_make_state_space_model`.\n    forecast_ssm = model._make_state_space_model(  # pylint: disable=protected-access\n        num_timesteps=num_steps_forecast,\n        param_map=parameter_samples_with_reordered_batch_dimension,\n        initial_state_prior=forecast_prior,\n        initial_step=num_observed_steps,\n        **kwargs)\n\n    num_posterior_draws = dist_util.prefer_static_value(\n        forecast_ssm.batch_shape_tensor())[-1]\n    return tfd.MixtureSameFamily(\n        mixture_distribution=tfd.Categorical(\n            logits=tf.zeros([num_posterior_draws], dtype=forecast_ssm.dtype)),\n        components_distribution=forecast_ssm)", "gt": ""}
{"prefix": "def _max_mask_non_finite(x, axis=-1, keepdims=False, mask=0):\n  \"\"\"Returns `max` or `mask` if `max` is not finite.\"\"\"\n  m = np.max(x, axis=_astuple(axis), keepdims=keepdims)\n  needs_masking = ~np.isfinite(m)\n  if needs_masking.ndim > 0:\n    m[needs_masking] = mask", "suffix": "    m = mask\n  return m", "gt": "  elif needs_masking:"}
{"prefix": "def _reduce_logsumexp(input_tensor, axis=None, keepdims=False, name=None):  # pylint: disable=unused-argument\n  \"\"\"Computes `log(sum(exp(input_tensor))) along the specified axis.\"\"\"\n  try:\n    return scipy_special.logsumexp(\n        input_tensor, axis=_astuple(axis), keepdims=keepdims)", "suffix": "    # We offer a non SP version just in case SP isn't installed and this\n    # because logsumexp is often used.\n    m = _max_mask_non_finite(input_tensor, axis=axis, keepdims=True)\n    y = input_tensor - m\n    y = np.exp(y, out=y)\n    return m + np.log(np.sum(y, axis=_astuple(axis), keepdims=keepdims))", "gt": "  except NotImplementedError:"}
{"prefix": "def assert_finite(x, data=None, summarize=None, message=None, name=None):\n  \"\"\"Assert all elements of `x` are finite.\n\n  Args:\n    x:  Numeric `Tensor`.\n    data:  The tensors to print out if the condition is False.  Defaults to\n      error message and first few entries of `x`.\n    summarize: Print this many entries of each tensor.\n    message: A string to prefix to the default message.\n    name: A name for this operation (optional).\n      Defaults to \"assert_finite\".\n\n  Returns:\n    Op raising `InvalidArgumentError` unless `x` has specified rank or lower.\n    If static checks determine `x` has correct rank, a `no_op` is returned.\n\n  Raises:\n    ValueError:  If static checks determine `x` has wrong rank.\n  \"\"\"\n  with tf.compat.v2.name_scope(name or 'assert_finite'):", "suffix": "    if x_ is not None:\n      if ~np.all(np.isfinite(x_)):\n        raise ValueError(message)\n      return x\n    assertion = tf.compat.v1.assert_equal(\n        tf.math.is_finite(x), tf.ones_like(x, tf.bool),\n        data=data, summarize=summarize, message=message)\n    with tf.control_dependencies([assertion]):\n      return tf.identity(x)", "gt": "    x_ = tf.get_static_value(x)"}
{"prefix": "def assert_rank_at_most(x, rank, data=None, summarize=None, message=None,\n                        name=None):\n  \"\"\"Assert `x` has rank equal to `rank` or smaller.\n\n  Example of adding a dependency to an operation:\n\n  ```python\n  with tf.control_dependencies([tf.assert_rank_at_most(x, 2)]):", "suffix": "  ```\n\n  Args:\n    x:  Numeric `Tensor`.\n    rank:  Scalar `Tensor`.\n    data:  The tensors to print out if the condition is False.  Defaults to\n      error message and first few entries of `x`.\n    summarize: Print this many entries of each tensor.\n    message: A string to prefix to the default message.\n    name: A name for this operation (optional).\n      Defaults to \"assert_rank_at_most\".\n\n  Returns:\n    Op raising `InvalidArgumentError` unless `x` has specified rank or lower.\n    If static checks determine `x` has correct rank, a `no_op` is returned.\n\n  Raises:\n    ValueError:  If static checks determine `x` has wrong rank.\n  \"\"\"\n  with tf.compat.v2.name_scope(name or 'assert_rank_at_most'):\n    return tf.compat.v1.assert_less_equal(\n        tf.rank(x), rank, data=data, summarize=summarize, message=message)", "gt": "    output = tf.reduce_sum(x)"}
{"prefix": "def _event_size(event_shape, name=None):\n  \"\"\"Computes the number of elements in a tensor with shape `event_shape`.\n\n  Args:\n    event_shape: A tensor shape.\n    name: The name to use for the tensor op to compute the number of elements\n      (if such an op needs to be created).\n\n  Returns:\n    event_size: The number of elements in `tensor_shape`.  Returns a numpy int\n    when the number of elements can be computed immediately.  Otherwise, returns\n    a scalar tensor.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'event_size', [event_shape]):\n    event_shape = tf.convert_to_tensor(\n        value=event_shape, dtype=tf.int32, name='event_shape')\n\n    event_shape_const = tf.get_static_value(event_shape)\n    if event_shape_const is not None:", "suffix": "    else:\n      return tf.reduce_prod(input_tensor=event_shape)", "gt": "      return np.prod(event_shape_const)"}
{"prefix": "def _eval_all_one_hot(fn, dist, name=None):\n  \"\"\"OneHotCategorical helper computing probs, cdf, etc over its support.\"\"\"\n  with tf.compat.v1.name_scope(name, 'eval_all_one_hot'):\n    event_size = dist.event_shape_tensor()[-1]\n    batch_ndims = tf.size(input=dist.batch_shape_tensor())\n    # Reshape `eye(d)` to: `[d] + [1]*batch_ndims + [d]`.\n    x = tf.reshape(\n        tf.eye(event_size, dtype=dist.dtype),\n        shape=tf.pad(\n            tensor=tf.ones(batch_ndims, tf.int32),\n            paddings=[[1, 1]],\n            constant_values=event_size))", "suffix": "    perm = tf.pad(tensor=tf.range(1, batch_ndims + 1), paddings=[[0, 1]])\n    return tf.transpose(a=fn(dist, x), perm=perm)", "gt": "    # Compute `fn(x)` then cyclically left-transpose one dim."}
{"prefix": "def _make_kl_divergence_fn(\n    distribution_b,\n    use_exact_kl=False,\n    test_points_reduce_axis=(),  # `None` == \"all\"; () == \"none\".\n    test_points_fn=tf.convert_to_tensor,", "suffix": "  \"\"\"Creates a callable computing `KL[a,b]` from `a`, a `tfd.Distribution`.\"\"\"\n\n  if use_exact_kl is None:\n    kl_divergence_fn = tfd.kl_divergence\n  else:\n    # Closure over: test_points_fn, test_points_reduce_axis.\n    def kl_divergence_fn(distribution_a, distribution_b):\n      z = test_points_fn(distribution_a)\n      return tf.reduce_mean(\n          input_tensor=distribution_a.log_prob(z) - distribution_b.log_prob(z),\n          axis=test_points_reduce_axis)\n\n  # Closure over: distribution_b, kl_divergence_fn, weight.\n  def _fn(distribution_a):\n    \"\"\"Closure that computes KLDiv as a function of `a` as in `KL[a, b]`.\"\"\"\n    with tf.compat.v1.name_scope('kldivergence_loss'):\n      # TODO(b/119756336): Due to eager/graph Jacobian graph caching bug\n      # we add here the capability for deferred construction of the prior.\n      # This capability can probably be removed once b/119756336 is resolved.\n      distribution_b_ = (distribution_b() if callable(distribution_b)\n                         else distribution_b)\n      kl = kl_divergence_fn(distribution_a, distribution_b_)\n      if weight is not None:\n        kl = tf.cast(weight, dtype=kl.dtype) * kl\n      # Losses appended with the model.add_loss and are expected to be a single\n      # scalar, unlike model.loss, which is expected to be the loss per sample.\n      # Therefore, we reduce over all dimensions, regardless of the shape.\n      # We take the sum because (apparently) Keras will add this to the *post*\n      # `reduce_sum` (total) loss.\n      # TODO(b/126259176): Add end-to-end Keras/TFP test to ensure the API's\n      # align, particularly wrt how losses are aggregated (across batch\n      # members).\n      return tf.reduce_sum(input_tensor=kl, name='batch_total_kl_divergence')\n\n  return _fn", "gt": "    weight=None):"}
{"prefix": "def _get_convert_to_tensor_fn(identifier):\n  \"\"\"Return a convert-to-tensor func, given a name, config, callable, etc.\"\"\"\n  if identifier is None:\n    return None\n\n  if isinstance(identifier, six.string_types):\n    identifier = str(identifier)\n    return _deserialize(identifier)\n\n  if isinstance(identifier, dict):\n    return _deserialize(identifier)\n\n  if isinstance(identifier, property):\n    identifier = identifier.fget\n  if callable(identifier):\n    return identifier\n", "suffix": "                   'convert-to-tensor function identifier:', identifier)", "gt": "  raise ValueError('Could not interpret '"}
{"prefix": "def get_config(self):\n    \"\"\"Returns the config of this layer.\n\n    This Layer's `make_distribution_fn` is serialized via a library built on\n    Python pickle.  This serialization of Python functions is provided for\n    convenience, but:\n\n      1. The use of this format for long-term storage of models is discouraged.\n         In particular, it may not be possible to deserialize in a different\n         version of Python.\n\n      2. While serialization is generally supported for lambdas, local\n         functions, and static methods (and closures over these constructs),\n         complex functions may fail to serialize.", "suffix": "      3. `Tensor` objects (and functions referencing `Tensor` objects) can only\n         be serialized when the tensor value is statically known.  (Such Tensors\n         are serialized as numpy arrays.)\n\n    Instead of relying on `DistributionLambda.get_config`, consider subclassing\n    `DistributionLambda` and directly implementing Keras serialization via\n    `get_config` / `from_config`.\n\n    NOTE: At the moment, `DistributionLambda` can only be serialized if the\n    `convert_to_tensor_fn` is a serializable Keras object (i.e., implements\n    `get_config`) or one of the standard values:\n     - `Distribution.sample` (or `\"sample\"`)\n     - `Distribution.mean` (or `\"mean\"`)\n     - `Distribution.mode` (or `\"mode\"`)\n     - `Distribution.stddev` (or `\"stddev\"`)\n     - `Distribution.variance` (or `\"variance\"`)\n    \"\"\"\n    config = {\n        'make_distribution_fn': _serialize_function(self._make_distribution_fn),\n        'convert_to_tensor_fn': _serialize(self._convert_to_tensor_fn),\n    }\n    base_config = super(DistributionLambda, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))", "gt": ""}
{"prefix": "def new(params, event_size, validate_args=False, name=None):\n    \"\"\"Create the distribution instance from a `params` vector.\"\"\"\n    with tf.compat.v1.name_scope(name, 'MultivariateNormalTriL',\n                                 [params, event_size]):\n      params = tf.convert_to_tensor(value=params, name='params')\n      scale_tril = tfb.ScaleTriL(\n          diag_shift=np.array(1e-5, params.dtype.as_numpy_dtype()),\n          validate_args=validate_args)\n      return tfd.MultivariateNormalTriL(", "suffix": "          scale_tril=scale_tril(params[..., event_size:]),\n          validate_args=validate_args)", "gt": "          loc=params[..., :event_size],"}
{"prefix": "", "suffix": "    \"\"\"The number of `params` needed to create a single distribution.\"\"\"\n    with tf.compat.v1.name_scope(name, 'MultivariateNormalTriL_params_size',\n                                 [event_size]):\n      return event_size + event_size * (event_size + 1) // 2", "gt": "def params_size(event_size, name=None):"}
{"prefix": "def new(params, event_size, dtype=None, validate_args=False, name=None):\n    \"\"\"Create the distribution instance from a `params` vector.\"\"\"", "suffix": "                                 [params, event_size]):\n      return tfd.OneHotCategorical(\n          logits=params,\n          dtype=dtype or params.dtype.base_dtype,\n          validate_args=validate_args)", "gt": "    with tf.compat.v1.name_scope(name, 'OneHotCategorical',"}
{"prefix": "def new(params, event_size, num_components,\n          dtype=None, validate_args=False, name=None):\n    \"\"\"Create the distribution instance from a `params` vector.\"\"\"\n    with tf.compat.v1.name_scope(name, 'CategoricalMixtureOfOneHotCategorical',", "suffix": "      dist = MixtureSameFamily.new(\n          params,\n          num_components,\n          OneHotCategorical(\n              event_size,\n              validate_args=False,  # So we can eval on simplex interior.\n              name=name),\n          validate_args=validate_args,\n          name=name)\n      # pylint: disable=protected-access\n      dist._mean = functools.partial(\n          _eval_all_one_hot, tfd.Distribution.prob, dist)\n      dist.log_mean = functools.partial(\n          _eval_all_one_hot, tfd.Distribution.log_prob, dist)\n      # pylint: enable=protected-access\n      return dist", "gt": "                                 [params, event_size, num_components]):"}
{"prefix": "def params_size(event_size, num_components, name=None):\n    \"\"\"The number of `params` needed to create a single distribution.\"\"\"\n    with tf.compat.v1.name_scope(\n        name, 'CategoricalMixtureOfOneHotCategorical_params_size',\n        [event_size, num_components]):\n      return MixtureSameFamily.params_size(\n          num_components,", "suffix": "          name=name)", "gt": "          OneHotCategorical.params_size(event_size, name=name),"}
{"prefix": "", "suffix": "    \"\"\"Create the distribution instance from a `params` vector.\"\"\"\n    with tf.compat.v1.name_scope(name, 'IndependentBernoulli',\n                                 [params, event_shape]):\n      params = tf.convert_to_tensor(value=params, name='params')\n      event_shape = dist_util.expand_to_vector(\n          tf.convert_to_tensor(\n              value=event_shape, name='event_shape', dtype_hint=tf.int32),\n          tensor_name='event_shape')\n      new_shape = tf.concat([\n          tf.shape(input=params)[:-1],\n          event_shape,\n      ], axis=0)\n      dist = tfd.Independent(\n          tfd.Bernoulli(\n              logits=tf.reshape(params, new_shape),\n              dtype=dtype or params.dtype.base_dtype,\n              validate_args=validate_args),\n          reinterpreted_batch_ndims=tf.size(input=event_shape),\n          validate_args=validate_args)\n      dist._logits = dist.distribution._logits  # pylint: disable=protected-access\n      dist._probs = dist.distribution._probs  # pylint: disable=protected-access\n      dist.logits = tfd.Bernoulli.logits\n      dist.probs = tfd.Bernoulli.probs\n      return dist", "gt": "def new(params, event_shape=(), dtype=None, validate_args=False, name=None):"}
{"prefix": "def get_config(self):", "suffix": "\n    NOTE: At the moment, this configuration can only be serialized if the\n    Layer's `convert_to_tensor_fn` is a serializable Keras object (i.e.,\n    implements `get_config`) or one of the standard values:\n     - `Distribution.sample` (or `\"sample\"`)\n     - `Distribution.mean` (or `\"mean\"`)\n     - `Distribution.mode` (or `\"mode\"`)\n     - `Distribution.stddev` (or `\"stddev\"`)\n     - `Distribution.variance` (or `\"variance\"`)\n    \"\"\"\n    config = {\n        'event_shape': self._event_shape,\n        'convert_to_tensor_fn': _serialize(self._convert_to_tensor_fn),\n        'sample_dtype': self._sample_dtype,\n        'validate_args': self._validate_args\n    }\n    base_config = super(IndependentBernoulli, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))", "gt": "    \"\"\"Returns the config of this layer."}
{"prefix": "def new(params, event_shape=(), validate_args=False, name=None):\n    \"\"\"Create the distribution instance from a `params` vector.\"\"\"\n    with tf.compat.v1.name_scope(name, 'IndependentLogistic',\n                                 [params, event_shape]):\n      params = tf.convert_to_tensor(value=params, name='params')\n      event_shape = dist_util.expand_to_vector(\n          tf.convert_to_tensor(\n              value=event_shape, name='event_shape', dtype_hint=tf.int32),\n          tensor_name='event_shape')\n      output_shape = tf.concat([\n          tf.shape(input=params)[:-1],\n          event_shape,\n      ],\n                               axis=0)\n      loc_params, scale_params = tf.split(params, 2, axis=-1)\n      return tfd.Independent(", "suffix": "              loc=tf.reshape(loc_params, output_shape),\n              scale=tf.math.softplus(tf.reshape(scale_params, output_shape)),\n              validate_args=validate_args),\n          reinterpreted_batch_ndims=tf.size(input=event_shape),\n          validate_args=validate_args)", "gt": "          tfd.Logistic("}
{"prefix": "def params_size(event_shape=(), name=None):\n    \"\"\"The number of `params` needed to create a single distribution.\"\"\"\n    with tf.compat.v1.name_scope(name, 'IndependentNormal_params_size',\n                                 [event_shape]):", "suffix": "          value=event_shape, name='event_shape', dtype_hint=tf.int32)\n      return 2 * _event_size(\n          event_shape, name=name or 'IndependentNormal_params_size')", "gt": "      event_shape = tf.convert_to_tensor("}
{"prefix": "def new(params, event_shape=(), validate_args=False, name=None):\n    \"\"\"Create the distribution instance from a `params` vector.\"\"\"\n    with tf.compat.v1.name_scope(name, 'IndependentPoisson',\n                                 [params, event_shape]):\n      params = tf.convert_to_tensor(value=params, name='params')\n      event_shape = dist_util.expand_to_vector(\n          tf.convert_to_tensor(\n              value=event_shape, name='event_shape', dtype_hint=tf.int32),\n          tensor_name='event_shape')\n      output_shape = tf.concat([\n          tf.shape(input=params)[:-1],\n          event_shape,\n      ],", "suffix": "      return tfd.Independent(\n          tfd.Poisson(\n              log_rate=tf.reshape(params, output_shape),\n              validate_args=validate_args),\n          reinterpreted_batch_ndims=tf.size(input=event_shape),\n          validate_args=validate_args)", "gt": "                               axis=0)"}
{"prefix": "def new(params, num_components, component_layer,\n          validate_args=False, name=None):\n    \"\"\"Create the distribution instance from a `params` vector.\"\"\"\n    with tf.compat.v1.name_scope(name, 'MixtureSameFamily',", "suffix": "      params = tf.convert_to_tensor(value=params, name='params')\n      num_components = tf.convert_to_tensor(\n          value=num_components, name='num_components', dtype_hint=tf.int32)\n\n      components_dist = component_layer(\n          tf.reshape(\n              params[..., num_components:],\n              tf.concat([tf.shape(input=params)[:-1], [num_components, -1]],\n                        axis=0)))\n      mixture_dist = tfd.Categorical(logits=params[..., :num_components])\n      return tfd.MixtureSameFamily(\n          mixture_dist,\n          components_dist,\n          # TODO(b/120154797): Change following to `validate_args=True` after\n          # fixing: \"ValueError: `mixture_distribution` must have scalar\n          # `event_dim`s.\" assertion in MixtureSameFamily.\n          validate_args=False)", "gt": "                                 [params, num_components, component_layer]):"}
{"prefix": "def params_size(num_components, component_params_size, name=None):\n    \"\"\"Number of `params` needed to create a `MixtureSameFamily` distribution.\n\n    Arguments:\n      num_components: Number of component distributions in the mixture\n        distribution.\n      component_params_size: Number of parameters needed to create a single\n        component distribution.\n      name: The name to use for the op to compute the number of parameters\n        (if such an op needs to be created).\n", "suffix": "     params_size: The number of parameters needed to create the mixture\n       distribution.\n    \"\"\"\n    with tf.compat.v1.name_scope(name, 'MixtureSameFamily_params_size',\n                                 [num_components, component_params_size]):\n      num_components = tf.convert_to_tensor(\n          value=num_components, name='num_components', dtype_hint=tf.int32)\n      component_params_size = tf.convert_to_tensor(\n          value=component_params_size, name='component_params_size')\n\n      num_components = dist_util.prefer_static_value(num_components)\n      component_params_size = dist_util.prefer_static_value(\n          component_params_size)\n\n      return num_components + num_components * component_params_size", "gt": "    Returns:"}
{"prefix": "", "suffix": "    \"\"\"The number of `params` needed to create a single distribution.\"\"\"\n    return MixtureSameFamily.params_size(\n        num_components,\n        IndependentNormal.params_size(event_shape, name=name),\n        name=name)", "gt": "def params_size(num_components, event_shape=(), name=None):"}
{"prefix": "def new(params, num_components, event_shape=(),\n          validate_args=False, name=None):\n    \"\"\"Create the distribution instance from a `params` vector.\"\"\"\n    return MixtureSameFamily.new(\n        params,\n        num_components,\n        IndependentLogistic(", "suffix": "        validate_args=validate_args,\n        name=name)", "gt": "            event_shape, validate_args=validate_args, name=name),"}
{"prefix": "def params_size(num_components, event_shape=(), name=None):\n    \"\"\"The number of `params` needed to create a single distribution.\"\"\"\n    return MixtureSameFamily.params_size(\n        num_components,\n        IndependentLogistic.params_size(event_shape, name=name),", "suffix": "", "gt": "        name=name)"}
{"prefix": "def get_next_interceptor():\n  \"\"\"Yields the top-most interceptor on the thread-local interceptor stack.\n\n  Operations may be intercepted by multiple nested interceptors. Once reached,\n  an operation can be forwarded through nested interceptors until resolved.\n  To allow for nesting, implement interceptors by re-wrapping their first\n  argument (`f`) as an `interceptable`. To avoid nesting, manipulate the\n  computation without using `interceptable`.\n\n  This function allows for nesting by manipulating the thread-local interceptor\n  stack, so that operations are intercepted in the order of interceptor nesting.\n\n  #### Examples\n\n  ```python\n  from tensorflow_probability import edward2 as ed\n\n  def model():\n    x = ed.Normal(loc=0., scale=1., name=\"x\")\n    y = ed.Normal(loc=x, scale=1., name=\"y\")\n    return x + y\n\n  def double(f, *args, **kwargs):\n    return 2. * interceptable(f)(*args, **kwargs)\n\n  def set_y(f, *args, **kwargs):\n    if kwargs.get(\"name\") == \"y\":\n      kwargs[\"value\"] = 0.42\n    return interceptable(f)(*args, **kwargs)\n\n  with interception(double):\n    with interception(set_y):\n      z = model()\n  ```\n\n  This will firstly put `double` on the stack, and then `set_y`,\n  resulting in the stack:\n  (TOP) set_y -> double -> apply (BOTTOM)\n\n  The execution of `model` is then (top lines are current stack state):\n  1) (TOP) set_y -> double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"x\")` is intercepted by `set_y`, and as the name is not \"y\"\n  the operation is simply forwarded to the next interceptor on the stack.\n\n  2) (TOP) double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"x\")` is intercepted by `double`, to produce\n  `2*ed.Normal(0., 1., \"x\")`, with the operation being forwarded down the stack.\n\n  3) (TOP) apply (BOTTOM);\n  `ed.Normal(0., 1., \"x\")` is intercepted by `apply`, which simply calls the\n  constructor.\n\n  (At this point, the nested calls to `get_next_interceptor()`, produced by\n  forwarding operations, exit, and the current stack is again:\n  (TOP) set_y -> double -> apply (BOTTOM))", "suffix": "  4) (TOP) set_y -> double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"y\")` is intercepted by `set_y`,\n  the value of `y` is set to 0.42 and the operation is forwarded down the stack.\n\n  5) (TOP) double -> apply (BOTTOM);\n  `ed.Normal(0., 1., \"y\")` is intercepted by `double`, to produce\n  `2*ed.Normal(0., 1., \"y\")`, with the operation being forwarded down the stack.\n\n  6) (TOP) apply (BOTTOM);\n  `ed.Normal(0., 1., \"y\")` is intercepted by `apply`, which simply calls the\n  constructor.\n\n  The final values for `x` and `y` inside of `model()` are tensors where `x` is\n  a random draw from Normal(0., 1.) doubled, and `y` is a constant 0.84, thus\n  z = 2 * Normal(0., 1.) + 0.84.\n  \"\"\"\n  try:\n    interceptor = _interceptor_stack.stack.pop()\n    yield interceptor\n  finally:\n    _interceptor_stack.stack.append(interceptor)", "gt": ""}
{"prefix": "def interceptable(func):\n  \"\"\"Decorator that wraps `func` so that its execution is intercepted.\n\n  The wrapper passes `func` to the interceptor for the current thread.\n", "suffix": "  That is, `func` terminates without forwarding its execution to another\n  interceptor.\n\n  Args:\n    func: Function to wrap.\n\n  Returns:\n    The decorated function.\n  \"\"\"\n  @functools.wraps(func)\n  def func_wrapped(*args, **kwargs):\n    with get_next_interceptor() as interceptor:\n      return interceptor(func, *args, **kwargs)\n\n  return func_wrapped", "gt": "  If there is no next interceptor, we perform an \"immediate\" call to `func`."}
{"prefix": "def tape():\n  \"\"\"Context manager for recording interceptable executions onto a tape.\n\n  Similar to `tf.GradientTape`, operations are recorded if they are executed\n  within this context manager. In addition, the operation must be registered\n  (wrapped) as `ed.interceptable`.\n\n  Yields:\n    tape: OrderedDict where operations are recorded in sequence. Keys are\n      the `name` keyword argument to the operation (typically, a random\n      variable's `name`) and values are the corresponding output of the\n      operation. If the operation has no name, it is not recorded.\n\n  #### Examples\n\n  ```python\n  from tensorflow_probability import edward2 as ed\n\n  def probabilistic_matrix_factorization():\n    users = ed.Normal(0., 1., sample_shape=[5000, 128], name=\"users\")\n    items = ed.Normal(0., 1., sample_shape=[7500, 128], name=\"items\")\n    ratings = ed.Normal(loc=tf.matmul(users, items, transpose_b=True),\n                        scale=0.1,\n                        name=\"ratings\")\n    return ratings\n\n  with ed.tape() as model_tape:\n    ratings = probabilistic_matrix_factorization()\n\n  assert model_tape[\"users\"].shape == (5000, 128)\n  assert model_tape[\"items\"].shape == (7500, 128)\n  assert model_tape[\"ratings\"] == ratings\n  ```\n\n  \"\"\"\n  tape_data = collections.OrderedDict({})\n\n  def record(f, *args, **kwargs):\n    \"\"\"Records execution to a tape.\"\"\"", "suffix": "    output = interceptable(f)(*args, **kwargs)\n    if name:\n      tape_data[name] = output\n    return output\n\n  with interception(record):\n    yield tape_data", "gt": "    name = kwargs.get(\"name\")"}
{"prefix": "def toy_logistic_data(num_examples, input_size=2, weights_prior_stddev=5.0):\n  \"\"\"Generates synthetic data for binary classification.\n\n  Args:\n    num_examples: The number of samples to generate (scalar Python `int`).\n    input_size: The input space dimension (scalar Python `int`).\n    weights_prior_stddev: The prior standard deviation of the weight\n      vector. (scalar Python `float`).\n\n  Returns:\n    random_weights: Sampled weights as a Numpy `array` of shape\n      `[input_size]`.\n    random_bias: Sampled bias as a scalar Python `float`.\n    design_matrix: Points sampled uniformly from the cube `[-1,\n       1]^{input_size}`, as a Numpy `array` of shape `(num_examples,\n       input_size)`.\n    labels: Labels sampled from the logistic model `p(label=1) =\n      logistic(dot(features, random_weights) + random_bias)`, as a Numpy\n      `int32` `array` of shape `(num_examples, 1)`.\n  \"\"\"\n  random_weights = weights_prior_stddev * np.random.randn(input_size)\n  random_bias = np.random.randn()\n  design_matrix = np.random.rand(num_examples, input_size) * 2 - 1\n  logits = np.reshape(", "suffix": "      (-1, 1))\n  p_labels = 1. / (1 + np.exp(-logits))\n  labels = np.int32(p_labels > np.random.rand(num_examples, 1))\n  return random_weights, random_bias, np.float32(design_matrix), labels", "gt": "      np.dot(design_matrix, random_weights) + random_bias,"}
{"prefix": "def visualize_decision(features, labels, true_w_b, candidate_w_bs, fname):\n  \"\"\"Utility method to visualize decision boundaries in R^2.\n\n  Args:\n    features: Input points, as a Numpy `array` of shape `[num_examples, 2]`.\n    labels: Numpy `float`-like array of shape `[num_examples, 1]` giving a\n      label for each point.\n    true_w_b: A `tuple` `(w, b)` where `w` is a Numpy array of\n       shape `[2]` and `b` is a scalar `float`, interpreted as a\n       decision rule of the form `dot(features, w) + b > 0`.\n    candidate_w_bs: Python `iterable` containing tuples of the same form as\n       true_w_b.\n    fname: The filename to save the plot as a PNG image (Python `str`).\n  \"\"\"\n  fig = figure.Figure(figsize=(6, 6))\n  canvas = backend_agg.FigureCanvasAgg(fig)\n  ax = fig.add_subplot(1, 1, 1)\n  ax.scatter(features[:, 0], features[:, 1],\n             c=np.float32(labels[:, 0]),\n             cmap=cm.get_cmap(\"binary\"),\n             edgecolors=\"k\")\n\n  def plot_weights(w, b, **kwargs):\n    w1, w2 = w\n    x1s = np.linspace(-1, 1, 100)\n    x2s = -(w1  * x1s + b) / w2\n    ax.plot(x1s, x2s, **kwargs)\n\n  for w, b in candidate_w_bs:\n    plot_weights(w, b,\n                 alpha=1./np.sqrt(len(candidate_w_bs)),\n                 lw=1, color=\"blue\")\n\n  if true_w_b is not None:\n    plot_weights(*true_w_b, lw=4,\n                 color=\"green\", label=\"true separator\")\n", "suffix": "  ax.set_ylim([-1.5, 1.5])\n  ax.legend()\n\n  canvas.print_figure(fname, format=\"png\")\n  print(\"saved {}\".format(fname))", "gt": "  ax.set_xlim([-1.5, 1.5])"}
{"prefix": "def build_input_pipeline(x, y, batch_size):\n  \"\"\"Build a Dataset iterator for supervised classification.\n\n  Args:\n    x: Numpy `array` of features, indexed by the first dimension.\n    y: Numpy `array` of labels, with the same first dimension as `x`.\n    batch_size: Number of elements in each training batch.\n\n  Returns:\n    batch_features: `Tensor` feed  features, of shape\n      `[batch_size] + x.shape[1:]`.\n    batch_labels: `Tensor` feed of labels, of shape\n      `[batch_size] + y.shape[1:]`.\n  \"\"\"\n  training_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n  training_batches = training_dataset.repeat().batch(batch_size)\n  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n  batch_features, batch_labels = training_iterator.get_next()", "suffix": "", "gt": "  return batch_features, batch_labels"}
{"prefix": "def _maybe_check_valid_map_values(map_values, validate_args):\n  \"\"\"Validate `map_values` if `validate_args`==True.\"\"\"\n  assertions = []\n\n  message = 'Rank of map_values must be 1.'\n  if tensorshape_util.rank(map_values.shape) is not None:\n    if tensorshape_util.rank(map_values.shape) != 1:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(assert_util.assert_rank(map_values, 1, message=message))\n\n  message = 'Size of map_values must be greater than 0.'\n  if tensorshape_util.num_elements(map_values.shape) is not None:\n    if tensorshape_util.num_elements(map_values.shape) == 0:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(", "suffix": "            tf.size(input=map_values), 0, message=message))\n\n  if validate_args:\n    assertions.append(\n        assert_util.assert_equal(\n            tf.math.is_strictly_increasing(map_values),\n            True,\n            message='map_values is not strictly increasing.'))\n\n  return assertions", "gt": "        assert_util.assert_greater("}
{"prefix": "def trace(state: State, fn: TransitionOperator, num_steps: IntTensor,\n          trace_fn: Callable[[State, TensorNest], TensorNest]\n         ) -> Tuple[State, TensorNest]:\n  \"\"\"`TransitionOperator` that runs `fn` repeatedly and traces its outputs.\n\n  Args:\n    state: A nest of `Tensor`s or None.\n    fn: A `TransitionOperator`.\n    num_steps: Number of steps to run the function for. Must be greater than 1.\n    trace_fn: Callable that the unpacked outputs of `fn` and returns a nest of\n      `Tensor`s. These will be stacked and returned.\n\n  Returns:", "suffix": "    traces: Stacked outputs of `trace_fn`.\n  \"\"\"\n\n  def fn_wrapper(args, _):\n    return tf.nest.map_structure(tf.convert_to_tensor, call_fn(fn, args[0]))\n\n  def trace_fn_wrapper(args):\n    return tf.nest.map_structure(tf.convert_to_tensor, call_fn(trace_fn, args))\n\n  state = call_fn(fn, state)\n  first_trace = trace_fn_wrapper(state)\n\n  state, full_trace = mcmc_util.trace_scan(\n      fn_wrapper, state, tf.ones(num_steps - 1), trace_fn=trace_fn_wrapper)\n\n  prepend = lambda x, y: tf.concat(  # pylint: disable=g-long-lambda\n      [tf.convert_to_tensor(value=x)[tf.newaxis], y], 0)\n\n  return state, tf.nest.map_structure(prepend, first_trace, full_trace)", "gt": "    state: The final state returned by `fn`."}
{"prefix": "def call_fn(fn: TransitionOperator, args: Union[Tuple[Any], Any]) -> Any:\n  \"\"\"Calls a transition operator with args, unpacking args if its a sequence.\n\n  Args:\n    fn: A `TransitionOperator`.\n    args: Arguments to `fn`\n\n  Returns:\n    ret: Return value of `fn`.\n  \"\"\"\n\n  if isinstance(args, (list, tuple)) and not mcmc_util.is_namedtuple_like(args):\n    args = args  # type: Tuple[Any]\n    return fn(*args)\n  else:", "suffix": "", "gt": "    return fn(args)"}
{"prefix": "def call_and_grads(fn: TransitionOperator, args: Union[Tuple[Any], Any]\n                  ) -> Tuple[tf.Tensor, TensorNest, TensorNest]:\n  \"\"\"Calls `fn` and returns the gradients with respect to `fn`'s first output.\n\n  Args:\n    fn: A `TransitionOperator`.\n    args: Arguments to `fn`", "suffix": "  Returns:\n    ret: First output of `fn`.\n    extra: Second output of `fn`.\n    grads: Gradients of `ret` with respect to `args`.\n  \"\"\"\n  with tf.GradientTape() as tape:\n    tape.watch(args)\n    ret, extra = call_fn(fn, args)\n  grads = tape.gradient(ret, args)\n  return ret, extra, grads", "gt": ""}
{"prefix": "def maybe_broadcast_structure(from_structure: Any, to_structure: Any) -> Any:\n  \"\"\"Maybe broadcasts `from_structure` to `to_structure`.", "suffix": "  If `from_structure` is a singleton, it is tiled to match the structure of\n  `to_structure`. Note that the elements in `from_structure` are not copied if\n  this tiling occurs.\n\n  Args:\n    from_structure: A structure.\n    to_structure: A structure.\n\n  Returns:\n    new_from_structure: Same structure as `to_structure`.\n  \"\"\"\n  flat_from = tf.nest.flatten(from_structure)\n  flat_to = tf.nest.flatten(to_structure)\n  if len(flat_from) == 1:\n    flat_from *= len(flat_to)\n  return tf.nest.pack_sequence_as(to_structure, flat_from)", "gt": ""}
{"prefix": "def transform_log_prob_fn(log_prob_fn: PotentialFn,\n                          bijector: BijectorNest,\n                          init_state: State = None\n                         ) -> Union[PotentialFn, Tuple[PotentialFn, State]]:\n  \"\"\"Transforms a log-prob function using a bijector.\n\n  This takes a log-prob function and creates a new log-prob function that now\n  takes takes state in the domain of the bijector, forward transforms that state\n  and calls the original log-prob function. It then returns the log-probability\n  that correctly accounts for this transformation.\n\n  The forward-transformed state is pre-pended to the original log-prob\n  function's extra returns and returned as the new extra return.\n\n  For convenience you can also pass the initial state (in the original space),\n  and this function will return the inverse transformed as the 2nd return value.\n  You'd use this to initialize MCMC operators that operate in the transformed\n  space.\n\n  Args:\n    log_prob_fn: Log prob fn.\n    bijector: Bijector(s), must be of the same structure as the `log_prob_fn`\n      inputs.", "suffix": "\n  Returns:\n    transformed_log_prob_fn: Transformed log prob fn.\n    transformed_init_state: If `init_state` is provided. Initial state in the\n      transformed space.\n  \"\"\"\n\n  def wrapper(*args):\n    \"\"\"Transformed wrapper.\"\"\"\n    bijector_ = bijector\n\n    args = tf.nest.map_structure(lambda x: 0. + x, args)\n    if len(args) == 1:\n      args = args[0]\n    elif isinstance(bijector_, list):\n      bijector_ = tuple(bijector_)\n\n    original_space_args = tf.nest.map_structure(lambda b, x: b.forward(x),\n                                                bijector_, args)\n    original_space_args = original_space_args  # type: Tuple[Any]\n    original_space_log_prob, extra = call_fn(log_prob_fn, original_space_args)\n    event_ndims = tf.nest.map_structure(\n        lambda x: tf.rank(x) - tf.rank(original_space_log_prob), args)\n\n    return original_space_log_prob + sum(\n        tf.nest.flatten(\n            tf.nest.map_structure(\n                lambda b, x, e: b.forward_log_det_jacobian(x, event_ndims=e),\n                bijector_, args, event_ndims))), [original_space_args, extra]\n\n  if init_state is None:\n    return wrapper\n  else:\n    return wrapper, tf.nest.map_structure(lambda b, s: b.inverse(s), bijector,\n                                          init_state)", "gt": "    init_state: Initial state, in the original space."}
{"prefix": "def leapfrog_step(leapfrog_step_state: LeapFrogStepState,\n                  step_size: FloatTensor, target_log_prob_fn: PotentialFn,\n                  kinetic_energy_fn: PotentialFn\n                 ) -> Tuple[LeapFrogStepState, LeapFrogStepExtras]:\n  \"\"\"Leapfrog `TransitionOperator`.\n\n  Args:\n    leapfrog_step_state: LeapFrogStepState.\n    step_size: Step size, structure broadcastable to the `target_log_prob_fn`\n      state.\n    target_log_prob_fn: Target log prob fn.\n    kinetic_energy_fn: Kinetic energy fn.\n\n  Returns:\n    leapfrog_step_state: LeapFrogStepState.\n    leapfrog_step_extras: LeapFrogStepExtras.\n  \"\"\"\n  state = leapfrog_step_state.state\n  state_grads = leapfrog_step_state.state_grads\n  momentum = leapfrog_step_state.momentum\n  step_size = maybe_broadcast_structure(step_size, state)\n\n  state = tf.nest.map_structure(tf.convert_to_tensor, state)\n  momentum = tf.nest.map_structure(tf.convert_to_tensor, momentum)\n  state = tf.nest.map_structure(tf.convert_to_tensor, state)\n\n  if state_grads is None:\n    _, _, state_grads = call_and_grads(target_log_prob_fn, state)", "suffix": "    state_grads = tf.nest.map_structure(tf.convert_to_tensor, state_grads)\n\n  momentum = tf.nest.map_structure(lambda m, sg, s: m + 0.5 * sg * s, momentum,\n                                   state_grads, step_size)\n\n  kinetic_energy, kinetic_energy_extra, momentum_grads = call_and_grads(\n      kinetic_energy_fn, momentum)\n\n  state = tf.nest.map_structure(lambda x, mg, s: x + mg * s, state,\n                                momentum_grads, step_size)\n\n  target_log_prob, state_extra, state_grads = call_and_grads(\n      target_log_prob_fn, state)\n\n  momentum = tf.nest.map_structure(lambda m, sg, s: m + 0.5 * sg * s, momentum,\n                                   state_grads, step_size)\n\n  return LeapFrogStepState(state, state_grads, momentum), LeapFrogStepExtras(\n      target_log_prob, state_extra, kinetic_energy, kinetic_energy_extra)", "gt": "  else:"}
{"prefix": "def metropolis_hastings_step(current_state: State,\n                             proposed_state: State,\n                             energy_change: FloatTensor,\n                             seed=None) -> Tuple[State, tf.Tensor, tf.Tensor]:\n  \"\"\"Metropolis-Hastings step.\n\n  This probabilistically chooses between `current_state` and `proposed_state`\n  based on the `energy_change` so as to preserve detailed balance.\n\n  Energy change is the negative of `log_accept_ratio`.\n\n  Args:\n    current_state: Current state.\n    proposed_state: Proposed state.\n    energy_change: E(proposed_state) - E(previous_state).\n    seed: For reproducibility.\n\n  Returns:\n    new_state: The chosen state.\n    is_accepted: Whether the proposed state was accepted.\n    log_uniform: The random number that was used to select between the two\n      states.\n  \"\"\"\n  flat_current = tf.nest.flatten(current_state)\n  flat_proposed = nest.flatten_up_to(current_state, proposed_state)\n  # Impute the None's in the current state.\n  flat_current = [\n      p if c is None else c for p, c in zip(flat_proposed, flat_current)\n  ]\n  current_state = tf.nest.pack_sequence_as(current_state, flat_current)\n\n  current_state = tf.nest.map_structure(tf.convert_to_tensor, current_state)\n  proposed_state = tf.nest.map_structure(tf.convert_to_tensor, proposed_state)\n  energy_change = tf.convert_to_tensor(value=energy_change)\n\n  log_accept_ratio = -energy_change\n\n  log_uniform = tf.math.log(\n      tf.random.uniform(\n          shape=tf.shape(input=log_accept_ratio),\n          dtype=log_accept_ratio.dtype.base_dtype,\n          seed=seed))\n  is_accepted = log_uniform < log_accept_ratio\n\n  next_state = mcmc_util.choose(", "suffix": "  return next_state, is_accepted, log_uniform", "gt": "      is_accepted, proposed_state, current_state, name='choose_next_state')"}
{"prefix": "def hamiltonian_monte_carlo(\n    hmc_state: HamiltonianMonteCarloState,\n    target_log_prob_fn: PotentialFn,\n    step_size: Any,\n    num_leapfrog_steps: IntTensor,\n    momentum: State = None,\n    kinetic_energy_fn: PotentialFn = None,\n    momentum_sample_fn: MomentumSampleFn = None,\n    leapfrog_trace_fn: Callable[[LeapFrogStepState, LeapFrogStepExtras],\n                                TensorNest] = lambda *args: (),\n    seed=None,\n) -> Tuple[HamiltonianMonteCarloState, HamiltonianMonteCarloExtra]:\n  \"\"\"Hamiltonian Monte Carlo `TransitionOperator`.\n\n  #### Example\n\n  ```python\n  step_size = 0.2\n  num_steps = 2000\n  num_leapfrog_steps = 10\n  state = tf.ones([16, 2])\n\n  base_mean = [1., 0]\n  base_cov = [[1, 0.5], [0.5, 1]]\n\n  bijector = tfb.Softplus()\n  base_dist = tfd.MultivariateNormalFullCovariance(\n      loc=base_mean, covariance_matrix=base_cov)\n  target_dist = bijector(base_dist)\n\n  def orig_target_log_prob_fn(x):\n    return target_dist.log_prob(x), ()\n\n  target_log_prob_fn, state = fun_mcmc.transform_log_prob_fn(\n      orig_target_log_prob_fn, bijector, state)\n\n  kernel = tf.function(lambda state: fun_mcmc.hamiltonian_monte_carlo(\n      state,\n      step_size=step_size,\n      num_leapfrog_steps=num_leapfrog_steps,\n      target_log_prob_fn=target_log_prob_fn,\n      seed=tfp_test_util.test_seed()))\n\n  _, chain = fun_mcmc.trace(\n      state=fun_mcmc.HamiltonianMonteCarloState(\n          state=state,\n          state_grads=None,\n          target_log_prob=None,\n          state_extra=None),\n      fn=kernel,\n      num_steps=num_steps,\n      trace_fn=lambda state, extra: state.state_extra[0])\n  ```\n\n  Args:\n    hmc_state: HamiltonianMonteCarloState.\n    target_log_prob_fn: Target log prob fn.\n    step_size: Step size, structure broadcastable to the `target_log_prob_fn`\n      state.\n    num_leapfrog_steps: Number of leapfrog steps to take.\n    momentum: Initial momentum, passed to `momentum_sample_fn`. Default: zeroes.\n    kinetic_energy_fn: Kinetic energy function.\n    momentum_sample_fn: Sampler for the momentum.\n    leapfrog_trace_fn: Trace function for the leapfrog integrator.\n    seed: For reproducibility.\n\n  Returns:\n    hmc_state: HamiltonianMonteCarloState\n    hmc_extra: HamiltonianMonteCarloExtra\n  \"\"\"\n  state = hmc_state.state\n  state_grads = hmc_state.state_grads\n  target_log_prob = hmc_state.target_log_prob\n  state_extra = hmc_state.state_extra\n\n  if kinetic_energy_fn is None:\n\n    # pylint: disable=function-redefined\n    def kinetic_energy_fn(*momentum):\n      return tf.add_n([\n          tf.reduce_sum(input_tensor=tf.square(x), axis=-1) / 2.\n          for x in tf.nest.flatten(momentum)\n      ]), ()\n\n  if momentum_sample_fn is None:\n\n    # pylint: disable=function-redefined\n    def momentum_sample_fn(*momentum):\n      ret = tf.nest.map_structure(\n          lambda x: tf.random.normal(tf.shape(input=x), dtype=x.dtype),\n          momentum)\n      if len(ret) == 1:\n        return ret[0]\n      else:\n        return ret\n\n  if momentum is None:\n    momentum = call_fn(momentum_sample_fn,\n                       tf.nest.map_structure(tf.zeros_like, state))\n  if target_log_prob is None:\n    target_log_prob, state_extra, state_grads = call_and_grads(\n        target_log_prob_fn, state)\n\n  kinetic_energy, _ = call_fn(kinetic_energy_fn, momentum)\n  current_energy = -target_log_prob + kinetic_energy\n  current_state = HamiltonianMonteCarloState(\n      state=state,\n      state_grads=state_grads,\n      state_extra=state_extra,\n      target_log_prob=target_log_prob)\n\n  def leapfrog_wrapper(leapfrog_state, target_log_prob, state_extra):\n    \"\"\"Leapfrog wrapper that tracks extra state.\"\"\"\n    del target_log_prob\n    del state_extra\n\n    leapfrog_state, leapfrog_extra = leapfrog_step(\n        leapfrog_state,\n        step_size=step_size,\n        target_log_prob_fn=target_log_prob_fn,\n        kinetic_energy_fn=kinetic_energy_fn)\n\n    return [\n        leapfrog_state, leapfrog_extra.target_log_prob,\n        leapfrog_extra.state_extra\n    ], leapfrog_extra\n", "suffix": "    return leapfrog_trace_fn(args[0], leapfrog_extra)\n\n  leapfrog_wrapper_state = (LeapFrogStepState(state, state_grads, momentum),\n                            target_log_prob, state_extra)\n\n  [[leapfrog_state, target_log_prob, state_extra], _], leapfrog_trace = trace(\n      leapfrog_wrapper_state,\n      leapfrog_wrapper,\n      num_leapfrog_steps,\n      trace_fn=leapfrog_trace_wrapper_fn)\n\n  kinetic_energy, _ = call_fn(kinetic_energy_fn, leapfrog_state.momentum)\n  proposed_energy = -target_log_prob + kinetic_energy\n  proposed_state = HamiltonianMonteCarloState(\n      state=leapfrog_state.state,\n      state_grads=leapfrog_state.state_grads,\n      target_log_prob=target_log_prob,\n      state_extra=state_extra)\n\n  energy_change = proposed_energy - current_energy\n  hmc_state, is_accepted, _ = metropolis_hastings_step(\n      current_state, proposed_state, energy_change, seed=seed)\n\n  hmc_state = hmc_state  # type: HamiltonianMonteCarloState\n  return hmc_state, HamiltonianMonteCarloExtra(\n      is_accepted=is_accepted,\n      proposed_hmc_state=proposed_state,\n      log_accept_ratio=-energy_change,\n      leapfrog_trace=leapfrog_trace)", "gt": "  def leapfrog_trace_wrapper_fn(args, leapfrog_extra):"}
{"prefix": "", "suffix": "                    output: FloatTensor,\n                    set_point: FloatTensor,\n                    adaptation_rate: FloatTensor = 0.01) -> FloatNest:\n  \"\"\"A function to do simple sign-based control of a variable.\n\n  ```\n  control = control * (1. + adaptation_rate) ** sign(output - set_point)\n  ```\n\n  Args:\n    control: The control variable.\n    output: The output variable.\n    set_point: The set point for `output`. This function will adjust `control`\n      so that `output` matches `set_point`.\n    adaptation_rate: Adaptation rate.\n\n  Returns:\n    control: New control.\n  \"\"\"\n\n  def _get_new_control(control, output, set_point):\n    new_control = mcmc_util.choose(output > set_point,\n                                   control * (1. + adaptation_rate),\n                                   control / (1. + adaptation_rate))\n    return new_control\n\n  output = maybe_broadcast_structure(output, control)\n  set_point = maybe_broadcast_structure(set_point, control)\n\n  return tf.nest.map_structure(_get_new_control, control, output, set_point)", "gt": "def sign_adaptation(control: FloatNest,"}
{"prefix": "def compute_output_shape(self, input_shape):\n    \"\"\"Computes the output shape of the layer.\n\n    Args:\n      input_shape: Shape tuple (tuple of integers) or list of shape tuples\n        (one per output tensor of the layer). Shape tuples can include None for\n        free dimensions, instead of an integer.\n\n    Returns:", "suffix": "    \"\"\"\n    input_shape = tf.TensorShape(input_shape).as_list()\n    if self.data_format == 'channels_last':\n      space = input_shape[1:-1]\n      new_space = []\n      for i in range(len(space)):\n        new_dim = tf_layers_util.conv_output_length(\n            space[i],\n            self.kernel_size[i],\n            padding=self.padding,\n            stride=self.strides[i],\n            dilation=self.dilation_rate[i])\n        new_space.append(new_dim)\n      return tf.TensorShape([input_shape[0]] + new_space + [self.filters])\n    else:\n      space = input_shape[2:]\n      new_space = []\n      for i in range(len(space)):\n        new_dim = tf_layers_util.conv_output_length(\n            space[i],\n            self.kernel_size[i],\n            padding=self.padding,\n            stride=self.strides[i],\n            dilation=self.dilation_rate[i])\n        new_space.append(new_dim)\n      return tf.TensorShape([input_shape[0], self.filters] + new_space)", "gt": "      output_shape: A tuple representing the output shape."}
{"prefix": "def get_config(self):\n    \"\"\"Returns the config of the layer.\n\n    A layer config is a Python dictionary (serializable) containing the\n    configuration of a layer. The same layer can be reinstantiated later\n    (without its trained weights) from this configuration.\n\n    Returns:\n      config: A Python dictionary of class keyword arguments and their\n        serialized values.\n    \"\"\"\n    config = {\n        'filters': self.filters,\n        'kernel_size': self.kernel_size,\n        'strides': self.strides,", "suffix": "        'data_format': self.data_format,\n        'dilation_rate': self.dilation_rate,\n        'activation': (tf.keras.activations.serialize(self.activation)\n                       if self.activation else None),\n        'activity_regularizer':\n            tf.keras.initializers.serialize(self.activity_regularizer),\n    }\n    function_keys = [\n        'kernel_posterior_fn',\n        'kernel_posterior_tensor_fn',\n        'kernel_prior_fn',\n        'kernel_divergence_fn',\n        'bias_posterior_fn',\n        'bias_posterior_tensor_fn',\n        'bias_prior_fn',\n        'bias_divergence_fn',\n    ]\n    for function_key in function_keys:\n      function = getattr(self, function_key)\n      if function is None:\n        function_name = None\n        function_type = None\n      else:\n        function_name, function_type = tfp_layers_util.serialize_function(\n            function)\n      config[function_key] = function_name\n      config[function_key + '_type'] = function_type\n    base_config = super(_ConvVariational, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))", "gt": "        'padding': self.padding,"}
{"prefix": "def from_config(cls, config):\n    \"\"\"Creates a layer from its config.\n\n    This method is the reverse of `get_config`, capable of instantiating the\n    same layer from the config dictionary.\n\n    Args:\n      config: A Python dictionary, typically the output of `get_config`.\n\n    Returns:\n      layer: A layer instance.\n    \"\"\"\n    config = config.copy()\n    function_keys = [\n        'kernel_posterior_fn',\n        'kernel_posterior_tensor_fn',\n        'kernel_prior_fn',\n        'kernel_divergence_fn',\n        'bias_posterior_fn',\n        'bias_posterior_tensor_fn',\n        'bias_prior_fn',\n        'bias_divergence_fn',\n    ]\n    for function_key in function_keys:\n      serial = config[function_key]\n      function_type = config.pop(function_key + '_type')\n      if serial is not None:\n        config[function_key] = tfp_layers_util.deserialize_function(\n            serial,", "suffix": "    return cls(**config)", "gt": "            function_type=function_type)"}
{"prefix": "def get_config(self):\n    \"\"\"Returns the config of the layer.\n\n    A layer config is a Python dictionary (serializable) containing the\n    configuration of a layer. The same layer can be reinstantiated later\n    (without its trained weights) from this configuration.\n\n    Returns:\n      config: A Python dictionary of class keyword arguments and their", "suffix": "    \"\"\"\n    config = {\n        'seed': self.seed,\n    }\n    base_config = super(_ConvFlipout, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))", "gt": "        serialized values."}
{"prefix": "def _as_tensor(x, name, dtype):\n  \"\"\"Convenience to convert to `Tensor` or leave as `None`.\"\"\"\n  return None if x is None else tf.convert_to_tensor(", "suffix": "", "gt": "      value=x, name=name, dtype=dtype)"}
{"prefix": "def _create_scale_operator(self, identity_multiplier, diag, tril,\n                             perturb_diag, perturb_factor, shift, validate_args,\n                             dtype):\n    \"\"\"Construct `scale` from various components.\n\n    Args:\n      identity_multiplier: floating point rank 0 `Tensor` representing a scaling\n        done to the identity matrix.\n      diag: Floating-point `Tensor` representing the diagonal matrix.`diag` has\n        shape `[N1, N2, ...  k]`, which represents a k x k diagonal matrix.\n      tril: Floating-point `Tensor` representing the lower triangular matrix.\n       `tril` has shape `[N1, N2, ...  k, k]`, which represents a k x k lower\n       triangular matrix.\n      perturb_diag: Floating-point `Tensor` representing the diagonal matrix of\n        the low rank update.\n      perturb_factor: Floating-point `Tensor` representing factor matrix.\n      shift: Floating-point `Tensor` representing `shift in `scale @ X + shift`.\n      validate_args: Python `bool` indicating whether arguments should be\n        checked for correctness.\n      dtype: `DType` for arg `Tensor` conversions.\n\n    Returns:\n      scale. In the case of scaling by a constant, scale is a\n      floating point `Tensor`. Otherwise, scale is a `LinearOperator`.\n\n    Raises:\n      ValueError: if all of `tril`, `diag` and `identity_multiplier` are `None`.\n    \"\"\"\n    identity_multiplier = _as_tensor(identity_multiplier, \"identity_multiplier\",\n                                     dtype)\n    diag = _as_tensor(diag, \"diag\", dtype)\n    tril = _as_tensor(tril, \"tril\", dtype)\n    perturb_diag = _as_tensor(perturb_diag, \"perturb_diag\", dtype)\n    perturb_factor = _as_tensor(perturb_factor, \"perturb_factor\", dtype)\n\n    # If possible, use the low rank update to infer the shape of\n    # the identity matrix, when scale represents a scaled identity matrix\n    # with a low rank update.\n    shape_hint = None\n    if perturb_factor is not None:\n      shape_hint = distribution_util.dimension_size(perturb_factor, axis=-2)\n\n    if self._is_only_identity_multiplier:\n      if validate_args:\n        return distribution_util.with_dependencies([\n            assert_util.assert_none_equal(\n                identity_multiplier, tf.zeros([], identity_multiplier.dtype),\n                [\"identity_multiplier should be non-zero.\"])\n        ], identity_multiplier)\n      return identity_multiplier\n\n    scale = distribution_util.make_tril_scale(\n        loc=shift,\n        scale_tril=tril,\n        scale_diag=diag,\n        scale_identity_multiplier=identity_multiplier,\n        validate_args=validate_args,\n        assert_positive=False,\n        shape_hint=shape_hint)\n", "suffix": "      return tf.linalg.LinearOperatorLowRankUpdate(\n          scale,\n          u=perturb_factor,\n          diag_update=perturb_diag,\n          is_diag_update_positive=perturb_diag is None,\n          is_non_singular=True,  # Implied by is_positive_definite=True.\n          is_self_adjoint=True,\n          is_positive_definite=True,\n          is_square=True)\n\n    return scale", "gt": "    if perturb_factor is not None:"}
{"prefix": "def random_walk_normal_fn(scale=1., name=None):\n  \"\"\"Returns a callable that adds a random normal perturbation to the input.\n\n  This function returns a callable that accepts a Python `list` of `Tensor`s of\n  any shapes and `dtypes`  representing the state parts of the `current_state`\n  and a random seed. The supplied argument `scale` must be a `Tensor` or Python\n  `list` of `Tensor`s representing the scale of the generated\n  proposal. `scale` must broadcast with the state parts of `current_state`.\n  The callable adds a sample from a zero-mean normal distribution with the\n  supplied scales to each state part and returns a same-type `list` of `Tensor`s\n  as the state parts of `current_state`.\n\n  Args:\n    scale: a `Tensor` or Python `list` of `Tensor`s of any shapes and `dtypes`\n      controlling the scale of the normal proposal distribution.\n    name: Python `str` name prefixed to Ops created by this function.\n        Default value: 'random_walk_normal_fn'.\n\n  Returns:\n    random_walk_normal_fn: A callable accepting a Python `list` of `Tensor`s\n      representing the state parts of the `current_state` and an `int`\n      representing the random seed to be used to generate the proposal. The\n      callable returns the same-type `list` of `Tensor`s as the input and\n      represents the proposal for the RWM algorithm.", "suffix": "  def _fn(state_parts, seed):\n    \"\"\"Adds a normal perturbation to the input state.\n\n    Args:\n      state_parts: A list of `Tensor`s of any shape and real dtype representing\n        the state parts of the `current_state` of the Markov chain.\n      seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n        applied.\n        Default value: `None`.\n\n    Returns:\n      perturbed_state_parts: A Python `list` of The `Tensor`s. Has the same\n        shape and type as the `state_parts`.\n\n    Raises:\n      ValueError: if `scale` does not broadcast with `state_parts`.\n    \"\"\"\n    with tf.compat.v1.name_scope(\n        name, 'random_walk_normal_fn', values=[state_parts, scale, seed]):\n      scales = scale if mcmc_util.is_list_like(scale) else [scale]\n      if len(scales) == 1:\n        scales *= len(state_parts)\n      if len(state_parts) != len(scales):\n        raise ValueError('`scale` must broadcast with `state_parts`.')\n      seed_stream = distributions.SeedStream(seed, salt='RandomWalkNormalFn')\n      next_state_parts = [\n          tf.random.normal(\n              mean=state_part,\n              stddev=scale_part,\n              shape=tf.shape(input=state_part),\n              dtype=state_part.dtype.base_dtype,\n              seed=seed_stream())\n          for scale_part, state_part in zip(scales, state_parts)\n      ]\n\n      return next_state_parts\n  return _fn", "gt": "  \"\"\""}
{"prefix": "def random_walk_uniform_fn(scale=1., name=None):\n  \"\"\"Returns a callable that adds a random uniform perturbation to the input.\n\n  For more details on `random_walk_uniform_fn`, see\n  `random_walk_normal_fn`. `scale` might\n  be a `Tensor` or a list of `Tensor`s that should broadcast with state parts\n  of the `current_state`. The generated uniform perturbation is sampled as a\n  uniform point on the rectangle `[-scale, scale]`.\n\n  Args:\n    scale: a `Tensor` or Python `list` of `Tensor`s of any shapes and `dtypes`\n      controlling the upper and lower bound of the uniform proposal\n      distribution.\n    name: Python `str` name prefixed to Ops created by this function.\n        Default value: 'random_walk_uniform_fn'.", "suffix": "  Returns:\n    random_walk_uniform_fn: A callable accepting a Python `list` of `Tensor`s\n      representing the state parts of the `current_state` and an `int`\n      representing the random seed used to generate the proposal. The callable\n      returns the same-type `list` of `Tensor`s as the input and represents the\n      proposal for the RWM algorithm.\n  \"\"\"\n  def _fn(state_parts, seed):\n    \"\"\"Adds a uniform perturbation to the input state.\n\n    Args:\n      state_parts: A list of `Tensor`s of any shape and real dtype representing\n        the state parts of the `current_state` of the Markov chain.\n      seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n        applied.\n        Default value: `None`.\n\n    Returns:\n      perturbed_state_parts: A Python `list` of The `Tensor`s. Has the same\n        shape and type as the `state_parts`.\n\n    Raises:\n      ValueError: if `scale` does not broadcast with `state_parts`.\n    \"\"\"\n    with tf.compat.v1.name_scope(\n        name, 'random_walk_uniform_fn', values=[state_parts, scale, seed]):\n      scales = scale if mcmc_util.is_list_like(scale) else [scale]\n      if len(scales) == 1:\n        scales *= len(state_parts)\n      if len(state_parts) != len(scales):\n        raise ValueError('`scale` must broadcast with `state_parts`.')\n      seed_stream = distributions.SeedStream(seed, salt='RandomWalkUniformFn')\n      next_state_parts = [\n          tf.random.uniform(\n              minval=state_part - scale_part,\n              maxval=state_part + scale_part,\n              shape=tf.shape(input=state_part),\n              dtype=state_part.dtype.base_dtype,\n              seed=seed_stream())\n          for scale_part, state_part in zip(scales, state_parts)\n      ]\n      return next_state_parts\n  return _fn", "gt": ""}
{"prefix": "def _kl_independent(a, b, name=\"kl_independent\"):\n  \"\"\"Batched KL divergence `KL(a || b)` for Independent distributions.\n\n  We can leverage the fact that\n  ```\n  KL(Independent(a) || Independent(b)) = sum(KL(a || b))\n  ```\n  where the sum is over the `reinterpreted_batch_ndims`.\n\n  Args:\n    a: Instance of `Independent`.\n    b: Instance of `Independent`.\n    name: (optional) name to use for created ops. Default \"kl_independent\".\n\n  Returns:\n    Batchwise `KL(a || b)`.\n\n  Raises:\n    ValueError: If the event space for `a` and `b`, or their underlying", "suffix": "  \"\"\"\n  p = a.distribution\n  q = b.distribution\n\n  # The KL between any two (non)-batched distributions is a scalar.\n  # Given that the KL between two factored distributions is the sum, i.e.\n  # KL(p1(x)p2(y) || q1(x)q2(y)) = KL(p1 || q1) + KL(q1 || q2), we compute\n  # KL(p || q) and do a `reduce_sum` on the reinterpreted batch dimensions.\n  if (tensorshape_util.is_fully_defined(a.event_shape) and\n      tensorshape_util.is_fully_defined(b.event_shape)):\n    if a.event_shape == b.event_shape:\n      if p.event_shape == q.event_shape:\n        num_reduce_dims = (tensorshape_util.rank(a.event_shape) -\n                           tensorshape_util.rank(p.event_shape))\n        reduce_dims = [-i - 1 for i in range(0, num_reduce_dims)]\n\n        return tf.reduce_sum(\n            input_tensor=kullback_leibler.kl_divergence(p, q, name=name),\n            axis=reduce_dims)\n      else:\n        raise NotImplementedError(\"KL between Independents with different \"\n                                  \"event shapes not supported.\")\n    else:\n      raise ValueError(\"Event shapes do not match.\")\n  else:\n    with tf.control_dependencies(\n        [\n            assert_util.assert_equal(a.event_shape_tensor(),\n                                     b.event_shape_tensor()),\n            assert_util.assert_equal(p.event_shape_tensor(),\n                                     q.event_shape_tensor())\n        ]):\n      num_reduce_dims = (\n          prefer_static.rank_from_shape(\n              a.event_shape_tensor, a.event_shape) -\n          prefer_static.rank_from_shape(\n              p.event_shape_tensor, a.event_shape))\n      reduce_dims = prefer_static.range(-num_reduce_dims - 1, -1, 1)\n      return tf.reduce_sum(\n          input_tensor=kullback_leibler.kl_divergence(p, q, name=name),\n          axis=reduce_dims)", "gt": "      distributions don't match."}
{"prefix": "def _get_default_reinterpreted_batch_ndims(self, distribution):\n    \"\"\"Computes the default value for reinterpreted_batch_ndim __init__ arg.\"\"\"", "suffix": "        distribution.batch_shape_tensor, distribution.batch_shape)\n    return prefer_static.maximum(0, ndims - 1)", "gt": "    ndims = prefer_static.rank_from_shape("}
{"prefix": "def _expand_to_event_rank(self, x):\n    \"\"\"Expand the rank of x up to static_event_rank times for broadcasting.\n\n    The static event rank was checked to not be None at construction time.\n\n    Args:\n      x: A tensor to expand.\n    Returns:\n      The expanded tensor.\n    \"\"\"", "suffix": "    for _ in range(tensorshape_util.rank(self.event_shape)):\n      expanded_x = tf.expand_dims(expanded_x, -1)\n    return expanded_x", "gt": "    expanded_x = x"}
{"prefix": "def entropy_lower_bound(self, name=\"entropy_lower_bound\"):\n    r\"\"\"A lower bound on the entropy of this mixture model.\n\n    The bound below is not always very tight, and its usefulness depends\n    on the mixture probabilities and the components in use.\n\n    A lower bound is useful for ELBO when the `Mixture` is the variational\n    distribution:\n\n    \\\\(\n    \\log p(x) >= ELBO = \\int q(z) \\log p(x, z) dz + H[q]\n    \\\\)\n\n    where \\\\( p \\\\) is the prior distribution, \\\\( q \\\\) is the variational,", "suffix": "    \\\\( G[q] \\\\) such that \\\\( H[q] \\geq G[q] \\\\) then it can be used in\n    place of \\\\( H[q] \\\\).\n\n    For a mixture of distributions \\\\( q(Z) = \\sum_i c_i q_i(Z) \\\\) with\n    \\\\( \\sum_i c_i = 1 \\\\), by the concavity of \\\\( f(x) = -x \\log x \\\\), a\n    simple lower bound is:\n\n    \\\\(\n    \\begin{align}\n    H[q] & = - \\int q(z) \\log q(z) dz \\\\\\\n       & = - \\int (\\sum_i c_i q_i(z)) \\log(\\sum_i c_i q_i(z)) dz \\\\\\\n       & \\geq - \\sum_i c_i \\int q_i(z) \\log q_i(z) dz \\\\\\\n       & = \\sum_i c_i H[q_i]\n    \\end{align}\n    \\\\)\n\n    This is the term we calculate below for \\\\( G[q] \\\\).\n\n    Args:\n      name: A name for this operation (optional).\n\n    Returns:\n      A lower bound on the Mixture's entropy.\n    \"\"\"\n    with self._name_scope(name):\n      with tf.control_dependencies(self._assertions):\n        distribution_entropies = [d.entropy() for d in self.components]\n        cat_probs = self._cat_probs(log_probs=False)\n        partial_entropies = [\n            c_p * m for (c_p, m) in zip(cat_probs, distribution_entropies)\n        ]\n        # These are all the same shape by virtue of matching batch_shape\n        return tf.add_n(partial_entropies)", "gt": "    and \\\\( H[q] \\\\) is the entropy of \\\\( q \\\\). If there is a lower bound"}
{"prefix": "def _cat_probs(self, log_probs):\n    \"\"\"Get a list of num_components batchwise probabilities.\"\"\"\n    which_softmax = tf.nn.log_softmax if log_probs else tf.nn.softmax\n    cat_probs = which_softmax(self.cat.logits)", "suffix": "    return cat_probs", "gt": "    cat_probs = tf.unstack(cat_probs, num=self.num_components, axis=-1)"}
{"prefix": "def _maybe_validate_args(outcomes, logits, probs, validate_args):\n  \"\"\"Validate `outcomes`, `logits` and `probs`'s shapes.\"\"\"\n  assertions = []\n\n  def validate_equal_last_dim(tensor_a, tensor_b, message):\n    if tensor_a.shape.is_fully_defined() and tensor_b.shape.is_fully_defined():\n      if tensor_a.shape[-1] != tensor_b.shape[-1]:\n        raise ValueError(message)", "suffix": "      assertions.append(\n          tf.compat.v1.assert_equal(\n              tf.shape(input=tensor_a)[-1],\n              tf.shape(input=tensor_b)[-1],\n              message=message))\n\n  if logits is not None:\n    validate_equal_last_dim(\n        outcomes,\n        logits,\n        message='Last dimension of outcomes and logits must be equal size.')\n  if probs is not None:\n    validate_equal_last_dim(\n        outcomes,\n        probs,\n        message='Last dimension of outcomes and probs must be equal size.')\n\n  message = 'Rank of outcomes must be 1.'\n  if outcomes.shape.ndims is not None:\n    if outcomes.shape.ndims != 1:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(tf.compat.v1.assert_rank(outcomes, 1, message=message))\n\n  message = 'Size of outcomes must be greater than 0.'\n  if outcomes.shape.num_elements() is not None:\n    if outcomes.shape.num_elements() == 0:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(\n        tf.compat.v1.assert_greater(\n            tf.size(input=outcomes), 0, message=message))\n\n  if validate_args:\n    assertions.append(\n        tf.compat.v1.assert_equal(\n            tf.math.is_strictly_increasing(outcomes),\n            True,\n            message='outcomes is not strictly increasing.'))\n\n  return assertions", "gt": "    elif validate_args:"}
{"prefix": "def _ensure_tf_install():  # pylint: disable=g-statement-before-imports\n  \"\"\"Attempt to import tensorflow, and ensure its version is sufficient.\n\n  Raises:\n    ImportError: if either tensorflow is not importable or its version is\n    inadequate.\n  \"\"\"", "suffix": "    import tensorflow as tf\n  except ImportError:\n    # Print more informative error message, then reraise.\n    print(\"\\n\\nFailed to import TensorFlow. Please note that TensorFlow is not \"\n          \"installed by default when you install TensorFlow Probability. This \"\n          \"is so that users can decide whether to install the GPU-enabled \"\n          \"TensorFlow package. To use TensorFlow Probability, please install \"\n          \"the most recent version of TensorFlow, by following instructions at \"\n          \"https://tensorflow.org/install.\\n\\n\")\n    raise\n\n  import distutils.version\n\n  #\n  # Update this whenever we need to depend on a newer TensorFlow release.\n  #\n  required_tensorflow_version = \"1.13\"\n\n  if (distutils.version.LooseVersion(tf.__version__) <\n      distutils.version.LooseVersion(required_tensorflow_version)):\n    raise ImportError(\n        \"This version of TensorFlow Probability requires TensorFlow \"\n        \"version >= {required}; Detected an installation of version {present}. \"\n        \"Please upgrade TensorFlow to proceed.\".format(\n            required=required_tensorflow_version,\n            present=tf.__version__))", "gt": "  try:"}
{"prefix": "def logistic_regression(features):\n  \"\"\"Bayesian logistic regression, which returns labels given features.\"\"\"", "suffix": "      loc=tf.zeros(features.shape[1]), name=\"coeffs\")\n  labels = ed.Bernoulli(\n      logits=tf.tensordot(features, coeffs, [[1], [0]]), name=\"labels\")\n  return labels", "gt": "  coeffs = ed.MultivariateNormalDiag("}
{"prefix": "def covertype():\n  \"\"\"Builds the Covertype data set.\"\"\"\n  import sklearn.datasets  # pylint: disable=g-import-not-at-top\n  data = sklearn.datasets.covtype.fetch_covtype()\n  features = data.data\n  labels = data.target\n\n  # Normalize features and append a column of ones for the intercept.\n  features -= features.mean(0)", "suffix": "  features = np.hstack([features, np.ones([features.shape[0], 1])])\n  features = tf.cast(features, dtype=tf.float32)\n\n  # Binarize outcomes on whether it is a specific category.\n  _, counts = np.unique(labels, return_counts=True)\n  specific_category = np.argmax(counts)\n  labels = (labels == specific_category)\n  labels = tf.cast(labels, dtype=tf.int32)\n  return features, labels", "gt": "  features /= features.std(0)"}
{"prefix": "def _kl_dirichlet_dirichlet(d1, d2, name=None):\n  \"\"\"Batchwise KL divergence KL(d1 || d2) with d1 and d2 Dirichlet.\n\n  Args:", "suffix": "    d2: instance of a Dirichlet distribution object.\n    name: (optional) Name to use for created operations.\n      default is \"kl_dirichlet_dirichlet\".\n\n  Returns:\n    Batchwise KL(d1 || d2)\n  \"\"\"\n  with tf.name_scope(name or \"kl_dirichlet_dirichlet\"):\n    # The KL between Dirichlet distributions can be derived as follows. We have\n    #\n    #   Dir(x; a) = 1 / B(a) * prod_i[x[i]^(a[i] - 1)]\n    #\n    # where B(a) is the multivariate Beta function:\n    #\n    #   B(a) = Gamma(a[1]) * ... * Gamma(a[n]) / Gamma(a[1] + ... + a[n])\n    #\n    # The KL is\n    #\n    #   KL(Dir(x; a), Dir(x; b)) = E_Dir(x; a){log(Dir(x; a) / Dir(x; b))}\n    #\n    # so we'll need to know the log density of the Dirichlet. This is\n    #\n    #   log(Dir(x; a)) = sum_i[(a[i] - 1) log(x[i])] - log B(a)\n    #\n    # The only term that matters for the expectations is the log(x[i]). To\n    # compute the expectation of this term over the Dirichlet density, we can\n    # use the following facts about the Dirichlet in exponential family form:\n    #   1. log(x[i]) is a sufficient statistic\n    #   2. expected sufficient statistics (of any exp family distribution) are\n    #      equal to derivatives of the log normalizer with respect to\n    #      corresponding natural parameters: E{T[i](x)} = dA/d(eta[i])\n    #\n    # To proceed, we can rewrite the Dirichlet density in exponential family\n    # form as follows:\n    #\n    #   Dir(x; a) = exp{eta(a) . T(x) - A(a)}\n    #\n    # where '.' is the dot product of vectors eta and T, and A is a scalar:\n    #\n    #   eta[i](a) = a[i] - 1\n    #     T[i](x) = log(x[i])\n    #        A(a) = log B(a)\n    #\n    # Now, we can use fact (2) above to write\n    #\n    #   E_Dir(x; a)[log(x[i])]\n    #       = dA(a) / da[i]\n    #       = d/da[i] log B(a)\n    #       = d/da[i] (sum_j lgamma(a[j])) - lgamma(sum_j a[j])\n    #       = digamma(a[i])) - digamma(sum_j a[j])\n    #\n    # Putting it all together, we have\n    #\n    # KL[Dir(x; a) || Dir(x; b)]\n    #     = E_Dir(x; a){log(Dir(x; a) / Dir(x; b)}\n    #     = E_Dir(x; a){sum_i[(a[i] - b[i]) log(x[i])} - (lbeta(a) - lbeta(b))\n    #     = sum_i[(a[i] - b[i]) * E_Dir(x; a){log(x[i])}] - lbeta(a) + lbeta(b)\n    #     = sum_i[(a[i] - b[i]) * (digamma(a[i]) - digamma(sum_j a[j]))]\n    #          - lbeta(a) + lbeta(b))\n\n    digamma_sum_d1 = tf.math.digamma(\n        tf.reduce_sum(input_tensor=d1.concentration, axis=-1, keepdims=True))\n    digamma_diff = tf.math.digamma(d1.concentration) - digamma_sum_d1\n    concentration_diff = d1.concentration - d2.concentration\n\n    return (\n        tf.reduce_sum(input_tensor=concentration_diff * digamma_diff, axis=-1) -\n        tf.math.lbeta(d1.concentration) + tf.math.lbeta(d2.concentration))", "gt": "    d1: instance of a Dirichlet distribution object."}
{"prefix": "def _maybe_assert_valid_concentration(self, concentration, validate_args):\n    \"\"\"Checks the validity of the concentration parameter.\"\"\"\n    if not validate_args:\n      return concentration\n    return distribution_util.with_dependencies([\n        assert_util.assert_positive(\n            concentration, message=\"Concentration parameter must be positive.\"),\n        assert_util.assert_rank_at_least(\n            concentration,", "suffix": "            message=\"Concentration parameter must have >=1 dimensions.\"),\n        assert_util.assert_less(\n            1,\n            tf.shape(input=concentration)[-1],\n            message=\"Concentration parameter must have event_size >= 2.\"),\n    ], concentration)", "gt": "            1,"}
{"prefix": "def _maybe_assert_valid_sample(self, x):\n    \"\"\"Checks the validity of a sample.\"\"\"\n    if not self.validate_args:\n      return x", "suffix": "        assert_util.assert_positive(x, message=\"samples must be positive\"),\n        assert_util.assert_near(\n            tf.ones([], dtype=self.dtype),\n            tf.reduce_sum(input_tensor=x, axis=-1),\n            message=\"sample last-dimension must sum to `1`\"),\n    ], x)", "gt": "    return distribution_util.with_dependencies(["}
{"prefix": "def auto_correlation(x,\n                     axis=-1,\n                     max_lags=None,\n                     center=True,\n                     normalize=True,\n                     name='auto_correlation'):\n  \"\"\"Auto correlation along one axis.\n\n  Given a `1-D` wide sense stationary (WSS) sequence `X`, the auto correlation\n  `RXX` may be defined as  (with `E` expectation and `Conj` complex conjugate)\n\n  ```\n  RXX[m] := E{ W[m] Conj(W[0]) } = E{ W[0] Conj(W[-m]) },\n  W[n]   := (X[n] - MU) / S,\n  MU     := E{ X[0] },\n  S**2   := E{ (X[0] - MU) Conj(X[0] - MU) }.\n  ```\n\n  This function takes the viewpoint that `x` is (along one axis) a finite\n  sub-sequence of a realization of (WSS) `X`, and then uses `x` to produce an", "suffix": "\n  After extending `x` from length `L` to `inf` by zero padding, the auto\n  correlation estimate `rxx[m]` is computed for `m = 0, 1, ..., max_lags` as\n\n  ```\n  rxx[m] := (L - m)**-1 sum_n w[n + m] Conj(w[n]),\n  w[n]   := (x[n] - mu) / s,\n  mu     := L**-1 sum_n x[n],\n  s**2   := L**-1 sum_n (x[n] - mu) Conj(x[n] - mu)\n  ```\n\n  The error in this estimate is proportional to `1 / sqrt(len(x) - m)`, so users\n  often set `max_lags` small enough so that the entire output is meaningful.\n\n  Note that since `mu` is an imperfect estimate of `E{ X[0] }`, and we divide by\n  `len(x) - m` rather than `len(x) - m - 1`, our estimate of auto correlation\n  contains a slight bias, which goes to zero as `len(x) - m --> infinity`.\n\n  Args:\n    x:  `float32` or `complex64` `Tensor`.\n    axis:  Python `int`. The axis number along which to compute correlation.\n      Other dimensions index different batch members.\n    max_lags:  Positive `int` tensor.  The maximum value of `m` to consider (in\n      equation above).  If `max_lags >= x.shape[axis]`, we effectively re-set\n      `max_lags` to `x.shape[axis] - 1`.\n    center:  Python `bool`.  If `False`, do not subtract the mean estimate `mu`\n      from `x[n]` when forming `w[n]`.\n    normalize:  Python `bool`.  If `False`, do not divide by the variance\n      estimate `s**2` when forming `w[n]`.\n    name:  `String` name to prepend to created ops.\n\n  Returns:\n    `rxx`: `Tensor` of same `dtype` as `x`.  `rxx.shape[i] = x.shape[i]` for\n      `i != axis`, and `rxx.shape[axis] = max_lags + 1`.\n\n  Raises:\n    TypeError:  If `x` is not a supported type.\n  \"\"\"\n  # Implementation details:\n  # Extend length N / 2 1-D array x to length N by zero padding onto the end.\n  # Then, set\n  #   F[x]_k := sum_n x_n exp{-i 2 pi k n / N }.\n  # It is not hard to see that\n  #   F[x]_k Conj(F[x]_k) = F[R]_k, where\n  #   R_m := sum_n x_n Conj(x_{(n - m) mod N}).\n  # One can also check that R_m / (N / 2 - m) is an unbiased estimate of RXX[m].\n\n  # Since F[x] is the DFT of x, this leads us to a zero-padding and FFT/IFFT\n  # based version of estimating RXX.\n  # Note that this is a special case of the Wiener-Khinchin Theorem.\n  with tf.compat.v1.name_scope(name, values=[x]):\n    x = tf.convert_to_tensor(value=x, name='x')\n\n    # Rotate dimensions of x in order to put axis at the rightmost dim.\n    # FFT op requires this.\n    rank = util.prefer_static_rank(x)\n    if axis < 0:\n      axis = rank + axis\n    shift = rank - 1 - axis\n    # Suppose x.shape[axis] = T, so there are T 'time' steps.\n    #   ==> x_rotated.shape = B + [T],\n    # where B is x_rotated's batch shape.\n    x_rotated = util.rotate_transpose(x, shift)\n\n    if center:\n      x_rotated -= tf.reduce_mean(\n          input_tensor=x_rotated, axis=-1, keepdims=True)\n\n    # x_len = N / 2 from above explanation.  The length of x along axis.\n    # Get a value for x_len that works in all cases.\n    x_len = util.prefer_static_shape(x_rotated)[-1]\n\n    # TODO(langmore) Investigate whether this zero padding helps or hurts.  At\n    # the moment is necessary so that all FFT implementations work.\n    # Zero pad to the next power of 2 greater than 2 * x_len, which equals\n    # 2**(ceil(Log_2(2 * x_len))).  Note: Log_2(X) = Log_e(X) / Log_e(2).\n    x_len_float64 = tf.cast(x_len, np.float64)\n    target_length = tf.pow(\n        np.float64(2.), tf.math.ceil(\n            tf.math.log(x_len_float64 * 2) / np.log(2.)))\n    pad_length = tf.cast(target_length - x_len_float64, np.int32)\n\n    # We should have:\n    # x_rotated_pad.shape = x_rotated.shape[:-1] + [T + pad_length]\n    #                     = B + [T + pad_length]\n    x_rotated_pad = util.pad(x_rotated, axis=-1, back=True, count=pad_length)\n\n    dtype = x.dtype\n    if not dtype.is_complex:\n      if not dtype.is_floating:\n        raise TypeError('Argument x must have either float or complex dtype'\n                        ' found: {}'.format(dtype))\n      x_rotated_pad = tf.complex(x_rotated_pad,\n                                 dtype.real_dtype.as_numpy_dtype(0.))\n\n    # Autocorrelation is IFFT of power-spectral density (up to some scaling).\n    fft_x_rotated_pad = tf.signal.fft(x_rotated_pad)\n    spectral_density = fft_x_rotated_pad * tf.math.conj(fft_x_rotated_pad)\n    # shifted_product is R[m] from above detailed explanation.\n    # It is the inner product sum_n X[n] * Conj(X[n - m]).\n    shifted_product = tf.signal.ifft(spectral_density)\n\n    # Cast back to real-valued if x was real to begin with.\n    shifted_product = tf.cast(shifted_product, dtype)\n\n    # Figure out if we can deduce the final static shape, and set max_lags.\n    # Use x_rotated as a reference, because it has the time dimension in the far\n    # right, and was created before we performed all sorts of crazy shape\n    # manipulations.\n    know_static_shape = True\n    if not x_rotated.shape.is_fully_defined():\n      know_static_shape = False\n    if max_lags is None:\n      max_lags = x_len - 1\n    else:\n      max_lags = tf.convert_to_tensor(value=max_lags, name='max_lags')\n      max_lags_ = tf.get_static_value(max_lags)\n      if max_lags_ is None or not know_static_shape:\n        know_static_shape = False\n        max_lags = tf.minimum(x_len - 1, max_lags)\n      else:\n        max_lags = min(x_len - 1, max_lags_)\n\n    # Chop off the padding.\n    # We allow users to provide a huge max_lags, but cut it off here.\n    # shifted_product_chopped.shape = x_rotated.shape[:-1] + [max_lags]\n    shifted_product_chopped = shifted_product[..., :max_lags + 1]\n\n    # If possible, set shape.\n    if know_static_shape:\n      chopped_shape = x_rotated.shape.as_list()\n      chopped_shape[-1] = min(x_len, max_lags + 1)\n      shifted_product_chopped.set_shape(chopped_shape)\n\n    # Recall R[m] is a sum of N / 2 - m nonzero terms x[n] Conj(x[n - m]).  The\n    # other terms were zeros arising only due to zero padding.\n    # `denominator = (N / 2 - m)` (defined below) is the proper term to\n    # divide by to make this an unbiased estimate of the expectation\n    # E[X[n] Conj(X[n - m])].\n    x_len = tf.cast(x_len, dtype.real_dtype)\n    max_lags = tf.cast(max_lags, dtype.real_dtype)\n    denominator = x_len - tf.range(0., max_lags + 1.)\n    denominator = tf.cast(denominator, dtype)\n    shifted_product_rotated = shifted_product_chopped / denominator\n\n    if normalize:\n      shifted_product_rotated /= shifted_product_rotated[..., :1]\n\n    # Transpose dimensions back to those of x.\n    return util.rotate_transpose(shifted_product_rotated, -shift)", "gt": "  estimate of `RXX[m]` as follows:"}
{"prefix": "def cholesky_covariance(x, sample_axis=0, keepdims=False, name=None):\n  \"\"\"Cholesky factor of the covariance matrix of vector-variate random samples.\n\n  This function can be use to fit a multivariate normal to data.\n\n  ```python\n  tf.enable_eager_execution()\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Assume data.shape = (1000, 2).  1000 samples of a random variable in R^2.\n  observed_data = read_data_samples(...)\n\n  # The mean is easy\n  mu = tf.reduce_mean(observed_data, axis=0)\n\n  # Get the scale matrix\n  L = tfp.stats.cholesky_covariance(observed_data)\n\n  # Make the best fit multivariate normal (under maximum likelihood condition).\n  mvn = tfd.MultivariateNormalTriL(loc=mu, scale_tril=L)\n\n  # Plot contours of the pdf.\n  xs, ys = tf.meshgrid(\n      tf.linspace(-5., 5., 50), tf.linspace(-5., 5., 50), indexing='ij')\n  xy = tf.stack((tf.reshape(xs, [-1]), tf.reshape(ys, [-1])), axis=-1)\n  pdf = tf.reshape(mvn.prob(xy), (50, 50))\n  CS = plt.contour(xs, ys, pdf, 10)\n  plt.clabel(CS, inline=1, fontsize=10)\n  ```\n\n  Why does this work?\n  Given vector-variate random variables `X = (X1, ..., Xd)`, one may obtain the\n  sample covariance matrix in `R^{d x d}` (see `tfp.stats.covariance`).\n\n  The [Cholesky factor](https://en.wikipedia.org/wiki/Cholesky_decomposition)\n  of this matrix is analogous to standard deviation for scalar random variables:\n  Suppose `X` has covariance matrix `C`, with Cholesky factorization `C = L L^T`\n  Then multiplying a vector of iid random variables which have unit variance by\n  `L` produces a vector with covariance `L L^T`, which is the same as `X`.\n\n  ```python\n  observed_data = read_data_samples(...)\n  L = tfp.stats.cholesky_covariance(observed_data, sample_axis=0)\n\n  # Make fake_data with the same covariance as observed_data.\n  uncorrelated_normal = tf.random_normal(shape=(500, 10))\n  fake_data = tf.linalg.matvec(L, uncorrelated_normal)\n  ```\n", "suffix": "    x:  Numeric `Tensor`.  The rightmost dimension of `x` indexes events. E.g.\n      dimensions of a random vector.\n    sample_axis: Scalar or vector `Tensor` designating axis holding samples.\n      Default value: `0` (leftmost dimension). Cannot be the rightmost dimension\n        (since this indexes events).\n    keepdims:  Boolean.  Whether to keep the sample axis as singletons.\n    name: Python `str` name prefixed to Ops created by this function.\n          Default value: `None` (i.e., `'covariance'`).\n\n  Returns:\n    chol:  `Tensor` of same `dtype` as `x`.  The last two dimensions hold\n      lower triangular matrices (the Cholesky factors).\n  \"\"\"\n  with tf.compat.v1.name_scope(\n      name, 'cholesky_covariance', values=[x, sample_axis]):\n    sample_axis = tf.convert_to_tensor(value=sample_axis, dtype=tf.int32)\n    cov = covariance(\n        x, sample_axis=sample_axis, event_axis=-1, keepdims=keepdims)\n    return tf.linalg.cholesky(cov)", "gt": "  Args:"}
{"prefix": "def covariance(x,\n               y=None,\n               sample_axis=0,\n               event_axis=-1,\n               keepdims=False,\n               name=None):\n  \"\"\"Sample covariance between observations indexed by `event_axis`.\n\n  Given `N` samples of scalar random variables `X` and `Y`, covariance may be\n  estimated as\n\n  ```none\n  Cov[X, Y] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(Y_n - Ybar)}\n  Xbar := N^{-1} sum_{n=1}^N X_n\n  Ybar := N^{-1} sum_{n=1}^N Y_n\n  ```\n\n  For vector-variate random variables `X = (X1, ..., Xd)`, `Y = (Y1, ..., Yd)`,\n  one is often interested in the covariance matrix, `C_{ij} := Cov[Xi, Yj]`.\n\n  ```python\n  x = tf.random_normal(shape=(100, 2, 3))\n  y = tf.random_normal(shape=(100, 2, 3))\n\n  # cov[i, j] is the sample covariance between x[:, i, j] and y[:, i, j].\n  cov = tfp.stats.covariance(x, y, sample_axis=0, event_axis=None)\n\n  # cov_matrix[i, m, n] is the sample covariance of x[:, i, m] and y[:, i, n]\n  cov_matrix = tfp.stats.covariance(x, y, sample_axis=0, event_axis=-1)\n  ```\n\n  Notice we divide by `N` (the numpy default), which does not create `NaN`\n  when `N = 1`, but is slightly biased.\n\n  Args:\n    x:  A numeric `Tensor` holding samples.\n    y:  Optional `Tensor` with same `dtype` and `shape` as `x`.\n      Default value: `None` (`y` is effectively set to `x`).\n    sample_axis: Scalar or vector `Tensor` designating axis holding samples, or\n      `None` (meaning all axis hold samples).\n      Default value: `0` (leftmost dimension).\n    event_axis:  Scalar or vector `Tensor`, or `None` (scalar events).\n      Axis indexing random events, whose covariance we are interested in.\n      If a vector, entries must form a contiguous block of dims. `sample_axis`\n      and `event_axis` should not intersect.\n      Default value: `-1` (rightmost axis holds events).\n    keepdims:  Boolean.  Whether to keep the sample axis as singletons.\n    name: Python `str` name prefixed to Ops created by this function.\n          Default value: `None` (i.e., `'covariance'`).\n\n  Returns:\n    cov: A `Tensor` of same `dtype` as the `x`, and rank equal to\n      `rank(x) - len(sample_axis) + 2 * len(event_axis)`.\n\n  Raises:\n    AssertionError:  If `x` and `y` are found to have different shape.\n    ValueError:  If `sample_axis` and `event_axis` are found to overlap.\n    ValueError:  If `event_axis` is found to not be contiguous.\n  \"\"\"\n\n  with tf.compat.v1.name_scope(\n      name, 'covariance', values=[x, y, event_axis, sample_axis]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    # Covariance *only* uses the centered versions of x (and y).\n    x -= tf.reduce_mean(input_tensor=x, axis=sample_axis, keepdims=True)\n\n    if y is None:\n      y = x\n    else:\n      y = tf.convert_to_tensor(value=y, name='y', dtype=x.dtype)\n      # If x and y have different shape, sample_axis and event_axis will likely\n      # be wrong for one of them!\n      x.shape.assert_is_compatible_with(y.shape)\n      y -= tf.reduce_mean(input_tensor=y, axis=sample_axis, keepdims=True)\n\n    if event_axis is None:\n      return tf.reduce_mean(\n          input_tensor=x * tf.math.conj(y), axis=sample_axis, keepdims=keepdims)\n\n    if sample_axis is None:\n      raise ValueError(\n          'sample_axis was None, which means all axis hold events, and this '\n          'overlaps with event_axis ({})'.format(event_axis))", "suffix": "    event_axis = _make_positive_axis(event_axis, tf.rank(x))\n    sample_axis = _make_positive_axis(sample_axis, tf.rank(x))\n\n    # If we get lucky and axis is statically defined, we can do some checks.\n    if _is_list_like(event_axis) and _is_list_like(sample_axis):\n      if set(event_axis).intersection(sample_axis):\n        raise ValueError(\n            'sample_axis ({}) and event_axis ({}) overlapped'.format(\n                sample_axis, event_axis))\n      if (np.diff(sorted(event_axis)) > 1).any():\n        raise ValueError(\n            'event_axis must be contiguous. Found: {}'.format(event_axis))\n      batch_axis = list(\n          sorted(\n              set(range(x.shape.ndims)).difference(sample_axis + event_axis)))\n    else:\n      batch_axis, _ = tf.compat.v1.setdiff1d(\n          tf.range(0, tf.rank(x)), tf.concat((sample_axis, event_axis), 0))\n\n    event_axis = tf.convert_to_tensor(\n        value=event_axis, name='event_axis', dtype=tf.int32)\n    sample_axis = tf.convert_to_tensor(\n        value=sample_axis, name='sample_axis', dtype=tf.int32)\n    batch_axis = tf.convert_to_tensor(\n        value=batch_axis, name='batch_axis', dtype=tf.int32)\n\n    # Permute x/y until shape = B + E + S\n    perm_for_xy = tf.concat((batch_axis, event_axis, sample_axis), 0)\n    x_permed = tf.transpose(a=x, perm=perm_for_xy)\n    y_permed = tf.transpose(a=y, perm=perm_for_xy)\n\n    batch_ndims = tf.size(input=batch_axis)\n    batch_shape = tf.shape(input=x_permed)[:batch_ndims]\n    event_ndims = tf.size(input=event_axis)\n    event_shape = tf.shape(input=x_permed)[batch_ndims:batch_ndims +\n                                           event_ndims]\n    sample_shape = tf.shape(input=x_permed)[batch_ndims + event_ndims:]\n    sample_ndims = tf.size(input=sample_shape)\n    n_samples = tf.reduce_prod(input_tensor=sample_shape)\n    n_events = tf.reduce_prod(input_tensor=event_shape)\n\n    # Flatten sample_axis into one long dim.\n    x_permed_flat = tf.reshape(\n        x_permed, tf.concat((batch_shape, event_shape, [n_samples]), 0))\n    y_permed_flat = tf.reshape(\n        y_permed, tf.concat((batch_shape, event_shape, [n_samples]), 0))\n    # Do the same for event_axis.\n    x_permed_flat = tf.reshape(\n        x_permed, tf.concat((batch_shape, [n_events], [n_samples]), 0))\n    y_permed_flat = tf.reshape(\n        y_permed, tf.concat((batch_shape, [n_events], [n_samples]), 0))\n\n    # After matmul, cov.shape = batch_shape + [n_events, n_events]\n    cov = tf.matmul(\n        x_permed_flat, y_permed_flat, adjoint_b=True) / tf.cast(\n            n_samples, x.dtype)\n\n    # Insert some singletons to make\n    # cov.shape = batch_shape + event_shape**2 + [1,...,1]\n    # This is just like x_permed.shape, except the sample_axis is all 1's, and\n    # the [n_events] became event_shape**2.\n    cov = tf.reshape(\n        cov,\n        tf.concat(\n            (\n                batch_shape,\n                # event_shape**2 used here because it is the same length as\n                # event_shape, and has the same number of elements as one\n                # batch of covariance.\n                event_shape**2,\n                tf.ones([sample_ndims], tf.int32)),\n            0))\n    # Permuting by the argsort inverts the permutation, making\n    # cov.shape have ones in the position where there were samples, and\n    # [n_events * n_events] in the event position.\n    cov = tf.transpose(a=cov, perm=tf.math.invert_permutation(perm_for_xy))\n\n    # Now expand event_shape**2 into event_shape + event_shape.\n    # We here use (for the first time) the fact that we require event_axis to be\n    # contiguous.\n    e_start = event_axis[0]\n    e_len = 1 + event_axis[-1] - event_axis[0]\n    cov = tf.reshape(\n        cov,\n        tf.concat((tf.shape(input=cov)[:e_start], event_shape, event_shape,\n                   tf.shape(input=cov)[e_start + e_len:]), 0))\n\n    # tf.squeeze requires python ints for axis, not Tensor.  This is enough to\n    # require our axis args to be constants.\n    if not keepdims:\n      squeeze_axis = tf.where(sample_axis < e_start, sample_axis,\n                              sample_axis + e_len)\n      cov = _squeeze(cov, axis=squeeze_axis)\n\n    return cov", "gt": ""}
{"prefix": "def correlation(x,\n                y=None,\n                sample_axis=0,\n                event_axis=-1,\n                keepdims=False,\n                name=None):\n  \"\"\"Sample correlation (Pearson) between observations indexed by `event_axis`.\n\n  Given `N` samples of scalar random variables `X` and `Y`, correlation may be\n  estimated as\n\n  ```none\n  Corr[X, Y] := Cov[X, Y] / Sqrt(Cov[X, X] * Cov[Y, Y]),\n  where\n  Cov[X, Y] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(Y_n - Ybar)}\n  Xbar := N^{-1} sum_{n=1}^N X_n\n  Ybar := N^{-1} sum_{n=1}^N Y_n\n  ```\n", "suffix": "\n  For vector-variate random variables `X = (X1, ..., Xd)`, `Y = (Y1, ..., Yd)`,\n  one is often interested in the correlation matrix, `C_{ij} := Corr[Xi, Yj]`.\n\n  ```python\n  x = tf.random_normal(shape=(100, 2, 3))\n  y = tf.random_normal(shape=(100, 2, 3))\n\n  # corr[i, j] is the sample correlation between x[:, i, j] and y[:, i, j].\n  corr = tfp.stats.correlation(x, y, sample_axis=0, event_axis=None)\n\n  # corr_matrix[i, m, n] is the sample correlation of x[:, i, m] and y[:, i, n]\n  corr_matrix = tfp.stats.correlation(x, y, sample_axis=0, event_axis=-1)\n  ```\n\n  Notice we divide by `N` (the numpy default), which does not create `NaN`\n  when `N = 1`, but is slightly biased.\n\n  Args:\n    x:  A numeric `Tensor` holding samples.\n    y:  Optional `Tensor` with same `dtype` and `shape` as `x`.\n      Default value: `None` (`y` is effectively set to `x`).\n    sample_axis: Scalar or vector `Tensor` designating axis holding samples, or\n      `None` (meaning all axis hold samples).\n      Default value: `0` (leftmost dimension).\n    event_axis:  Scalar or vector `Tensor`, or `None` (scalar events).\n      Axis indexing random events, whose correlation we are interested in.\n      If a vector, entries must form a contiguous block of dims. `sample_axis`\n      and `event_axis` should not intersect.\n      Default value: `-1` (rightmost axis holds events).\n    keepdims:  Boolean.  Whether to keep the sample axis as singletons.\n    name: Python `str` name prefixed to Ops created by this function.\n          Default value: `None` (i.e., `'correlation'`).\n\n  Returns:\n    corr: A `Tensor` of same `dtype` as the `x`, and rank equal to\n      `rank(x) - len(sample_axis) + 2 * len(event_axis)`.\n\n  Raises:\n    AssertionError:  If `x` and `y` are found to have different shape.\n    ValueError:  If `sample_axis` and `event_axis` are found to overlap.\n    ValueError:  If `event_axis` is found to not be contiguous.\n  \"\"\"\n\n  with tf.compat.v1.name_scope(\n      name, 'correlation', values=[x, y, event_axis, sample_axis]):\n    # Corr[X, Y] = Cov[X, Y] / (Stddev[X] * Stddev[Y])\n    #            = Cov[X / Stddev[X], Y / Stddev[Y]]\n    # So we could compute covariance first then divide by stddev, or\n    # divide by stddev and compute covariance.\n    # Dividing by stddev then computing covariance is potentially more stable.\n    # But... computing covariance first then dividing involves 2 fewer large\n    # broadcasts.  We choose to divide first, largely because it avoids\n    # difficulties with the various options for sample/event axis kwargs.\n\n    x /= stddev(x, sample_axis=sample_axis, keepdims=True)\n    if y is not None:\n      y /= stddev(y, sample_axis=sample_axis, keepdims=True)\n\n    return covariance(\n        x=x,\n        y=y,\n        event_axis=event_axis,\n        sample_axis=sample_axis,\n        keepdims=keepdims)", "gt": "  Correlation is always in the interval `[-1, 1]`, and `Corr[X, X] == 1`."}
{"prefix": "def stddev(x, sample_axis=0, keepdims=False, name=None):\n  \"\"\"Estimate standard deviation using samples.\n\n  Given `N` samples of scalar valued random variable `X`, standard deviation may\n  be estimated as\n\n  ```none\n  Stddev[X] := Sqrt[Var[X]],\n  Var[X] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(X_n - Xbar)},\n  Xbar := N^{-1} sum_{n=1}^N X_n\n  ```\n\n  ```python\n  x = tf.random_normal(shape=(100, 2, 3))\n", "suffix": "  stddev = tfp.stats.stddev(x, sample_axis=0)\n  ```\n\n  Scaling a unit normal by a standard deviation produces normal samples\n  with that standard deviation.\n\n  ```python\n  observed_data = read_data_samples(...)\n  stddev = tfp.stats.stddev(observed_data)\n\n  # Make fake_data with the same standard deviation as observed_data.\n  fake_data = stddev * tf.random_normal(shape=(100,))\n  ```\n\n  Notice we divide by `N` (the numpy default), which does not create `NaN`\n  when `N = 1`, but is slightly biased.\n\n  Args:\n    x:  A numeric `Tensor` holding samples.\n    sample_axis: Scalar or vector `Tensor` designating axis holding samples, or\n      `None` (meaning all axis hold samples).\n      Default value: `0` (leftmost dimension).\n    keepdims:  Boolean.  Whether to keep the sample axis as singletons.\n    name: Python `str` name prefixed to Ops created by this function.\n          Default value: `None` (i.e., `'stddev'`).\n\n  Returns:\n    stddev: A `Tensor` of same `dtype` as the `x`, and rank equal to\n      `rank(x) - len(sample_axis)`\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'stddev', values=[x, sample_axis]):\n    return tf.sqrt(variance(x, sample_axis=sample_axis, keepdims=keepdims))", "gt": "  # stddev[i, j] is the sample standard deviation of the (i, j) batch member."}
{"prefix": "def variance(x, sample_axis=0, keepdims=False, name=None):\n  \"\"\"Estimate variance using samples.\n\n  Given `N` samples of scalar valued random variable `X`, variance may\n  be estimated as\n\n  ```none\n  Var[X] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(X_n - Xbar)}\n  Xbar := N^{-1} sum_{n=1}^N X_n\n  ```\n\n  ```python\n  x = tf.random_normal(shape=(100, 2, 3))\n\n  # var[i, j] is the sample variance of the (i, j) batch member of x.\n  var = tfp.stats.variance(x, sample_axis=0)\n  ```\n\n  Notice we divide by `N` (the numpy default), which does not create `NaN`\n  when `N = 1`, but is slightly biased.\n\n  Args:", "suffix": "    sample_axis: Scalar or vector `Tensor` designating axis holding samples, or\n      `None` (meaning all axis hold samples).\n      Default value: `0` (leftmost dimension).\n    keepdims:  Boolean.  Whether to keep the sample axis as singletons.\n    name: Python `str` name prefixed to Ops created by this function.\n          Default value: `None` (i.e., `'variance'`).\n\n  Returns:\n    var: A `Tensor` of same `dtype` as the `x`, and rank equal to\n      `rank(x) - len(sample_axis)`\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'variance', values=[x, sample_axis]):\n    return covariance(\n        x, y=None, sample_axis=sample_axis, event_axis=None, keepdims=keepdims)", "gt": "    x:  A numeric `Tensor` holding samples."}
{"prefix": "def _make_list_or_1d_tensor(values):", "suffix": "  values = tf.convert_to_tensor(value=values, name='values')\n  values_ = tf.get_static_value(values)\n\n  # Static didn't work.\n  if values_ is None:\n    # Cheap way to bring to at least 1d.\n    return values + tf.zeros([1], dtype=values.dtype)\n\n  # Static worked!\n  if values_.ndim > 1:\n    raise ValueError('values had > 1 dim: {}'.format(values_.shape))\n  # Cheap way to bring to at least 1d.\n  values_ = values_ + np.zeros([1], dtype=values_.dtype)\n  return list(values_)", "gt": "  \"\"\"Return a list (preferred) or 1d Tensor from values, if values.ndims < 2.\"\"\""}
{"prefix": "", "suffix": "  \"\"\"Rectify possibly negatively axis. Prefer return Python list.\"\"\"\n  axis = _make_list_or_1d_tensor(axis)\n\n  ndims = tf.convert_to_tensor(value=ndims, name='ndims', dtype=tf.int32)\n  ndims_ = tf.get_static_value(ndims)\n\n  if _is_list_like(axis) and ndims_ is not None:\n    # Static case\n    positive_axis = []\n    for a in axis:\n      if a < 0:\n        a = ndims_ + a\n      positive_axis.append(a)\n  else:\n    # Dynamic case\n    axis = tf.convert_to_tensor(value=axis, name='axis', dtype=tf.int32)\n    positive_axis = tf.where(axis >= 0, axis, axis + ndims)\n\n  return positive_axis", "gt": "def _make_positive_axis(axis, ndims):"}
{"prefix": "def _squeeze(x, axis):\n  \"\"\"A version of squeeze that works with dynamic axis.\"\"\"\n  x = tf.convert_to_tensor(value=x, name='x')\n  if axis is None:", "suffix": "  axis = tf.convert_to_tensor(value=axis, name='axis', dtype=tf.int32)\n  axis += tf.zeros([1], dtype=axis.dtype)  # Make axis at least 1d.\n  keep_axis, _ = tf.compat.v1.setdiff1d(tf.range(0, tf.rank(x)), axis)\n  return tf.reshape(x, tf.gather(tf.shape(input=x), keep_axis))", "gt": "    return tf.squeeze(x, axis=None)"}
{"prefix": "def _kl_normal_normal(n_a, n_b, name=None):\n  \"\"\"Calculate the batched KL divergence KL(n_a || n_b) with n_a and n_b Normal.\n\n  Args:\n    n_a: instance of a Normal distribution object.\n    n_b: instance of a Normal distribution object.\n    name: (optional) Name to use for created operations.\n      default is \"kl_normal_normal\".", "suffix": "  Returns:\n    Batchwise KL(n_a || n_b)\n  \"\"\"\n  with tf.name_scope(name or \"kl_normal_normal\"):\n    one = tf.constant(1, dtype=n_a.dtype)\n    two = tf.constant(2, dtype=n_a.dtype)\n    half = tf.constant(0.5, dtype=n_a.dtype)\n    s_a_squared = tf.square(n_a.scale)\n    s_b_squared = tf.square(n_b.scale)\n    ratio = s_a_squared / s_b_squared\n    return (tf.square(n_a.loc - n_b.loc) / (two * s_b_squared) + half *\n            (ratio - one - tf.math.log(ratio)))", "gt": ""}
{"prefix": "def _z(self, x):\n    \"\"\"Standardize input `x` to a unit normal.\"\"\"\n    with tf.name_scope(\"standardize\"):", "suffix": "", "gt": "      return (x - self.loc) / self.scale"}
{"prefix": "def _inv_z(self, z):\n    \"\"\"Reconstruct input `x` from a its normalized version.\"\"\"", "suffix": "      return z * self.scale + self.loc", "gt": "    with tf.name_scope(\"reconstruct\"):"}
{"prefix": "def semilocal_linear_trend_transition_matrix(autoregressive_coef):\n  \"\"\"Build the transition matrix for a semi-local linear trend model.\"\"\"\n  # We want to write the following 2 x 2 matrix:\n  #  [[1., 1., ],    # level(t+1) = level(t) + slope(t)\n  #   [0., ar_coef], # slope(t+1) = ar_coef * slope(t)\n  # but it's slightly tricky to properly incorporate the batch shape of\n  # autoregressive_coef. E.g., if autoregressive_coef has shape [4,6], we want\n  # to return shape [4, 6, 2, 2]. We do this by breaking the matrix into its\n  # fixed entries, written explicitly, and then the autoregressive_coef part\n  # which we add in after using a mask to broadcast to the correct matrix shape.\n\n  fixed_entries = tf.constant(\n      [[1., 1.],", "suffix": "      dtype=autoregressive_coef.dtype)\n\n  autoregressive_coef_mask = tf.constant([[0., 0.],\n                                          [0., 1.]],\n                                         dtype=autoregressive_coef.dtype)\n  bottom_right_entry = (autoregressive_coef[..., tf.newaxis, tf.newaxis] *\n                        autoregressive_coef_mask)\n  return tf.linalg.LinearOperatorFullMatrix(\n      fixed_entries + bottom_right_entry)", "gt": "       [0., 0.]],"}
{"prefix": "def semilocal_linear_trend_transition_noise(level_scale,\n                                            slope_mean,\n                                            slope_scale,\n                                            autoregressive_coef):", "suffix": "\n  # At each timestep, the stochasticity of `level` and `slope` are given\n  # by `level_scale` and `slope_scale` respectively.\n  broadcast_batch_shape = dist_util.get_broadcast_shape(\n      level_scale, slope_mean, slope_scale, autoregressive_coef)\n  broadcast_ones = tf.ones(broadcast_batch_shape, dtype=level_scale.dtype)\n  scale_diag = tf.stack([level_scale * broadcast_ones,\n                         slope_scale * broadcast_ones],\n                        axis=-1)\n\n  # We additionally fold in a bias term implementing the nonzero `slope_mean`.\n  # The overall `slope` update is (from `SemiLocalLinearTrend` docstring)\n  #   slope[t] = (slope_mean +\n  #               autoregressive_coef * (slope[t-1] - slope_mean) +\n  #               Normal(0., slope_scale))\n  # which we rewrite as\n  #   slope[t] = (\n  #    autoregressive_coef * slope[t-1] +                  # linear transition\n  #    Normal(loc=slope_mean - autoregressive_coef * slope_mean,  # noise bias\n  #           scale=slope_scale))                                 # noise scale\n  bias = tf.stack([tf.zeros_like(broadcast_ones),\n                   slope_mean * (1 - autoregressive_coef) * broadcast_ones],\n                  axis=-1)\n  return tfd.MultivariateNormalDiag(\n      loc=bias,\n      scale_diag=scale_diag)", "gt": "  \"\"\"Build the transition noise model for a semi-local linear trend model.\"\"\""}
{"prefix": "def sample_halton_sequence(dim,\n                           num_results=None,\n                           sequence_indices=None,\n                           dtype=tf.float32,\n                           randomized=True,\n                           seed=None,\n                           name=None):\n  r\"\"\"Returns a sample from the `dim` dimensional Halton sequence.\n\n  Warning: The sequence elements take values only between 0 and 1. Care must be\n  taken to appropriately transform the domain of a function if it differs from\n  the unit cube before evaluating integrals using Halton samples. It is also\n  important to remember that quasi-random numbers without randomization are not\n  a replacement for pseudo-random numbers in every context. Quasi random numbers\n  are completely deterministic and typically have significant negative\n  autocorrelation unless randomization is used.\n\n  Computes the members of the low discrepancy Halton sequence in dimension\n  `dim`. The `dim`-dimensional sequence takes values in the unit hypercube in\n  `dim` dimensions. Currently, only dimensions up to 1000 are supported. The\n  prime base for the k-th axes is the k-th prime starting from 2. For example,\n  if `dim` = 3, then the bases will be [2, 3, 5] respectively and the first\n  element of the non-randomized sequence will be: [0.5, 0.333, 0.2]. For a more\n  complete description of the Halton sequences see\n  [here](https://en.wikipedia.org/wiki/Halton_sequence). For low discrepancy\n  sequences and their applications see\n  [here](https://en.wikipedia.org/wiki/Low-discrepancy_sequence).\n\n  If `randomized` is true, this function produces a scrambled version of the\n  Halton sequence introduced by [Owen (2017)][1]. For the advantages of\n  randomization of low discrepancy sequences see [here](\n  https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method#Randomization_of_quasi-Monte_Carlo).\n\n  The number of samples produced is controlled by the `num_results` and\n  `sequence_indices` parameters. The user must supply either `num_results` or\n  `sequence_indices` but not both.\n  The former is the number of samples to produce starting from the first\n  element. If `sequence_indices` is given instead, the specified elements of\n  the sequence are generated. For example, sequence_indices=tf.range(10) is\n  equivalent to specifying n=10.\n\n  #### Examples\n\n  ```python\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  # Produce the first 1000 members of the Halton sequence in 3 dimensions.\n  num_results = 1000\n  dim = 3\n  sample = tfp.mcmc.sample_halton_sequence(\n    dim,\n    num_results=num_results,\n    seed=127)\n\n  # Evaluate the integral of x_1 * x_2^2 * x_3^3  over the three dimensional\n  # hypercube.\n  powers = tf.range(1.0, limit=dim + 1)\n  integral = tf.reduce_mean(tf.reduce_prod(sample ** powers, axis=-1))\n  true_value = 1.0 / tf.reduce_prod(powers + 1.0)\n  with tf.Session() as session:\n    values = session.run((integral, true_value))\n\n  # Produces a relative absolute error of 1.7%.\n  print (\"Estimated: %f, True Value: %f\" % values)\n\n  # Now skip the first 1000 samples and recompute the integral with the next\n  # thousand samples. The sequence_indices argument can be used to do this.\n\n\n  sequence_indices = tf.range(start=1000, limit=1000 + num_results,\n                              dtype=tf.int32)\n  sample_leaped = tfp.mcmc.sample_halton_sequence(\n      dim,\n      sequence_indices=sequence_indices,\n      seed=111217)\n\n  integral_leaped = tf.reduce_mean(tf.reduce_prod(sample_leaped ** powers,\n                                                  axis=-1))\n  with tf.Session() as session:\n    values = session.run((integral_leaped, true_value))\n  # Now produces a relative absolute error of 0.05%.\n  print (\"Leaped Estimated: %f, True Value: %f\" % values)\n  ```\n\n  Args:\n    dim: Positive Python `int` representing each sample's `event_size.` Must\n      not be greater than 1000.\n    num_results: (Optional) Positive scalar `Tensor` of dtype int32. The number\n      of samples to generate. Either this parameter or sequence_indices must\n      be specified but not both. If this parameter is None, then the behaviour\n      is determined by the `sequence_indices`.\n      Default value: `None`.\n    sequence_indices: (Optional) `Tensor` of dtype int32 and rank 1. The\n      elements of the sequence to compute specified by their position in the\n      sequence. The entries index into the Halton sequence starting with 0 and\n      hence, must be whole numbers. For example, sequence_indices=[0, 5, 6] will\n      produce the first, sixth and seventh elements of the sequence. If this\n      parameter is None, then the `num_results` parameter must be specified\n      which gives the number of desired samples starting from the first sample.\n      Default value: `None`.\n    dtype: (Optional) The dtype of the sample. One of: `float16`, `float32` or\n      `float64`.\n      Default value: `tf.float32`.\n    randomized: (Optional) bool indicating whether to produce a randomized\n      Halton sequence. If True, applies the randomization described in\n      [Owen (2017)][1].\n      Default value: `True`.\n    seed: (Optional) Python integer to seed the random number generator. Only\n      used if `randomized` is True. If not supplied and `randomized` is True,\n      no seed is set.", "suffix": "    name:  (Optional) Python `str` describing ops managed by this function. If\n      not supplied the name of this function is used.\n      Default value: \"sample_halton_sequence\".\n\n  Returns:\n    halton_elements: Elements of the Halton sequence. `Tensor` of supplied dtype\n      and `shape` `[num_results, dim]` if `num_results` was specified or shape\n      `[s, dim]` where s is the size of `sequence_indices` if `sequence_indices`\n      were specified.\n\n  Raises:\n    ValueError: if both `sequence_indices` and `num_results` were specified or\n      if dimension `dim` is less than 1 or greater than 1000.\n\n  #### References\n\n  [1]: Art B. Owen. A randomized Halton algorithm in R. _arXiv preprint\n       arXiv:1706.02808_, 2017. https://arxiv.org/abs/1706.02808\n  \"\"\"\n  if dim < 1 or dim > _MAX_DIMENSION:\n    raise ValueError(\n        'Dimension must be between 1 and {}. Supplied {}'.format(_MAX_DIMENSION,\n                                                                 dim))\n  if (num_results is None) == (sequence_indices is None):\n    raise ValueError('Either `num_results` or `sequence_indices` must be'\n                     ' specified but not both.')\n\n  if not dtype.is_floating:\n    raise ValueError('dtype must be of `float`-type')\n\n  with tf.compat.v1.name_scope(\n      name, 'sample', values=[num_results, sequence_indices]):\n    # Here and in the following, the shape layout is as follows:\n    # [sample dimension, event dimension, coefficient dimension].\n    # The coefficient dimension is an intermediate axes which will hold the\n    # weights of the starting integer when expressed in the (prime) base for\n    # an event dimension.\n    if num_results is not None:\n      num_results = tf.convert_to_tensor(value=num_results)\n    if sequence_indices is not None:\n      sequence_indices = tf.convert_to_tensor(value=sequence_indices)\n    indices = _get_indices(num_results, sequence_indices, dtype)\n    radixes = tf.constant(_PRIMES[0:dim], dtype=dtype, shape=[dim, 1])\n\n    max_sizes_by_axes = _base_expansion_size(\n        tf.reduce_max(input_tensor=indices), radixes)\n\n    max_size = tf.reduce_max(input_tensor=max_sizes_by_axes)\n\n    # The powers of the radixes that we will need. Note that there is a bit\n    # of an excess here. Suppose we need the place value coefficients of 7\n    # in base 2 and 3. For 2, we will have 3 digits but we only need 2 digits\n    # for base 3. However, we can only create rectangular tensors so we\n    # store both expansions in a [2, 3] tensor. This leads to the problem that\n    # we might end up attempting to raise large numbers to large powers. For\n    # example, base 2 expansion of 1024 has 10 digits. If we were in 10\n    # dimensions, then the 10th prime (29) we will end up computing 29^10 even\n    # though we don't need it. We avoid this by setting the exponents for each\n    # axes to 0 beyond the maximum value needed for that dimension.\n    exponents_by_axes = tf.tile([tf.range(max_size)], [dim, 1])\n\n    # The mask is true for those coefficients that are irrelevant.\n    weight_mask = exponents_by_axes >= max_sizes_by_axes\n    capped_exponents = tf.where(\n        weight_mask,\n        tf.zeros_like(exponents_by_axes),\n        exponents_by_axes)\n    weights = radixes ** capped_exponents\n    # The following computes the base b expansion of the indices. Suppose,\n    # x = a0 + a1*b + a2*b^2 + ... Then, performing a floor div of x with\n    # the vector (1, b, b^2, b^3, ...) will produce\n    # (a0 + s1 * b, a1 + s2 * b, ...) where s_i are coefficients we don't care\n    # about. Noting that all a_i < b by definition of place value expansion,\n    # we see that taking the elements mod b of the above vector produces the\n    # place value expansion coefficients.\n    coeffs = tf.math.floordiv(indices, weights)\n    coeffs *= 1. - tf.cast(weight_mask, dtype)\n    coeffs %= radixes\n    if not randomized:\n      coeffs /= radixes\n      return tf.reduce_sum(input_tensor=coeffs / weights, axis=-1)\n    stream = distributions.SeedStream(seed, salt='MCMCSampleHaltonSequence')\n    coeffs = _randomize(coeffs, radixes, seed=stream())\n    # Remove the contribution from randomizing the trailing zero for the\n    # axes where max_size_by_axes < max_size. This will be accounted\n    # for separately below (using zero_correction).\n    coeffs *= 1. - tf.cast(weight_mask, dtype)\n    coeffs /= radixes\n    base_values = tf.reduce_sum(input_tensor=coeffs / weights, axis=-1)\n\n    # The randomization used in Owen (2017) does not leave 0 invariant. While\n    # we have accounted for the randomization of the first `max_size_by_axes`\n    # coefficients, we still need to correct for the trailing zeros. Luckily,\n    # this is equivalent to adding a uniform random value scaled so the first\n    # `max_size_by_axes` coefficients are zero. The following statements perform\n    # this correction.\n    zero_correction = tf.random.uniform([dim, 1], seed=stream(), dtype=dtype)\n    zero_correction /= radixes ** max_sizes_by_axes\n    return base_values + tf.reshape(zero_correction, [-1])", "gt": "      Default value: `None`."}
{"prefix": "def _randomize(coeffs, radixes, seed=None):\n  \"\"\"Applies the Owen (2017) randomization to the coefficients.\"\"\"\n  given_dtype = coeffs.dtype\n  coeffs = tf.cast(coeffs, dtype=tf.int32)\n  num_coeffs = tf.shape(input=coeffs)[-1]\n  radixes = tf.reshape(tf.cast(radixes, dtype=tf.int32), shape=[-1])\n  stream = distributions.SeedStream(seed, salt='MCMCSampleHaltonSequence2')\n  perms = _get_permutations(num_coeffs, radixes, seed=stream())\n  perms = tf.reshape(perms, shape=[-1])\n  radix_sum = tf.reduce_sum(input_tensor=radixes)\n  radix_offsets = tf.reshape(tf.cumsum(radixes, exclusive=True),\n                             shape=[-1, 1])\n  offsets = radix_offsets + tf.range(num_coeffs) * radix_sum", "suffix": "  return tf.cast(permuted_coeffs, dtype=given_dtype)", "gt": "  permuted_coeffs = tf.gather(perms, coeffs + offsets)"}
{"prefix": "def _get_permutations(num_results, dims, seed=None):\n  \"\"\"Uniform iid sample from the space of permutations.\n\n  Draws a sample of size `num_results` from the group of permutations of degrees\n  specified by the `dims` tensor. These are packed together into one tensor\n  such that each row is one sample from each of the dimensions in `dims`. For\n  example, if dims = [2,3] and num_results = 2, the result is a tensor of shape\n  [2, 2 + 3] and the first row of the result might look like:\n  [1, 0, 2, 0, 1]. The first two elements are a permutation over 2 elements\n  while the next three are a permutation over 3 elements.\n\n  Args:\n    num_results: A positive scalar `Tensor` of integral type. The number of\n      draws from the discrete uniform distribution over the permutation groups.\n    dims: A 1D `Tensor` of the same dtype as `num_results`. The degree of the\n      permutation groups from which to sample.\n    seed: (Optional) Python integer to seed the random number generator.\n\n  Returns:\n    permutations: A `Tensor` of shape `[num_results, sum(dims)]` and the same\n    dtype as `dims`.", "suffix": "  sample_range = tf.range(num_results)\n  stream = distributions.SeedStream(seed, salt='MCMCSampleHaltonSequence3')\n  def generate_one(d):\n    seed = stream()\n    fn = lambda _: tf.random.shuffle(tf.range(d), seed=seed)\n    return tf.map_fn(\n        fn,\n        sample_range,\n        parallel_iterations=1 if seed is not None else 10)\n  return tf.concat([generate_one(d) for d in tf.unstack(dims)],\n                   axis=-1)", "gt": "  \"\"\""}
